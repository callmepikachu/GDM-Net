"""
ä¿®å¤å¤šGPUè®­ç»ƒä¸­çš„è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
"""

import torch
import os
import gc


def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    print("ğŸ§¹ æ¸…ç†GPUå†…å­˜...")
    
    gc.collect()
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            with torch.cuda.device(i):
                torch.cuda.empty_cache()
        print(f"âœ… å·²æ¸…ç† {torch.cuda.device_count()} ä¸ªGPUçš„å†…å­˜")
    else:
        print("âŒ CUDAä¸å¯ç”¨")


def setup_multi_gpu_env():
    """è®¾ç½®å¤šGPUç¯å¢ƒå˜é‡"""
    print("ğŸ”§ è®¾ç½®å¤šGPUç¯å¢ƒå˜é‡...")
    
    # NCCLè®¾ç½®
    os.environ['NCCL_DEBUG'] = 'WARN'
    os.environ['NCCL_TREE_THRESHOLD'] = '0'
    os.environ['NCCL_IB_DISABLE'] = '1'
    os.environ['NCCL_SOCKET_IFNAME'] = 'lo'
    
    # CUDAè®¾ç½®
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'
    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(i) for i in range(torch.cuda.device_count()))
    
    # PyTorchè®¾ç½®
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    print("âœ… å¤šGPUç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")


def test_multi_gpu_communication():
    """æµ‹è¯•å¤šGPUé€šä¿¡"""
    print("ğŸ” æµ‹è¯•å¤šGPUé€šä¿¡...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    num_gpus = torch.cuda.device_count()
    if num_gpus < 2:
        print(f"âš ï¸ åªæœ‰ {num_gpus} ä¸ªGPUï¼Œæ— æ³•æµ‹è¯•å¤šGPUé€šä¿¡")
        return False
    
    try:
        # åœ¨æ¯ä¸ªGPUä¸Šåˆ›å»ºå¼ é‡
        tensors = []
        for i in range(num_gpus):
            with torch.cuda.device(i):
                tensor = torch.randn(100, 100, device=f'cuda:{i}')
                tensors.append(tensor)
        
        print(f"âœ… åœ¨ {num_gpus} ä¸ªGPUä¸Šåˆ›å»ºå¼ é‡æˆåŠŸ")
        
        # æµ‹è¯•å¼ é‡ä¼ è¾“
        tensor_0_to_1 = tensors[0].to('cuda:1')
        print("âœ… GPUé—´å¼ é‡ä¼ è¾“æˆåŠŸ")
        
        # æµ‹è¯•all_reduceæ“ä½œï¼ˆæ¨¡æ‹ŸDDPé€šä¿¡ï¼‰
        if torch.distributed.is_available():
            print("âœ… åˆ†å¸ƒå¼é€šä¿¡å¯ç”¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šGPUé€šä¿¡æµ‹è¯•å¤±è´¥: {e}")
        return False


def fix_device_mismatch():
    """ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜"""
    print("ğŸ”§ ä¿®å¤è®¾å¤‡ä¸åŒ¹é…é—®é¢˜...")
    
    # è®¾ç½®é»˜è®¤è®¾å¤‡
    if torch.cuda.is_available():
        torch.cuda.set_device(0)
        print("âœ… è®¾ç½®é»˜è®¤CUDAè®¾å¤‡ä¸º0")
    
    # ç¦ç”¨æŸäº›å¯èƒ½å¯¼è‡´è®¾å¤‡ä¸åŒ¹é…çš„ä¼˜åŒ–
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print("âœ… ç¦ç”¨å¯èƒ½å¯¼è‡´é—®é¢˜çš„CUDNNä¼˜åŒ–")


def create_single_gpu_fallback_config():
    """åˆ›å»ºå•GPUå›é€€é…ç½®"""
    print("ğŸ”„ åˆ›å»ºå•GPUå›é€€é…ç½®...")
    
    fallback_config = """
# Single GPU Fallback Configuration
# å½“å¤šGPUè®­ç»ƒå¤±è´¥æ—¶ä½¿ç”¨

seed: 42

model:
  bert_model_name: "bert-base-uncased"
  hidden_size: 512  # å‡å°‘æ¨¡å‹å¤§å°
  num_entities: 9
  num_relations: 10
  num_classes: 5
  gnn_type: "rgcn"
  num_gnn_layers: 1  # å‡å°‘å±‚æ•°
  num_reasoning_hops: 2
  fusion_method: "gate"
  learning_rate: 3e-5
  dropout_rate: 0.1

data:
  train_path: "data/hotpotqa_official_train.json"
  val_path: "data/hotpotqa_official_val.json"
  test_path: "data/hotpotqa_official_val.json"
  max_length: 256  # å‡å°‘åºåˆ—é•¿åº¦
  max_query_length: 32

training:
  max_epochs: 5
  batch_size: 1  # å•GPUå°æ‰¹æ¬¡
  num_workers: 0  # ç¦ç”¨å¤šè¿›ç¨‹
  accelerator: "gpu"
  devices: 1  # å¼ºåˆ¶å•GPU
  precision: 32
  gradient_clip_val: 1.0
  accumulate_grad_batches: 16  # å¢åŠ ç´¯ç§¯
  val_check_interval: 0.5
  log_every_n_steps: 50
  checkpoint_dir: "checkpoints"
  early_stopping: true
  patience: 3

logging:
  type: "tensorboard"
  save_dir: "logs"
  name: "gdmnet-single-gpu-fallback"
"""
    
    with open('config/single_gpu_fallback_config.yaml', 'w') as f:
        f.write(fallback_config.strip())
    
    print("âœ… å•GPUå›é€€é…ç½®å·²åˆ›å»º: config/single_gpu_fallback_config.yaml")


def comprehensive_multi_gpu_fix():
    """ç»¼åˆå¤šGPUé—®é¢˜ä¿®å¤"""
    print("ğŸ› ï¸ å¤šGPUè®­ç»ƒé—®é¢˜ç»¼åˆä¿®å¤")
    print("=" * 50)
    
    # 1. æ¸…ç†GPUå†…å­˜
    clear_gpu_memory()
    print()
    
    # 2. è®¾ç½®ç¯å¢ƒå˜é‡
    setup_multi_gpu_env()
    print()
    
    # 3. ä¿®å¤è®¾å¤‡ä¸åŒ¹é…
    fix_device_mismatch()
    print()
    
    # 4. æµ‹è¯•å¤šGPUé€šä¿¡
    comm_ok = test_multi_gpu_communication()
    print()
    
    # 5. åˆ›å»ºå›é€€é…ç½®
    create_single_gpu_fallback_config()
    print()
    
    # 6. ç»™å‡ºå»ºè®®
    if comm_ok:
        print("ğŸ‰ å¤šGPUç¯å¢ƒä¿®å¤å®Œæˆï¼")
        print("ğŸ’¡ å»ºè®®:")
        print("1. é‡æ–°è¿è¡Œè®­ç»ƒè„šæœ¬")
        print("2. å¦‚æœä»æœ‰é—®é¢˜ï¼Œä½¿ç”¨å•GPUå›é€€é…ç½®")
        print("3. ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ")
    else:
        print("âš ï¸ å¤šGPUé€šä¿¡ä»æœ‰é—®é¢˜")
        print("ğŸ’¡ å»ºè®®:")
        print("1. ä½¿ç”¨å•GPUå›é€€é…ç½®")
        print("2. æ£€æŸ¥CUDAå’Œé©±åŠ¨ç‰ˆæœ¬")
        print("3. é‡å¯ç³»ç»Ÿ")
    
    return comm_ok


def quick_single_gpu_fix():
    """å¿«é€Ÿåˆ‡æ¢åˆ°å•GPUæ¨¡å¼"""
    print("âš¡ å¿«é€Ÿåˆ‡æ¢åˆ°å•GPUæ¨¡å¼")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶å•GPU
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    # æ¸…ç†å†…å­˜
    clear_gpu_memory()
    
    print("âœ… å·²åˆ‡æ¢åˆ°å•GPUæ¨¡å¼")
    print("ğŸ’¡ è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é‡æ–°è®­ç»ƒ:")
    print("python train/train.py --config config/single_gpu_fallback_config.yaml --mode train")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--single-gpu':
        quick_single_gpu_fix()
    else:
        success = comprehensive_multi_gpu_fix()
        
        if not success:
            print("\nâš¡ è‡ªåŠ¨åˆ‡æ¢åˆ°å•GPUæ¨¡å¼...")
            quick_single_gpu_fix()
