"""
æœ€ç»ˆä¿®å¤è„šæœ¬ - å½»åº•è§£å†³æ‰€æœ‰ç´¢å¼•é—®é¢˜
ç¡®ä¿æ‰€æœ‰æ ‡ç­¾éƒ½ä¸¥æ ¼åœ¨æ¨¡å‹å®šä¹‰çš„èŒƒå›´å†…
"""

import json
import numpy as np


def final_fix_dataset(input_path, output_path):
    """æœ€ç»ˆä¿®å¤æ•°æ®é›†ï¼Œç¡®ä¿æ‰€æœ‰ç´¢å¼•éƒ½åœ¨æ­£ç¡®èŒƒå›´å†…"""
    print(f"ğŸ”§ æœ€ç»ˆä¿®å¤: {input_path}")
    
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    fixed_data = []
    
    for i, item in enumerate(data):
        try:
            fixed_item = {
                "document": item.get("document", ""),
                "query": item.get("query", ""),
                "entities": [],
                "relations": [],
                "label": max(0, min(item.get("label", 0), 4)),  # å¼ºåˆ¶åœ¨0-4èŒƒå›´å†…
                "metadata": item.get("metadata", {})
            }
            
            # å¤„ç†å®ä½“ - ç¡®ä¿æ‰€æœ‰å€¼éƒ½åœ¨0-7èŒƒå›´å†…
            for entity in item.get("entities", []):
                span = entity.get("span", [0, 0])
                if not isinstance(span, list) or len(span) != 2:
                    span = [0, 0]
                
                # ç¡®ä¿spanæ˜¯æœ‰æ•ˆçš„æ•´æ•°
                try:
                    span = [max(0, int(span[0])), max(0, int(span[1]))]
                except:
                    span = [0, 0]
                
                # è·å–å®ä½“ç±»å‹å¹¶å¼ºåˆ¶åœ¨0-7èŒƒå›´å†…
                entity_type = entity.get("type", 0)
                if isinstance(entity_type, str):
                    # å­—ç¬¦ä¸²æ˜ å°„
                    type_map = {
                        'TITLE': 1, 'ENTITY': 2, 'PERSON': 3, 'ORGANIZATION': 4,
                        'LOCATION': 5, 'MISC': 6, 'DATE': 7
                    }
                    entity_type = type_map.get(entity_type, 0)
                else:
                    try:
                        entity_type = int(entity_type)
                    except:
                        entity_type = 0
                
                # å¼ºåˆ¶åœ¨0-7èŒƒå›´å†…
                entity_type = max(0, min(entity_type, 7))
                
                fixed_entity = {
                    "span": span,
                    "type": entity_type,
                    "text": str(entity.get("text", ""))
                }
                fixed_item["entities"].append(fixed_entity)
            
            # å¤„ç†å…³ç³» - ç¡®ä¿æ‰€æœ‰å€¼éƒ½åœ¨0-3èŒƒå›´å†…
            for relation in item.get("relations", []):
                try:
                    head = max(0, min(int(relation.get("head", 0)), len(fixed_item["entities"]) - 1))
                    tail = max(0, min(int(relation.get("tail", 0)), len(fixed_item["entities"]) - 1))
                    
                    # åªæœ‰å½“headå’Œtailéƒ½æœ‰æ•ˆæ—¶æ‰æ·»åŠ å…³ç³»
                    if head < len(fixed_item["entities"]) and tail < len(fixed_item["entities"]) and head != tail:
                        relation_type = relation.get("type", 0)
                        if isinstance(relation_type, str):
                            type_map = {
                                'SUPPORTS': 1, 'RELATED': 2, 'PART_OF': 3
                            }
                            relation_type = type_map.get(relation_type, 0)
                        else:
                            try:
                                relation_type = int(relation_type)
                            except:
                                relation_type = 0
                        
                        # å¼ºåˆ¶åœ¨0-3èŒƒå›´å†…
                        relation_type = max(0, min(relation_type, 3))
                        
                        fixed_relation = {
                            "head": head,
                            "tail": tail,
                            "type": relation_type
                        }
                        fixed_item["relations"].append(fixed_relation)
                except:
                    continue  # è·³è¿‡æœ‰é—®é¢˜çš„å…³ç³»
            
            fixed_data.append(fixed_item)
            
        except Exception as e:
            print(f"âš ï¸  è·³è¿‡æ ·æœ¬ {i}: {e}")
            continue
    
    # ä¿å­˜ä¿®å¤åçš„æ•°æ®
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æœ€ç»ˆä¿®å¤å®Œæˆ: {output_path} ({len(fixed_data)} æ ·æœ¬)")
    
    # è¯¦ç»†éªŒè¯
    verify_fixed_dataset(output_path)
    return output_path


def verify_fixed_dataset(filepath):
    """è¯¦ç»†éªŒè¯ä¿®å¤åçš„æ•°æ®é›†"""
    print(f"ğŸ” è¯¦ç»†éªŒè¯: {filepath}")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    entity_types = []
    relation_types = []
    labels = []
    issues = []
    
    for i, item in enumerate(data):
        # æ£€æŸ¥æ ‡ç­¾
        label = item.get("label", 0)
        labels.append(label)
        if label < 0 or label > 4:
            issues.append(f"æ ·æœ¬ {i}: æ ‡ç­¾ {label} è¶…å‡ºèŒƒå›´ [0,4]")
        
        # æ£€æŸ¥å®ä½“
        for j, entity in enumerate(item.get("entities", [])):
            entity_type = entity.get("type", 0)
            entity_types.append(entity_type)
            if entity_type < 0 or entity_type > 7:
                issues.append(f"æ ·æœ¬ {i}, å®ä½“ {j}: ç±»å‹ {entity_type} è¶…å‡ºèŒƒå›´ [0,7]")
        
        # æ£€æŸ¥å…³ç³»
        for j, relation in enumerate(item.get("relations", [])):
            relation_type = relation.get("type", 0)
            relation_types.append(relation_type)
            if relation_type < 0 or relation_type > 3:
                issues.append(f"æ ·æœ¬ {i}, å…³ç³» {j}: ç±»å‹ {relation_type} è¶…å‡ºèŒƒå›´ [0,3]")
            
            # æ£€æŸ¥headå’Œtailç´¢å¼•
            head = relation.get("head", 0)
            tail = relation.get("tail", 0)
            num_entities = len(item.get("entities", []))
            if head >= num_entities or tail >= num_entities:
                issues.append(f"æ ·æœ¬ {i}, å…³ç³» {j}: head={head} æˆ– tail={tail} è¶…å‡ºå®ä½“æ•°é‡ {num_entities}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ ·æœ¬æ•°é‡: {len(data)}")
    print(f"  æ ‡ç­¾èŒƒå›´: {min(labels)} - {max(labels)}")
    print(f"  å®ä½“ç±»å‹èŒƒå›´: {min(entity_types) if entity_types else 'N/A'} - {max(entity_types) if entity_types else 'N/A'}")
    print(f"  å…³ç³»ç±»å‹èŒƒå›´: {min(relation_types) if relation_types else 'N/A'} - {max(relation_types) if relation_types else 'N/A'}")
    
    # æŠ¥å‘Šé—®é¢˜
    if issues:
        print(f"âŒ å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for issue in issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
            print(f"  {issue}")
        if len(issues) > 5:
            print(f"  ... è¿˜æœ‰ {len(issues) - 5} ä¸ªé—®é¢˜")
        return False
    else:
        print("âœ… éªŒè¯é€šè¿‡ï¼Œæ‰€æœ‰ç´¢å¼•éƒ½åœ¨æ­£ç¡®èŒƒå›´å†…")
        return True


def create_minimal_test_data():
    """åˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®é›†ï¼Œç¡®ä¿æ²¡æœ‰ä»»ä½•é—®é¢˜"""
    print("ğŸ“ åˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®é›†...")
    
    minimal_data = []
    
    # åˆ›å»º10ä¸ªç®€å•çš„æµ‹è¯•æ ·æœ¬
    for i in range(10):
        sample = {
            "document": f"This is test document {i}. It contains some text for testing.",
            "query": f"What is the content of document {i}?",
            "entities": [
                {
                    "span": [0, 4],  # "This"
                    "type": 0,
                    "text": "This"
                },
                {
                    "span": [8, 12],  # "test"
                    "type": 1,
                    "text": "test"
                }
            ],
            "relations": [
                {
                    "head": 0,
                    "tail": 1,
                    "type": 0
                }
            ],
            "label": i % 5,  # 0-4èŒƒå›´å†…
            "metadata": {
                "source": "minimal_test",
                "id": f"test_{i}"
            }
        }
        minimal_data.append(sample)
    
    # ä¿å­˜è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_data = minimal_data[:8]
    val_data = minimal_data[8:]
    
    with open('data/minimal_train.json', 'w', encoding='utf-8') as f:
        json.dump(train_data, f, indent=2, ensure_ascii=False)
    
    with open('data/minimal_val.json', 'w', encoding='utf-8') as f:
        json.dump(val_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æœ€å°æµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
    
    # éªŒè¯æœ€å°æ•°æ®é›†
    verify_fixed_dataset('data/minimal_train.json')
    verify_fixed_dataset('data/minimal_val.json')


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æœ€ç»ˆä¿®å¤å·¥å…·")
    print("=" * 50)
    
    # é€‰é¡¹1: ä¿®å¤ç°æœ‰æ•°æ®é›†
    datasets_to_fix = [
        ('data/hotpotqa_small_train.json', 'data/hotpotqa_final_train.json'),
        ('data/hotpotqa_small_val.json', 'data/hotpotqa_final_val.json')
    ]
    
    all_success = True
    
    for input_path, output_path in datasets_to_fix:
        try:
            if not final_fix_dataset(input_path, output_path):
                all_success = False
            print()
        except Exception as e:
            print(f"âŒ ä¿®å¤å¤±è´¥ {input_path}: {e}")
            all_success = False
    
    # é€‰é¡¹2: åˆ›å»ºæœ€å°æµ‹è¯•æ•°æ®é›†
    print("=" * 50)
    create_minimal_test_data()
    
    if all_success:
        print("\nğŸ‰ æ‰€æœ‰æ•°æ®é›†ä¿®å¤å®Œæˆï¼")
        print("å»ºè®®ä½¿ç”¨ä»¥ä¸‹é…ç½®:")
        print("- å®Œæ•´æ•°æ®é›†: data/hotpotqa_final_*.json")
        print("- æœ€å°æµ‹è¯•é›†: data/minimal_*.json")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æ•°æ®é›†ä¿®å¤å¤±è´¥ï¼Œå»ºè®®ä½¿ç”¨æœ€å°æµ‹è¯•é›†")


if __name__ == "__main__":
    main()
