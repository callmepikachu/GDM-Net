"""
ä¸‹è½½å®˜æ–¹ HotpotQA æ•°æ®é›†
æŒ‰ç…§å®˜æ–¹æ–‡æ¡£ https://hotpotqa.github.io/ çš„æ–¹å¼ä¸‹è½½
"""

import os
import json
import requests
import zipfile
from pathlib import Path
from tqdm import tqdm
import gzip


def download_file(url, filename):
    """ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦"""
    print(f"ğŸ“¥ ä¸‹è½½ {filename}...")
    
    response = requests.get(url, stream=True)
    response.raise_for_status()
    
    total_size = int(response.headers.get('content-length', 0))
    
    with open(filename, 'wb') as f, tqdm(
        desc=filename,
        total=total_size,
        unit='B',
        unit_scale=True,
        unit_divisor=1024,
    ) as pbar:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                pbar.update(len(chunk))
    
    print(f"âœ… ä¸‹è½½å®Œæˆ: {filename}")


def download_official_hotpotqa():
    """ä¸‹è½½å®˜æ–¹ HotpotQA æ•°æ®é›†"""
    print("ğŸ¯ ä¸‹è½½å®˜æ–¹ HotpotQA æ•°æ®é›†")
    print("=" * 50)
    
    # å®˜æ–¹æ•°æ®é›†URL (æ ¹æ®å®˜ç½‘)
    urls = {
        "train": "http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json",
        "dev_distractor": "http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_distractor_v1.json",
        "dev_fullwiki": "http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_fullwiki_v1.json"
    }
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    data_dir = Path("data/official_hotpotqa")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    downloaded_files = {}
    
    try:
        for split, url in urls.items():
            filename = str(data_dir / f"{split}.json")  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²

            if os.path.exists(filename):
                print(f"âš ï¸  æ–‡ä»¶å·²å­˜åœ¨: {filename}")
                choice = input("æ˜¯å¦é‡æ–°ä¸‹è½½? (y/N): ").strip().lower()
                if choice != 'y':
                    print(f"è·³è¿‡ä¸‹è½½: {split}")
                    with open(filename, 'r', encoding='utf-8') as f:
                        downloaded_files[split] = json.load(f)
                    continue

            try:
                download_file(url, filename)
                
                # è¯»å–ä¸‹è½½çš„æ–‡ä»¶
                with open(filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    downloaded_files[split] = data
                    print(f"ğŸ“Š {split}: {len(data)} æ ·æœ¬")
                    
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥ {split}: {e}")
                continue
        
        return downloaded_files
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
        return {}


def convert_official_hotpotqa(data, max_samples=None):
    """è½¬æ¢å®˜æ–¹ HotpotQA æ•°æ®ä¸º GDM-Net æ ¼å¼"""
    print("ğŸ”„ è½¬æ¢æ•°æ®æ ¼å¼...")
    
    converted = []
    
    # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
    if max_samples:
        data = data[:max_samples]
    
    for i, item in enumerate(tqdm(data, desc="è½¬æ¢æ•°æ®")):
        try:
            # åˆå¹¶æ‰€æœ‰ä¸Šä¸‹æ–‡æ®µè½
            document_parts = []
            entities = []
            entity_positions = {}
            
            for title, sentences in item['context']:
                # åˆå¹¶å¥å­
                content = ' '.join(sentences)
                doc_part = f"{title}: {content}"
                
                # è®¡ç®—ä½ç½®
                start_pos = len(' '.join(document_parts))
                if document_parts:
                    start_pos += 1  # ç©ºæ ¼
                
                document_parts.append(doc_part)
                
                # æ·»åŠ æ ‡é¢˜ä½œä¸ºå®ä½“
                entities.append({
                    "span": [start_pos, start_pos + len(title)],
                    "type": "TITLE",
                    "text": title
                })
                
                # è®°å½•å®ä½“ä½ç½®ï¼Œç”¨äºå…³ç³»æŠ½å–
                entity_positions[title] = len(entities) - 1
            
            combined_doc = ' '.join(document_parts)
            
            # ä»æ”¯æ’‘äº‹å®åˆ›å»ºå…³ç³»
            relations = []
            supporting_titles = set()
            
            for fact in item['supporting_facts']:
                title = fact[0]
                supporting_titles.add(title)
            
            # åˆ›å»ºæ”¯æ’‘å®ä½“ä¹‹é—´çš„å…³ç³»
            supporting_entities = [entity_positions[title] for title in supporting_titles if title in entity_positions]
            for i in range(len(supporting_entities)):
                for j in range(i+1, len(supporting_entities)):
                    relations.append({
                        "head": supporting_entities[i],
                        "tail": supporting_entities[j],
                        "type": "SUPPORTS"
                    })
            
            # ç­”æ¡ˆç±»å‹åˆ†ç±»
            answer = item['answer'].lower()
            if answer in ['yes', 'no']:
                label = 1 if answer == 'yes' else 0
            else:
                # æ ¹æ®ç­”æ¡ˆé•¿åº¦ç®€å•åˆ†ç±»
                if len(answer.split()) == 1:
                    label = 2  # å•è¯ç­”æ¡ˆ
                elif len(answer.split()) <= 3:
                    label = 3  # çŸ­ç­”æ¡ˆ
                else:
                    label = 4  # é•¿ç­”æ¡ˆ
            
            converted_item = {
                "document": combined_doc[:2000],  # é™åˆ¶é•¿åº¦é¿å…å†…å­˜é—®é¢˜
                "query": item['question'],
                "entities": entities[:15],  # é™åˆ¶å®ä½“æ•°é‡
                "relations": relations[:10],  # é™åˆ¶å…³ç³»æ•°é‡
                "label": label,
                "metadata": {
                    "source": "official_hotpotqa",
                    "answer": item['answer'],
                    "supporting_facts": item['supporting_facts'],
                    "level": item.get('level', 'unknown'),
                    "type": item.get('type', 'unknown'),
                    "id": item.get('_id', f'sample_{i}')
                }
            }
            
            converted.append(converted_item)
            
        except Exception as e:
            print(f"âš ï¸  è·³è¿‡æ ·æœ¬ {i}: {e}")
            continue
    
    print(f"âœ… è½¬æ¢å®Œæˆ: {len(converted)} æ ·æœ¬")
    return converted


def save_converted_data(converted_data, split_name):
    """ä¿å­˜è½¬æ¢åçš„æ•°æ®"""
    output_file = f"data/hotpotqa_{split_name}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(converted_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ å·²ä¿å­˜: {output_file} ({len(converted_data)} æ ·æœ¬)")
    return output_file


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å®˜æ–¹ HotpotQA æ•°æ®é›†ä¸‹è½½å™¨")
    print("åŸºäºå®˜æ–¹æ–‡æ¡£: https://hotpotqa.github.io/")
    print("=" * 60)
    
    # ä¸‹è½½å®˜æ–¹æ•°æ®
    downloaded_files = download_official_hotpotqa()
    
    if not downloaded_files:
        print("âŒ æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•æ•°æ®é›†")
        return
    
    print(f"\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
    for split, data in downloaded_files.items():
        print(f"  {split}: {len(data)} æ ·æœ¬")
    
    # è½¬æ¢æ•°æ®æ ¼å¼
    print(f"\nğŸ”„ å¼€å§‹è½¬æ¢æ•°æ®æ ¼å¼...")
    
    converted_files = []
    
    # è½¬æ¢è®­ç»ƒé›† (é™åˆ¶æ ·æœ¬æ•°é‡)
    if 'train' in downloaded_files:
        print(f"\nå¤„ç†è®­ç»ƒé›†...")
        train_converted = convert_official_hotpotqa(downloaded_files['train'], max_samples=5000)
        train_file = save_converted_data(train_converted, 'official_train')
        converted_files.append(train_file)
    
    # è½¬æ¢éªŒè¯é›†
    if 'dev_distractor' in downloaded_files:
        print(f"\nå¤„ç†éªŒè¯é›†...")
        val_converted = convert_official_hotpotqa(downloaded_files['dev_distractor'], max_samples=1000)
        val_file = save_converted_data(val_converted, 'official_val')
        converted_files.append(val_file)
    
    # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
    if converted_files:
        print(f"\nğŸ“‹ æ•°æ®æ ·æœ¬é¢„è§ˆ:")
        with open(converted_files[0], 'r', encoding='utf-8') as f:
            sample_data = json.load(f)
            if sample_data:
                sample = sample_data[0]
                print(f"é—®é¢˜: {sample['query']}")
                print(f"æ–‡æ¡£: {sample['document'][:200]}...")
                print(f"å®ä½“æ•°é‡: {len(sample['entities'])}")
                print(f"å…³ç³»æ•°é‡: {len(sample['relations'])}")
                print(f"ç­”æ¡ˆ: {sample['metadata']['answer']}")
    
    print(f"\nğŸ‰ å®˜æ–¹ HotpotQA æ•°æ®é›†å‡†å¤‡å®Œæˆ!")
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥:")
    print(f"1. æ£€æŸ¥è½¬æ¢åçš„æ•°æ®: data/hotpotqa_official_*.json")
    print(f"2. æ›´æ–°é…ç½®æ–‡ä»¶ä½¿ç”¨æ–°æ•°æ®:")
    print(f"   ä¿®æ”¹ config/optimized_config.yaml ä¸­çš„ train_path å’Œ val_path")
    print(f"3. å¼€å§‹è®­ç»ƒ:")
    print(f"   python train/train.py --config config/optimized_config.yaml --mode train")


if __name__ == "__main__":
    main()
