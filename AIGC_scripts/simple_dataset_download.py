"""
ç®€åŒ–çš„æ•°æ®é›†ä¸‹è½½è„šæœ¬
ç›´æ¥ä¸‹è½½å’Œè½¬æ¢æ•°æ®é›†ï¼Œé¿å…å¤æ‚çš„ä¾èµ–é—®é¢˜
"""

import os
import json
import requests
from pathlib import Path
from tqdm import tqdm
import random


def create_sample_hotpotqa():
    """åˆ›å»º HotpotQA é£æ ¼çš„ç¤ºä¾‹æ•°æ®é›†"""
    print("ğŸ“ åˆ›å»º HotpotQA é£æ ¼çš„ç¤ºä¾‹æ•°æ®é›†...")
    
    # ç¤ºä¾‹æ•°æ®æ¨¡æ¿
    templates = [
        {
            "question": "Which company was founded first, {company1} or {company2}?",
            "contexts": [
                ["{company1}", ["{company1} was founded in {year1} by {founder1}.", "It is a {industry1} company."]],
                ["{company2}", ["{company2} was founded in {year2} by {founder2}.", "It specializes in {industry2}."]]
            ],
            "answer": "{answer}",
            "supporting_facts": [["{company1}", 0], ["{company2}", 0]]
        },
        {
            "question": "Who is the current CEO of {company}?",
            "contexts": [
                ["{company}", ["{company} is a technology company.", "{ceo} is the current CEO of {company}."]],
                ["Leadership", ["The company has undergone several leadership changes.", "Current leadership focuses on innovation."]]
            ],
            "answer": "{ceo}",
            "supporting_facts": [["{company}", 1]]
        },
        {
            "question": "What industry does {company} operate in?",
            "contexts": [
                ["{company}", ["{company} was founded by {founder}.", "The company operates in the {industry} industry."]],
                ["Industry Overview", ["The {industry} industry has grown rapidly.", "Many companies compete in this space."]]
            ],
            "answer": "{industry}",
            "supporting_facts": [["{company}", 1]]
        }
    ]
    
    # æ•°æ®å¡«å……
    companies = ["Apple Inc.", "Microsoft Corp.", "Google LLC", "Amazon Inc.", "Tesla Inc.", "Meta Platforms", "Netflix Inc."]
    founders = ["Steve Jobs", "Bill Gates", "Larry Page", "Jeff Bezos", "Elon Musk", "Mark Zuckerberg", "Reed Hastings"]
    ceos = ["Tim Cook", "Satya Nadella", "Sundar Pichai", "Andy Jassy", "Elon Musk", "Mark Zuckerberg", "Reed Hastings"]
    industries = ["technology", "software", "e-commerce", "automotive", "social media", "entertainment"]
    years = ["1976", "1975", "1998", "1994", "2003", "2004", "1997"]
    
    # ç”Ÿæˆæ•°æ®
    train_data = []
    val_data = []
    
    for i in range(1000):  # ç”Ÿæˆ1000ä¸ªè®­ç»ƒæ ·æœ¬
        template = random.choice(templates)
        
        if "company1" in template["question"]:
            # æ¯”è¾ƒç±»é—®é¢˜
            idx1, idx2 = random.sample(range(len(companies)), 2)
            year1, year2 = int(years[idx1]), int(years[idx2])
            answer = companies[idx1] if year1 < year2 else companies[idx2]
            
            data = {
                "question": template["question"].format(
                    company1=companies[idx1], 
                    company2=companies[idx2]
                ),
                "contexts": [
                    [companies[idx1], [
                        f"{companies[idx1]} was founded in {years[idx1]} by {founders[idx1]}.",
                        f"It is a {random.choice(industries)} company."
                    ]],
                    [companies[idx2], [
                        f"{companies[idx2]} was founded in {years[idx2]} by {founders[idx2]}.",
                        f"It specializes in {random.choice(industries)}."
                    ]]
                ],
                "answer": answer,
                "supporting_facts": [[companies[idx1], 0], [companies[idx2], 0]]
            }
        else:
            # å•ä¸€é—®é¢˜
            idx = random.randint(0, len(companies)-1)
            
            if "CEO" in template["question"]:
                answer = ceos[idx]
                contexts = [
                    [companies[idx], [
                        f"{companies[idx]} is a technology company.",
                        f"{ceos[idx]} is the current CEO of {companies[idx]}."
                    ]],
                    ["Leadership", [
                        "The company has undergone several leadership changes.",
                        "Current leadership focuses on innovation."
                    ]]
                ]
            else:  # industry question
                answer = random.choice(industries)
                contexts = [
                    [companies[idx], [
                        f"{companies[idx]} was founded by {founders[idx]}.",
                        f"The company operates in the {answer} industry."
                    ]],
                    ["Industry Overview", [
                        f"The {answer} industry has grown rapidly.",
                        "Many companies compete in this space."
                    ]]
                ]
            
            data = {
                "question": template["question"].format(company=companies[idx], ceo=ceos[idx]),
                "contexts": contexts,
                "answer": answer,
                "supporting_facts": [[companies[idx], 1]]
            }
        
        if i < 800:
            train_data.append(data)
        else:
            val_data.append(data)
    
    # è½¬æ¢ä¸ºGDM-Netæ ¼å¼
    train_converted = convert_hotpotqa_format(train_data)
    val_converted = convert_hotpotqa_format(val_data)
    
    # ä¿å­˜æ•°æ®
    os.makedirs("data", exist_ok=True)
    save_json(train_converted, "data/hotpotqa_train.json")
    save_json(val_converted, "data/hotpotqa_val.json")
    
    print(f"âœ… HotpotQA é£æ ¼æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(train_converted)} è®­ç»ƒæ ·æœ¬, {len(val_converted)} éªŒè¯æ ·æœ¬")
    return True


def convert_hotpotqa_format(data):
    """è½¬æ¢ HotpotQA æ ¼å¼åˆ° GDM-Net æ ¼å¼"""
    converted = []
    
    for item in data:
        # åˆå¹¶æ‰€æœ‰ä¸Šä¸‹æ–‡
        document_parts = []
        entities = []
        entity_id = 0
        
        for title, sentences in item['contexts']:
            content = ' '.join(sentences)
            doc_part = f"{title}: {content}"
            start_pos = len(' '.join(document_parts))
            if document_parts:
                start_pos += 1  # ç©ºæ ¼
            
            document_parts.append(doc_part)
            
            # æ·»åŠ æ ‡é¢˜ä½œä¸ºå®ä½“
            entities.append({
                "span": [start_pos, start_pos + len(title)],
                "type": "ENTITY",
                "text": title
            })
            entity_id += 1
        
        combined_doc = ' '.join(document_parts)
        
        # åˆ›å»ºå…³ç³»
        relations = []
        if len(entities) >= 2:
            relations.append({
                "head": 0,
                "tail": 1,
                "type": "RELATED"
            })
        
        # ç®€å•çš„ç­”æ¡ˆåˆ†ç±»
        answer_lower = item['answer'].lower()
        if any(word in answer_lower for word in ['apple', 'microsoft', 'google', 'amazon', 'tesla']):
            label = 0  # å…¬å¸
        elif any(word in answer_lower for word in ['cook', 'gates', 'pichai', 'bezos', 'musk']):
            label = 1  # äººå
        else:
            label = 2  # å…¶ä»–
        
        converted.append({
            "document": combined_doc[:1500],  # é™åˆ¶é•¿åº¦
            "query": item['question'],
            "entities": entities[:10],
            "relations": relations,
            "label": label,
            "metadata": {
                "source": "hotpotqa_style",
                "answer": item['answer'],
                "supporting_facts": item['supporting_facts']
            }
        })
    
    return converted


def download_docred_simple():
    """ç®€åŒ–çš„ DocRED ä¸‹è½½"""
    print("ğŸ“¥ ä¸‹è½½ DocRED æ•°æ®é›†...")
    
    # DocRED çš„ç›´æ¥ä¸‹è½½é“¾æ¥
    urls = {
        "train": "https://raw.githubusercontent.com/thunlp/DocRED/master/data/train_annotated.json",
        "dev": "https://raw.githubusercontent.com/thunlp/DocRED/master/data/dev.json"
    }
    
    try:
        for split, url in urls.items():
            print(f"ä¸‹è½½ {split} æ•°æ®...")
            
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                data = response.json()
                converted = convert_docred_format(data[:100])  # åªå–å‰100ä¸ªæ ·æœ¬
                
                filename = f"data/docred_{split}.json"
                save_json(converted, filename)
                print(f"âœ… ä¿å­˜ {filename}: {len(converted)} æ ·æœ¬")
            else:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {url}")
                return False
        
        print("âœ… DocRED ä¸‹è½½å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ DocRED ä¸‹è½½å¤±è´¥: {e}")
        return False


def convert_docred_format(data):
    """è½¬æ¢ DocRED æ ¼å¼"""
    converted = []
    
    for item in data:
        # åˆå¹¶å¥å­
        document = ' '.join([' '.join(sent) for sent in item['sents']])
        
        # è½¬æ¢å®ä½“
        entities = []
        for i, entity_group in enumerate(item['vertexSet'][:10]):  # é™åˆ¶å®ä½“æ•°é‡
            mention = entity_group[0]  # å–ç¬¬ä¸€ä¸ªæåŠ
            entities.append({
                "span": mention['pos'],
                "type": mention.get('type', 'MISC'),
                "text": mention['name']
            })
        
        # è½¬æ¢å…³ç³»
        relations = []
        for relation in item.get('labels', [])[:5]:  # é™åˆ¶å…³ç³»æ•°é‡
            if relation['h'] < len(entities) and relation['t'] < len(entities):
                relations.append({
                    "head": relation['h'],
                    "tail": relation['t'],
                    "type": f"R{relation['r']}"
                })
        
        converted.append({
            "document": document[:1500],
            "query": f"What relations exist in this document?",
            "entities": entities,
            "relations": relations,
            "label": min(len(relations), 4),
            "metadata": {
                "source": "docred",
                "title": item.get('title', '')
            }
        })
    
    return converted


def save_json(data, filepath):
    """ä¿å­˜JSONæ–‡ä»¶"""
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ å·²ä¿å­˜: {filepath} ({len(data)} æ ·æœ¬)")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç®€åŒ–æ•°æ®é›†ä¸‹è½½å™¨")
    print("=" * 40)
    
    print("å¯ç”¨æ•°æ®é›†:")
    print("1. HotpotQA é£æ ¼æ•°æ®é›† (æ¨è)")
    print("2. DocRED æ•°æ®é›†")
    print("3. ä¸¤ä¸ªéƒ½ä¸‹è½½")
    
    choice = input("\nè¯·é€‰æ‹© (1-3): ").strip()
    
    success = False
    
    if choice == "1":
        success = create_sample_hotpotqa()
    elif choice == "2":
        success = download_docred_simple()
    elif choice == "3":
        success1 = create_sample_hotpotqa()
        success2 = download_docred_simple()
        success = success1 or success2
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    if success:
        print("\nğŸ‰ æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print("1. æ£€æŸ¥ data/ ç›®å½•ä¸­çš„æ–‡ä»¶")
        print("2. å¼€å§‹è®­ç»ƒ:")
        print("   python train/train.py --config config/optimized_config.yaml --mode train")
        print("3. ç›‘æ§è®­ç»ƒ:")
        print("   tensorboard --logdir logs/")
    else:
        print("\nâŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥")


if __name__ == "__main__":
    main()
