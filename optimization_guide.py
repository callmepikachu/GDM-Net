"""
GDM-Net ä¼˜åŒ–æŒ‡å—
è§£å†³GPUåŠ é€Ÿã€å†…å­˜å ç”¨å’ŒçœŸå®æ•°æ®é›†æ¥å…¥é—®é¢˜
"""

import os
import json
import torch
import yaml
from pathlib import Path


def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    print("ğŸ” ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥")
    print("=" * 40)
    
    # PyTorchä¿¡æ¯
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°CUDAæ”¯æŒ")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥æ˜¯å¦æœ‰NVIDIA GPU")
        print("2. å®‰è£…CUDAé©±åŠ¨")
        print("3. é‡æ–°å®‰è£…æ”¯æŒCUDAçš„PyTorch")
    
    # å†…å­˜ä¿¡æ¯
    import psutil
    memory = psutil.virtual_memory()
    print(f"\nğŸ’¾ ç³»ç»Ÿå†…å­˜:")
    print(f"æ€»å†…å­˜: {memory.total / (1024**3):.1f} GB")
    print(f"å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f} GB")
    print(f"ä½¿ç”¨ç‡: {memory.percent}%")


def create_gpu_environment():
    """åˆ›å»ºæ”¯æŒGPUçš„ç¯å¢ƒé…ç½®"""
    print("\nğŸš€ åˆ›å»ºGPUç¯å¢ƒé…ç½®")
    print("=" * 40)
    
    gpu_env_config = {
        'name': 'gdmnet-gpu',
        'channels': ['pytorch', 'nvidia', 'conda-forge', 'defaults'],
        'dependencies': [
            'python=3.9',
            'pytorch>=1.12.0',
            'torchvision',
            'torchaudio',
            'pytorch-cuda=11.8',  # æˆ– 11.7
            'cudatoolkit',
            'pip',
            {
                'pip': [
                    'torch-geometric>=2.1.0',
                    'transformers>=4.20.0',
                    'pytorch-lightning>=1.7.0',
                    'datasets>=2.0.0',
                    'numpy>=1.21.0',
                    'pandas>=1.3.0',
                    'PyYAML>=6.0',
                    'tensorboard>=2.8.0',
                    'wandb>=0.12.0',
                    'tqdm>=4.64.0',
                    'scikit-learn>=1.1.0',
                    'matplotlib>=3.5.0',
                    'seaborn>=0.11.0',
                    'psutil'  # ç”¨äºå†…å­˜ç›‘æ§
                ]
            }
        ]
    }
    
    with open('environment_gpu.yml', 'w', encoding='utf-8') as f:
        yaml.dump(gpu_env_config, f, default_flow_style=False, allow_unicode=True)
    
    print("âœ… å·²åˆ›å»º environment_gpu.yml")
    print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print("conda env create -f environment_gpu.yml")
    print("conda activate gdmnet-gpu")


def create_optimized_config():
    """åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
    print("\nâš¡ åˆ›å»ºä¼˜åŒ–é…ç½®")
    print("=" * 40)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰GPU
    has_gpu = torch.cuda.is_available()
    
    optimized_config = {
        'seed': 42,
        'model': {
            'bert_model_name': 'bert-base-uncased',
            'hidden_size': 768,
            'num_entities': 10,
            'num_relations': 20,
            'num_classes': 5,
            'gnn_type': 'rgcn',
            'num_gnn_layers': 2,
            'num_reasoning_hops': 3,
            'fusion_method': 'gate',
            'learning_rate': 2e-5,
            'dropout_rate': 0.1
        },
        'data': {
            'train_path': 'data/train.json',
            'val_path': 'data/val.json',
            'test_path': 'data/test.json',
            'max_length': 256,  # å‡å°‘åºåˆ—é•¿åº¦ä»¥èŠ‚çœå†…å­˜
            'max_query_length': 32
        },
        'training': {
            'max_epochs': 5,  # å‡å°‘epochæ•°
            'batch_size': 2 if not has_gpu else 8,  # CPUç”¨å°batch size
            'num_workers': 2,  # å‡å°‘workeræ•°é‡
            'accelerator': 'gpu' if has_gpu else 'cpu',
            'devices': 1,
            'precision': 32,  # ä½¿ç”¨32ä½ç²¾åº¦é¿å…é—®é¢˜
            'gradient_clip_val': 1.0,
            'accumulate_grad_batches': 4 if not has_gpu else 1,  # CPUç”¨æ¢¯åº¦ç´¯ç§¯
            'val_check_interval': 1.0,
            'log_every_n_steps': 10,
            'checkpoint_dir': 'checkpoints',
            'early_stopping': True,
            'patience': 3
        },
        'logging': {
            'type': 'tensorboard',
            'save_dir': 'logs',
            'name': 'gdmnet-optimized'
        }
    }
    
    with open('config/optimized_config.yaml', 'w', encoding='utf-8') as f:
        yaml.dump(optimized_config, f, default_flow_style=False, allow_unicode=True)
    
    print("âœ… å·²åˆ›å»º config/optimized_config.yaml")
    print(f"ğŸ¯ é…ç½®ç‰¹ç‚¹:")
    print(f"- è®¾å¤‡: {'GPU' if has_gpu else 'CPU'}")
    print(f"- æ‰¹æ¬¡å¤§å°: {optimized_config['training']['batch_size']}")
    print(f"- åºåˆ—é•¿åº¦: {optimized_config['data']['max_length']}")
    print(f"- æ¢¯åº¦ç´¯ç§¯: {optimized_config['training']['accumulate_grad_batches']}")


def create_real_dataset_template():
    """åˆ›å»ºçœŸå®æ•°æ®é›†æ¨¡æ¿å’Œè½¬æ¢è„šæœ¬"""
    print("\nğŸ“Š åˆ›å»ºçœŸå®æ•°æ®é›†æ¨¡æ¿")
    print("=" * 40)
    
    # åˆ›å»ºæ•°æ®é›†æ¨¡æ¿
    template = {
        "document": "Your document text here",
        "query": "Your query here", 
        "entities": [
            {
                "span": [0, 5],  # å­—ç¬¦ä½ç½® [start, end]
                "type": "PERSON",  # å®ä½“ç±»å‹
                "text": "Apple"  # å®ä½“æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
            }
        ],
        "relations": [
            {
                "head": 0,  # å¤´å®ä½“ç´¢å¼•
                "tail": 1,  # å°¾å®ä½“ç´¢å¼•
                "type": "CEO_OF"  # å…³ç³»ç±»å‹
            }
        ],
        "label": 0,  # åˆ†ç±»æ ‡ç­¾
        "metadata": {
            "source": "dataset_name",
            "id": "sample_001"
        }
    }
    
    # ä¿å­˜æ¨¡æ¿
    with open('data_template.json', 'w', encoding='utf-8') as f:
        json.dump(template, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºæ•°æ®è½¬æ¢è„šæœ¬
    converter_script = '''"""
çœŸå®æ•°æ®é›†è½¬æ¢è„šæœ¬
å°†æ‚¨çš„æ•°æ®è½¬æ¢ä¸ºGDM-Netæ ¼å¼
"""

import json
import pandas as pd
from typing import List, Dict, Any


def convert_from_csv(csv_path: str, output_path: str):
    """ä»CSVæ–‡ä»¶è½¬æ¢æ•°æ®"""
    df = pd.read_csv(csv_path)
    
    converted_data = []
    for _, row in df.iterrows():
        sample = {
            "document": row['document'],
            "query": row.get('query', ''),
            "entities": json.loads(row.get('entities', '[]')),
            "relations": json.loads(row.get('relations', '[]')),
            "label": int(row.get('label', 0)),
            "metadata": {"source": "csv", "id": str(row.get('id', ''))}
        }
        converted_data.append(sample)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(converted_data, f, indent=2, ensure_ascii=False)
    
    print(f"è½¬æ¢å®Œæˆ: {len(converted_data)} ä¸ªæ ·æœ¬ -> {output_path}")


def convert_from_jsonl(jsonl_path: str, output_path: str):
    """ä»JSONLæ–‡ä»¶è½¬æ¢æ•°æ®"""
    converted_data = []
    
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data = json.loads(line)
                # æ ¹æ®æ‚¨çš„æ•°æ®æ ¼å¼è°ƒæ•´è¿™é‡Œ
                sample = {
                    "document": data.get('text', data.get('document', '')),
                    "query": data.get('query', ''),
                    "entities": data.get('entities', []),
                    "relations": data.get('relations', []),
                    "label": data.get('label', 0),
                    "metadata": data.get('metadata', {})
                }
                converted_data.append(sample)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(converted_data, f, indent=2, ensure_ascii=False)
    
    print(f"è½¬æ¢å®Œæˆ: {len(converted_data)} ä¸ªæ ·æœ¬ -> {output_path}")


def convert_docred_format(docred_path: str, output_path: str):
    """è½¬æ¢DocREDæ ¼å¼æ•°æ®"""
    with open(docred_path, 'r', encoding='utf-8') as f:
        docred_data = json.load(f)
    
    converted_data = []
    for item in docred_data:
        # åˆå¹¶å¥å­
        document = ' '.join([' '.join(sent) for sent in item['sents']])
        
        # è½¬æ¢å®ä½“
        entities = []
        for entity in item['vertexSet']:
            for mention in entity:
                entities.append({
                    "span": [mention['pos'][0], mention['pos'][1]],
                    "type": mention.get('type', 'MISC'),
                    "text": mention['name']
                })
        
        # è½¬æ¢å…³ç³»
        relations = []
        for relation in item.get('labels', []):
            relations.append({
                "head": relation['h'],
                "tail": relation['t'],
                "type": relation['r']
            })
        
        sample = {
            "document": document,
            "query": "",  # DocREDæ²¡æœ‰æŸ¥è¯¢
            "entities": entities,
            "relations": relations,
            "label": 0,  # é»˜è®¤æ ‡ç­¾
            "metadata": {"source": "docred", "title": item.get('title', '')}
        }
        converted_data.append(sample)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(converted_data, f, indent=2, ensure_ascii=False)
    
    print(f"è½¬æ¢å®Œæˆ: {len(converted_data)} ä¸ªæ ·æœ¬ -> {output_path}")


if __name__ == "__main__":
    print("æ•°æ®è½¬æ¢è„šæœ¬")
    print("è¯·æ ¹æ®æ‚¨çš„æ•°æ®æ ¼å¼é€‰æ‹©ç›¸åº”çš„è½¬æ¢å‡½æ•°")
    print("1. convert_from_csv() - CSVæ ¼å¼")
    print("2. convert_from_jsonl() - JSONLæ ¼å¼") 
    print("3. convert_docred_format() - DocREDæ ¼å¼")
'''
    
    with open('convert_dataset.py', 'w', encoding='utf-8') as f:
        f.write(converter_script)
    
    print("âœ… å·²åˆ›å»ºæ•°æ®é›†æ¨¡æ¿å’Œè½¬æ¢è„šæœ¬:")
    print("- data_template.json: æ•°æ®æ ¼å¼æ¨¡æ¿")
    print("- convert_dataset.py: æ•°æ®è½¬æ¢è„šæœ¬")


def create_memory_monitor():
    """åˆ›å»ºå†…å­˜ç›‘æ§è„šæœ¬"""
    print("\nğŸ’¾ åˆ›å»ºå†…å­˜ç›‘æ§è„šæœ¬")
    print("=" * 40)
    
    monitor_script = '''"""
å†…å­˜ç›‘æ§è„šæœ¬
ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
"""

import psutil
import torch
import time
import matplotlib.pyplot as plt
from collections import deque
import threading


class MemoryMonitor:
    def __init__(self, max_points=100):
        self.max_points = max_points
        self.cpu_memory = deque(maxlen=max_points)
        self.gpu_memory = deque(maxlen=max_points)
        self.timestamps = deque(maxlen=max_points)
        self.monitoring = False
        
    def start_monitoring(self, interval=1.0):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, args=(interval,))
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print("å†…å­˜ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        print("å†…å­˜ç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self, interval):
        """ç›‘æ§å¾ªç¯"""
        start_time = time.time()
        
        while self.monitoring:
            current_time = time.time() - start_time
            
            # CPUå†…å­˜
            cpu_mem = psutil.virtual_memory()
            cpu_usage = cpu_mem.used / (1024**3)  # GB
            
            # GPUå†…å­˜
            gpu_usage = 0
            if torch.cuda.is_available():
                gpu_usage = torch.cuda.memory_allocated() / (1024**3)  # GB
            
            self.cpu_memory.append(cpu_usage)
            self.gpu_memory.append(gpu_usage)
            self.timestamps.append(current_time)
            
            time.sleep(interval)
    
    def plot_memory_usage(self, save_path='memory_usage.png'):
        """ç»˜åˆ¶å†…å­˜ä½¿ç”¨å›¾"""
        if not self.timestamps:
            print("æ²¡æœ‰ç›‘æ§æ•°æ®")
            return
        
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 2, 1)
        plt.plot(list(self.timestamps), list(self.cpu_memory), 'b-', label='CPU Memory')
        plt.xlabel('Time (seconds)')
        plt.ylabel('Memory Usage (GB)')
        plt.title('CPU Memory Usage')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        if torch.cuda.is_available():
            plt.plot(list(self.timestamps), list(self.gpu_memory), 'r-', label='GPU Memory')
            plt.xlabel('Time (seconds)')
            plt.ylabel('Memory Usage (GB)')
            plt.title('GPU Memory Usage')
            plt.legend()
        else:
            plt.text(0.5, 0.5, 'No GPU Available', ha='center', va='center', transform=plt.gca().transAxes)
            plt.title('GPU Memory Usage')
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        print(f"å†…å­˜ä½¿ç”¨å›¾å·²ä¿å­˜åˆ°: {save_path}")
    
    def get_current_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        cpu_mem = psutil.virtual_memory()
        cpu_usage = cpu_mem.used / (1024**3)
        cpu_percent = cpu_mem.percent
        
        gpu_usage = 0
        gpu_percent = 0
        if torch.cuda.is_available():
            gpu_usage = torch.cuda.memory_allocated() / (1024**3)
            gpu_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            gpu_percent = (gpu_usage / gpu_total) * 100 if gpu_total > 0 else 0
        
        return {
            'cpu_usage_gb': cpu_usage,
            'cpu_percent': cpu_percent,
            'gpu_usage_gb': gpu_usage,
            'gpu_percent': gpu_percent
        }


def optimize_memory_usage():
    """å†…å­˜ä¼˜åŒ–å»ºè®®"""
    print("ğŸ’¡ å†…å­˜ä¼˜åŒ–å»ºè®®:")
    print("1. å‡å°‘batch_size")
    print("2. å‡å°‘max_length")
    print("3. ä½¿ç”¨gradient_checkpointing")
    print("4. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    print("5. å®šæœŸæ¸…ç†GPUç¼“å­˜: torch.cuda.empty_cache()")


if __name__ == "__main__":
    monitor = MemoryMonitor()
    
    # æ˜¾ç¤ºå½“å‰å†…å­˜ä½¿ç”¨
    usage = monitor.get_current_usage()
    print(f"å½“å‰å†…å­˜ä½¿ç”¨:")
    print(f"CPU: {usage['cpu_usage_gb']:.1f} GB ({usage['cpu_percent']:.1f}%)")
    print(f"GPU: {usage['gpu_usage_gb']:.1f} GB ({usage['gpu_percent']:.1f}%)")
    
    optimize_memory_usage()
'''
    
    with open('memory_monitor.py', 'w', encoding='utf-8') as f:
        f.write(monitor_script)
    
    print("âœ… å·²åˆ›å»º memory_monitor.py")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ› ï¸  GDM-Net ä¼˜åŒ–æŒ‡å—")
    print("=" * 50)
    
    # æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
    check_system_info()
    
    # åˆ›å»ºGPUç¯å¢ƒé…ç½®
    create_gpu_environment()
    
    # åˆ›å»ºä¼˜åŒ–é…ç½®
    create_optimized_config()
    
    # åˆ›å»ºæ•°æ®é›†æ¨¡æ¿
    create_real_dataset_template()
    
    # åˆ›å»ºå†…å­˜ç›‘æ§
    create_memory_monitor()
    
    print("\nğŸ‰ ä¼˜åŒ–æŒ‡å—åˆ›å»ºå®Œæˆï¼")
    print("\nğŸ“‹ åç»­æ“ä½œ:")
    print("1. GPUåŠ é€Ÿ: å¦‚æœæœ‰GPUï¼Œè¿è¡Œ conda env create -f environment_gpu.yml")
    print("2. å†…å­˜ä¼˜åŒ–: ä½¿ç”¨ config/optimized_config.yaml è®­ç»ƒ")
    print("3. çœŸå®æ•°æ®: å‚è€ƒ data_template.json å‡†å¤‡æ•°æ®ï¼Œä½¿ç”¨ convert_dataset.py è½¬æ¢")
    print("4. å†…å­˜ç›‘æ§: è¿è¡Œ python memory_monitor.py")


if __name__ == "__main__":
    main()
