#!/usr/bin/env python3
"""
é¢„å¤„ç†è„šæœ¬ï¼šå°†HotpotQAæ•°æ®é›†è¿›è¡ŒBERT tokenizationå¹¶ä¿å­˜
æ¶ˆé™¤è®­ç»ƒæ—¶çš„CPUç“¶é¢ˆ
"""

import os
import json
import torch
import pickle
from tqdm import tqdm
from transformers import AutoTokenizer
from typing import Dict, Any, List
import argparse


def preprocess_dataset(
    input_path: str,
    output_dir: str,
    tokenizer_name: str = "models",
    max_length: int = 512,
    max_query_length: int = 64,
    batch_size: int = 64,
    use_gpu: bool = True,
    shard_size: int = 1000
):
    """
    é¢„å¤„ç†æ•°æ®é›†ï¼Œè¿›è¡Œtokenizationå¹¶ä¿å­˜

    Args:
        input_path: åŸå§‹JSONæ•°æ®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        tokenizer_name: tokenizeråç§°æˆ–è·¯å¾„
        max_length: æ–‡æ¡£æœ€å¤§é•¿åº¦
        max_query_length: æŸ¥è¯¢æœ€å¤§é•¿åº¦
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆGPUåŠ é€Ÿç”¨ï¼‰
        use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
        shard_size: æ¯ä¸ªåˆ†ç‰‡çš„æ ·æœ¬æ•°é‡
    """
    
    print(f"ğŸš€ å¼€å§‹é¢„å¤„ç†æ•°æ®é›†: {input_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½tokenizer
    print(f"ğŸ“ åŠ è½½tokenizer: {tokenizer_name}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    
    # åŠ è½½åŸå§‹æ•°æ®
    print(f"ğŸ“– åŠ è½½åŸå§‹æ•°æ®...")
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(data)} æ ·æœ¬")
    
    # ğŸš€ GPUåŠ é€Ÿçš„æ‰¹é‡é¢„å¤„ç†
    tokenized_data = []

    # å‡†å¤‡æ‰¹é‡æ•°æ®
    queries = []
    documents = []
    original_samples = []

    for sample in data:
        query = sample.get('question', '')

        # é€‚é…å®˜æ–¹HotpotQAæ ¼å¼ï¼šcontextæ˜¯[title, sentences]çš„åˆ—è¡¨
        context = sample.get('context', [])
        if isinstance(context, list) and len(context) > 0:
            # å°†contextè½¬æ¢ä¸ºæ–‡æ¡£æ–‡æœ¬
            document_parts = []
            for ctx_item in context:
                if isinstance(ctx_item, list) and len(ctx_item) >= 2:
                    title = ctx_item[0]  # æ–‡æ¡£æ ‡é¢˜
                    sentences = ctx_item[1]  # å¥å­åˆ—è¡¨
                    if isinstance(sentences, list):
                        doc_text = f"{title}. " + " ".join(sentences)
                    else:
                        doc_text = f"{title}. {sentences}"
                    document_parts.append(doc_text)
            document = " ".join(document_parts)
        else:
            document = sample.get('document', '')

        queries.append(query)
        documents.append(document)
        original_samples.append(sample)

    # æ‰¹é‡å¤„ç†
    num_batches = (len(data) + batch_size - 1) // batch_size

    for batch_idx in tqdm(range(num_batches), desc="GPU Tokenizing"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(data))

        batch_queries = queries[start_idx:end_idx]
        batch_documents = documents[start_idx:end_idx]
        batch_originals = original_samples[start_idx:end_idx]

        try:
            # ğŸš€ æ‰¹é‡tokenizeæŸ¥è¯¢ï¼ˆGPUåŠ é€Ÿï¼‰
            query_tokens = tokenizer(
                batch_queries,
                max_length=max_query_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            # ğŸš€ æ‰¹é‡tokenizeæ–‡æ¡£ï¼ˆGPUåŠ é€Ÿï¼‰
            doc_tokens = tokenizer(
                batch_documents,
                max_length=max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            # å¦‚æœä½¿ç”¨GPUï¼Œå°†å¼ é‡ç§»åˆ°GPU
            if use_gpu and torch.cuda.is_available():
                query_tokens = {k: v.to(device) for k, v in query_tokens.items()}
                doc_tokens = {k: v.to(device) for k, v in doc_tokens.items()}

            # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
            for i in range(len(batch_queries)):
                tokenized_sample = {
                    'query_input_ids': query_tokens['input_ids'][i].cpu(),
                    'query_attention_mask': query_tokens['attention_mask'][i].cpu(),
                    'doc_input_ids': doc_tokens['input_ids'][i].cpu(),
                    'doc_attention_mask': doc_tokens['attention_mask'][i].cpu(),
                    'original_sample': batch_originals[i]
                }
                tokenized_data.append(tokenized_sample)

        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ‰¹æ¬¡ {batch_idx} æ—¶å‡ºé”™: {e}")
            # å›é€€åˆ°å•ä¸ªæ ·æœ¬å¤„ç†
            for i, (query, document, original) in enumerate(zip(batch_queries, batch_documents, batch_originals)):
                try:
                    # å¤„ç†å•ä¸ªæ ·æœ¬çš„contextæ ¼å¼
                    context = original.get('context', [])
                    if isinstance(context, list) and len(context) > 0:
                        document_parts = []
                        for ctx_item in context:
                            if isinstance(ctx_item, list) and len(ctx_item) >= 2:
                                title = ctx_item[0]
                                sentences = ctx_item[1]
                                if isinstance(sentences, list):
                                    doc_text = f"{title}. " + " ".join(sentences)
                                else:
                                    doc_text = f"{title}. {sentences}"
                                document_parts.append(doc_text)
                        document = " ".join(document_parts)
                    else:
                        document = original.get('document', '')

                    query_tokens = tokenizer(query, max_length=max_query_length, padding='max_length', truncation=True, return_tensors='pt')
                    doc_tokens = tokenizer(document, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')

                    tokenized_sample = {
                        'query_input_ids': query_tokens['input_ids'].squeeze(0),
                        'query_attention_mask': query_tokens['attention_mask'].squeeze(0),
                        'doc_input_ids': doc_tokens['input_ids'].squeeze(0),
                        'doc_attention_mask': doc_tokens['attention_mask'].squeeze(0),
                        'original_sample': original
                    }
                    tokenized_data.append(tokenized_sample)
                except Exception as e2:
                    print(f"âš ï¸  è·³è¿‡æ ·æœ¬ {start_idx + i}: {e2}")
                    continue
    
    # ğŸš€ åˆ†ç‰‡ä¿å­˜é¢„å¤„ç†ç»“æœ
    base_name = os.path.basename(input_path).replace('.json', '')
    num_shards = (len(tokenized_data) + shard_size - 1) // shard_size

    print(f"ğŸ’¾ åˆ†ç‰‡ä¿å­˜é¢„å¤„ç†ç»“æœ: {num_shards} ä¸ªåˆ†ç‰‡ï¼Œæ¯ç‰‡ {shard_size} æ ·æœ¬")

    shard_files = []
    for shard_idx in range(num_shards):
        start_idx = shard_idx * shard_size
        end_idx = min(start_idx + shard_size, len(tokenized_data))
        shard_data = tokenized_data[start_idx:end_idx]

        shard_file = os.path.join(output_dir, f"tokenized_{base_name}_shard_{shard_idx:04d}.pkl")
        with open(shard_file, 'wb') as f:
            pickle.dump(shard_data, f)

        shard_files.append(shard_file)
        print(f"   ä¿å­˜åˆ†ç‰‡ {shard_idx+1}/{num_shards}: {len(shard_data)} æ ·æœ¬ -> {os.path.basename(shard_file)}")

    # ä¿å­˜åˆ†ç‰‡ç´¢å¼•å…ƒæ•°æ®
    metadata = {
        'num_samples': len(tokenized_data),
        'num_shards': num_shards,
        'shard_size': shard_size,
        'shard_files': [os.path.basename(f) for f in shard_files],
        'tokenizer_name': tokenizer_name,
        'max_length': max_length,
        'max_query_length': max_query_length,
        'original_file': input_path
    }

    metadata_file = os.path.join(output_dir, f"sharded_metadata_{base_name}.json")
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"âœ… åˆ†ç‰‡é¢„å¤„ç†å®Œæˆ!")
    print(f"   - æ€»æ ·æœ¬æ•°: {len(tokenized_data)}")
    print(f"   - åˆ†ç‰‡æ•°é‡: {num_shards}")
    print(f"   - æ¯ç‰‡å¤§å°: {shard_size}")
    print(f"   - å…ƒæ•°æ®æ–‡ä»¶: {os.path.basename(metadata_file)}")


def main():
    parser = argparse.ArgumentParser(description="é¢„å¤„ç†HotpotQAæ•°æ®é›†è¿›è¡Œtokenization")
    parser.add_argument("--input", required=True, help="è¾“å…¥JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", default="/root/autodl-tmp/hotpotqa-pretokenized", 
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--tokenizer", default="models", help="Tokenizeråç§°æˆ–è·¯å¾„")
    parser.add_argument("--max_length", type=int, default=512, help="æ–‡æ¡£æœ€å¤§é•¿åº¦")
    parser.add_argument("--max_query_length", type=int, default=64, help="æŸ¥è¯¢æœ€å¤§é•¿åº¦")
    parser.add_argument("--batch_size", type=int, default=64, help="æ‰¹å¤„ç†å¤§å°ï¼ˆGPUåŠ é€Ÿï¼‰")
    parser.add_argument("--no_gpu", action="store_true", help="ç¦ç”¨GPUåŠ é€Ÿ")
    parser.add_argument("--shard_size", type=int, default=1000, help="æ¯ä¸ªåˆ†ç‰‡çš„æ ·æœ¬æ•°é‡")
    
    args = parser.parse_args()
    
    preprocess_dataset(
        input_path=args.input,
        output_dir=args.output_dir,
        tokenizer_name=args.tokenizer,
        max_length=args.max_length,
        max_query_length=args.max_query_length,
        batch_size=args.batch_size,
        use_gpu=not args.no_gpu,
        shard_size=args.shard_size
    )


if __name__ == "__main__":
    main()
