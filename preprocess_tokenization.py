#!/usr/bin/env python3
"""
é¢„å¤„ç†è„šæœ¬ï¼šå°†HotpotQAæ•°æ®é›†è¿›è¡ŒBERT tokenizationå¹¶ä¿å­˜
æ¶ˆé™¤è®­ç»ƒæ—¶çš„CPUç“¶é¢ˆ
"""

import os
import json
import torch
import pickle
from tqdm import tqdm
from transformers import AutoTokenizer
from typing import Dict, Any, List
import argparse


def preprocess_dataset(
    input_path: str,
    output_dir: str,
    tokenizer_name: str = "models",
    max_length: int = 512,
    max_query_length: int = 64,
    batch_size: int = 64,
    use_gpu: bool = True
):
    """
    é¢„å¤„ç†æ•°æ®é›†ï¼Œè¿›è¡Œtokenizationå¹¶ä¿å­˜

    Args:
        input_path: åŸå§‹JSONæ•°æ®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        tokenizer_name: tokenizeråç§°æˆ–è·¯å¾„
        max_length: æ–‡æ¡£æœ€å¤§é•¿åº¦
        max_query_length: æŸ¥è¯¢æœ€å¤§é•¿åº¦
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆGPUåŠ é€Ÿç”¨ï¼‰
        use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
    """
    
    print(f"ğŸš€ å¼€å§‹é¢„å¤„ç†æ•°æ®é›†: {input_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # åŠ è½½tokenizer
    print(f"ğŸ“ åŠ è½½tokenizer: {tokenizer_name}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    
    # åŠ è½½åŸå§‹æ•°æ®
    print(f"ğŸ“– åŠ è½½åŸå§‹æ•°æ®...")
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(data)} æ ·æœ¬")
    
    # ğŸš€ GPUåŠ é€Ÿçš„æ‰¹é‡é¢„å¤„ç†
    tokenized_data = []

    # å‡†å¤‡æ‰¹é‡æ•°æ®
    queries = []
    documents = []
    original_samples = []

    for sample in data:
        query = sample.get('question', '')
        if isinstance(sample.get('context'), list):
            document = ' '.join(sample.get('context', []))
        else:
            document = sample.get('document', '')

        queries.append(query)
        documents.append(document)
        original_samples.append(sample)

    # æ‰¹é‡å¤„ç†
    num_batches = (len(data) + batch_size - 1) // batch_size

    for batch_idx in tqdm(range(num_batches), desc="GPU Tokenizing"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(data))

        batch_queries = queries[start_idx:end_idx]
        batch_documents = documents[start_idx:end_idx]
        batch_originals = original_samples[start_idx:end_idx]

        try:
            # ğŸš€ æ‰¹é‡tokenizeæŸ¥è¯¢ï¼ˆGPUåŠ é€Ÿï¼‰
            query_tokens = tokenizer(
                batch_queries,
                max_length=max_query_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            # ğŸš€ æ‰¹é‡tokenizeæ–‡æ¡£ï¼ˆGPUåŠ é€Ÿï¼‰
            doc_tokens = tokenizer(
                batch_documents,
                max_length=max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            # å¦‚æœä½¿ç”¨GPUï¼Œå°†å¼ é‡ç§»åˆ°GPU
            if use_gpu and torch.cuda.is_available():
                query_tokens = {k: v.to(device) for k, v in query_tokens.items()}
                doc_tokens = {k: v.to(device) for k, v in doc_tokens.items()}

            # å¤„ç†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬
            for i in range(len(batch_queries)):
                tokenized_sample = {
                    'query_input_ids': query_tokens['input_ids'][i].cpu(),
                    'query_attention_mask': query_tokens['attention_mask'][i].cpu(),
                    'doc_input_ids': doc_tokens['input_ids'][i].cpu(),
                    'doc_attention_mask': doc_tokens['attention_mask'][i].cpu(),
                    'original_sample': batch_originals[i]
                }
                tokenized_data.append(tokenized_sample)

        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ‰¹æ¬¡ {batch_idx} æ—¶å‡ºé”™: {e}")
            # å›é€€åˆ°å•ä¸ªæ ·æœ¬å¤„ç†
            for i, (query, document, original) in enumerate(zip(batch_queries, batch_documents, batch_originals)):
                try:
                    query_tokens = tokenizer(query, max_length=max_query_length, padding='max_length', truncation=True, return_tensors='pt')
                    doc_tokens = tokenizer(document, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')

                    tokenized_sample = {
                        'query_input_ids': query_tokens['input_ids'].squeeze(0),
                        'query_attention_mask': query_tokens['attention_mask'].squeeze(0),
                        'doc_input_ids': doc_tokens['input_ids'].squeeze(0),
                        'doc_attention_mask': doc_tokens['attention_mask'].squeeze(0),
                        'original_sample': original
                    }
                    tokenized_data.append(tokenized_sample)
                except Exception as e2:
                    print(f"âš ï¸  è·³è¿‡æ ·æœ¬ {start_idx + i}: {e2}")
                    continue
    
    # ä¿å­˜é¢„å¤„ç†ç»“æœ
    output_file = os.path.join(output_dir, f"tokenized_{os.path.basename(input_path).replace('.json', '.pkl')}")
    
    print(f"ğŸ’¾ ä¿å­˜é¢„å¤„ç†ç»“æœåˆ°: {output_file}")
    with open(output_file, 'wb') as f:
        pickle.dump(tokenized_data, f)
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'num_samples': len(tokenized_data),
        'tokenizer_name': tokenizer_name,
        'max_length': max_length,
        'max_query_length': max_query_length,
        'original_file': input_path
    }
    
    metadata_file = os.path.join(output_dir, f"metadata_{os.path.basename(input_path).replace('.json', '.json')}")
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"âœ… é¢„å¤„ç†å®Œæˆ!")
    print(f"   - å¤„ç†æ ·æœ¬æ•°: {len(tokenized_data)}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"   - å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")


def main():
    parser = argparse.ArgumentParser(description="é¢„å¤„ç†HotpotQAæ•°æ®é›†è¿›è¡Œtokenization")
    parser.add_argument("--input", required=True, help="è¾“å…¥JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", default="/root/autodl-tmp/hotpotqa-pretokenized", 
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--tokenizer", default="models", help="Tokenizeråç§°æˆ–è·¯å¾„")
    parser.add_argument("--max_length", type=int, default=512, help="æ–‡æ¡£æœ€å¤§é•¿åº¦")
    parser.add_argument("--max_query_length", type=int, default=64, help="æŸ¥è¯¢æœ€å¤§é•¿åº¦")
    parser.add_argument("--batch_size", type=int, default=64, help="æ‰¹å¤„ç†å¤§å°ï¼ˆGPUåŠ é€Ÿï¼‰")
    parser.add_argument("--no_gpu", action="store_true", help="ç¦ç”¨GPUåŠ é€Ÿ")
    
    args = parser.parse_args()
    
    preprocess_dataset(
        input_path=args.input,
        output_dir=args.output_dir,
        tokenizer_name=args.tokenizer,
        max_length=args.max_length,
        max_query_length=args.max_query_length,
        batch_size=args.batch_size,
        use_gpu=not args.no_gpu
    )


if __name__ == "__main__":
    main()
