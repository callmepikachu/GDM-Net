"""
ä¸‹è½½å’Œè½¬æ¢æ¨èæ•°æ®é›†çš„è„šæœ¬
æ”¯æŒ HotpotQA, DocRED, 2WikiMultiHopQA, FEVER ç­‰æ•°æ®é›†
"""

import os
import json
import requests
import zipfile
import tarfile
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from datasets import load_dataset
from tqdm import tqdm


class DatasetDownloader:
    def __init__(self, data_dir="datasets"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
    def download_hotpotqa(self):
        """ä¸‹è½½ HotpotQA æ•°æ®é›†"""
        print("ğŸ“¥ ä¸‹è½½ HotpotQA æ•°æ®é›†...")
        
        try:
            # ä½¿ç”¨ Hugging Face datasets åº“
            dataset = load_dataset("hotpot_qa", "fullwiki")
            
            # è½¬æ¢è®­ç»ƒé›†
            train_data = self._convert_hotpotqa(dataset['train'])
            self._save_json(train_data[:8000], "data/hotpotqa_train.json")
            
            # è½¬æ¢éªŒè¯é›†
            val_data = self._convert_hotpotqa(dataset['validation'])
            self._save_json(val_data[:1000], "data/hotpotqa_val.json")
            
            print(f"âœ… HotpotQA ä¸‹è½½å®Œæˆ: {len(train_data)} è®­ç»ƒæ ·æœ¬, {len(val_data)} éªŒè¯æ ·æœ¬")
            return True
            
        except Exception as e:
            print(f"âŒ HotpotQA ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def _convert_hotpotqa(self, dataset) -> List[Dict]:
        """è½¬æ¢ HotpotQA æ ¼å¼"""
        converted = []
        
        for item in tqdm(dataset, desc="è½¬æ¢ HotpotQA"):
            # åˆå¹¶æ‰€æœ‰ä¸Šä¸‹æ–‡æ–‡æ¡£
            documents = []
            for title, sentences in item['context']:
                doc_text = ' '.join(sentences)
                documents.append(f"{title}: {doc_text}")
            
            combined_doc = ' '.join(documents)
            
            # æå–å®ä½“ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            entities = []
            entity_id = 0
            
            # ä»æ”¯æ’‘äº‹å®ä¸­æå–å®ä½“
            for fact in item['supporting_facts']:
                title = fact[0]
                start_pos = combined_doc.find(title)
                if start_pos != -1:
                    entities.append({
                        "span": [start_pos, start_pos + len(title)],
                        "type": "ENTITY",
                        "text": title
                    })
                    entity_id += 1
            
            # åˆ›å»ºå…³ç³»ï¼ˆåŸºäºæ”¯æ’‘äº‹å®ï¼‰
            relations = []
            if len(entities) >= 2:
                relations.append({
                    "head": 0,
                    "tail": 1,
                    "type": "SUPPORTS"
                })
            
            # ç­”æ¡ˆç±»å‹åˆ†ç±»
            answer_type = 0  # yes/no
            if item['answer'].lower() in ['yes', 'no']:
                answer_type = 1 if item['answer'].lower() == 'yes' else 0
            else:
                answer_type = 2  # å…¶ä»–ç­”æ¡ˆ
            
            converted.append({
                "document": combined_doc[:2000],  # é™åˆ¶é•¿åº¦
                "query": item['question'],
                "entities": entities[:10],  # é™åˆ¶å®ä½“æ•°é‡
                "relations": relations,
                "label": answer_type,
                "metadata": {
                    "source": "hotpotqa",
                    "answer": item['answer'],
                    "supporting_facts": item['supporting_facts']
                }
            })
        
        return converted
    
    def download_docred(self):
        """ä¸‹è½½ DocRED æ•°æ®é›†"""
        print("ğŸ“¥ ä¸‹è½½ DocRED æ•°æ®é›†...")
        
        urls = {
            "train": "https://github.com/thunlp/DocRED/raw/master/data/train_annotated.json",
            "dev": "https://github.com/thunlp/DocRED/raw/master/data/dev.json"
        }
        
        try:
            for split, url in urls.items():
                print(f"ä¸‹è½½ {split} æ•°æ®...")
                response = requests.get(url)
                response.raise_for_status()
                
                data = response.json()
                converted = self._convert_docred(data)
                
                if split == "train":
                    self._save_json(converted[:5000], "data/docred_train.json")
                else:
                    self._save_json(converted[:500], "data/docred_val.json")
            
            print("âœ… DocRED ä¸‹è½½å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ DocRED ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def _convert_docred(self, dataset) -> List[Dict]:
        """è½¬æ¢ DocRED æ ¼å¼"""
        converted = []
        
        for item in tqdm(dataset, desc="è½¬æ¢ DocRED"):
            # åˆå¹¶å¥å­
            document = ' '.join([' '.join(sent) for sent in item['sents']])
            
            # è½¬æ¢å®ä½“
            entities = []
            for entity_group in item['vertexSet']:
                # å–ç¬¬ä¸€ä¸ªæåŠ
                mention = entity_group[0]
                entities.append({
                    "span": mention['pos'],
                    "type": mention.get('type', 'MISC'),
                    "text": mention['name']
                })
            
            # è½¬æ¢å…³ç³»
            relations = []
            for relation in item.get('labels', []):
                relations.append({
                    "head": relation['h'],
                    "tail": relation['t'],
                    "type": f"R{relation['r']}"  # å…³ç³»IDè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                })
            
            converted.append({
                "document": document[:2000],
                "query": f"What relations exist in this document about {item.get('title', 'the topic')}?",
                "entities": entities[:15],
                "relations": relations[:10],
                "label": min(len(relations), 4),  # åŸºäºå…³ç³»æ•°é‡çš„ç®€å•åˆ†ç±»
                "metadata": {
                    "source": "docred",
                    "title": item.get('title', '')
                }
            })
        
        return converted
    
    def download_2wikimultihopqa(self):
        """ä¸‹è½½ 2WikiMultiHopQA æ•°æ®é›†"""
        print("ğŸ“¥ ä¸‹è½½ 2WikiMultiHopQA æ•°æ®é›†...")
        
        try:
            dataset = load_dataset("2WikiMultihopQA", "2WikiMultihopQA")
            
            # è½¬æ¢æ•°æ®
            train_data = self._convert_2wiki(dataset['train'])
            val_data = self._convert_2wiki(dataset['validation'])
            
            self._save_json(train_data[:6000], "data/2wiki_train.json")
            self._save_json(val_data[:600], "data/2wiki_val.json")
            
            print(f"âœ… 2WikiMultiHopQA ä¸‹è½½å®Œæˆ: {len(train_data)} è®­ç»ƒæ ·æœ¬")
            return True
            
        except Exception as e:
            print(f"âŒ 2WikiMultiHopQA ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def _convert_2wiki(self, dataset) -> List[Dict]:
        """è½¬æ¢ 2WikiMultiHopQA æ ¼å¼"""
        converted = []
        
        for item in tqdm(dataset, desc="è½¬æ¢ 2WikiMultiHopQA"):
            # åˆå¹¶ä¸Šä¸‹æ–‡
            context_text = ""
            entities = []
            entity_id = 0
            
            for context in item['context']:
                title = context[0]
                content = ' '.join(context[1])
                context_text += f"{title}: {content} "
                
                # æ·»åŠ æ ‡é¢˜ä½œä¸ºå®ä½“
                start_pos = len(context_text) - len(content) - len(title) - 2
                entities.append({
                    "span": [start_pos, start_pos + len(title)],
                    "type": "ENTITY",
                    "text": title
                })
                entity_id += 1
            
            # åˆ›å»ºå…³ç³»
            relations = []
            if len(entities) >= 2:
                relations.append({
                    "head": 0,
                    "tail": 1,
                    "type": "RELATED"
                })
            
            converted.append({
                "document": context_text[:2000],
                "query": item['question'],
                "entities": entities[:10],
                "relations": relations,
                "label": 0,  # ç®€åŒ–åˆ†ç±»
                "metadata": {
                    "source": "2wikimultihopqa",
                    "answer": item['answer']
                }
            })
        
        return converted
    
    def download_fever(self):
        """ä¸‹è½½ FEVER æ•°æ®é›†"""
        print("ğŸ“¥ ä¸‹è½½ FEVER æ•°æ®é›†...")
        
        try:
            dataset = load_dataset("fever", "v1.0")
            
            # åªä½¿ç”¨æœ‰æ ‡ç­¾çš„æ•°æ®
            train_data = [item for item in dataset['train'] if item['label'] != 'NOT ENOUGH INFO']
            val_data = [item for item in dataset['validation'] if item['label'] != 'NOT ENOUGH INFO']
            
            converted_train = self._convert_fever(train_data)
            converted_val = self._convert_fever(val_data)
            
            self._save_json(converted_train[:8000], "data/fever_train.json")
            self._save_json(converted_val[:800], "data/fever_val.json")
            
            print(f"âœ… FEVER ä¸‹è½½å®Œæˆ: {len(converted_train)} è®­ç»ƒæ ·æœ¬")
            return True
            
        except Exception as e:
            print(f"âŒ FEVER ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def _convert_fever(self, dataset) -> List[Dict]:
        """è½¬æ¢ FEVER æ ¼å¼"""
        converted = []
        
        for item in tqdm(dataset, desc="è½¬æ¢ FEVER"):
            # åˆå¹¶è¯æ®
            evidence_text = ""
            entities = []
            
            if item['evidence']:
                for evidence_set in item['evidence']:
                    for evidence in evidence_set:
                        if len(evidence) >= 3:
                            page_title = evidence[2]
                            evidence_text += f"{page_title}. "
                            
                            # æ·»åŠ é¡µé¢æ ‡é¢˜ä½œä¸ºå®ä½“
                            start_pos = len(evidence_text) - len(page_title) - 2
                            entities.append({
                                "span": [start_pos, start_pos + len(page_title)],
                                "type": "EVIDENCE",
                                "text": page_title
                            })
            
            # æ ‡ç­¾æ˜ å°„
            label_map = {"SUPPORTS": 1, "REFUTES": 0}
            label = label_map.get(item['label'], 2)
            
            converted.append({
                "document": evidence_text[:1500],
                "query": item['claim'],
                "entities": entities[:8],
                "relations": [],
                "label": label,
                "metadata": {
                    "source": "fever",
                    "original_label": item['label']
                }
            })
        
        return converted
    
    def _save_json(self, data: List[Dict], filepath: str):
        """ä¿å­˜JSONæ–‡ä»¶"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å·²ä¿å­˜: {filepath} ({len(data)} æ ·æœ¬)")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ GDM-Net æ•°æ®é›†ä¸‹è½½å™¨")
    print("=" * 50)
    
    downloader = DatasetDownloader()
    
    datasets = {
        "1": ("HotpotQA", downloader.download_hotpotqa),
        "2": ("DocRED", downloader.download_docred),
        "3": ("2WikiMultiHopQA", downloader.download_2wikimultihopqa),
        "4": ("FEVER", downloader.download_fever),
        "5": ("å…¨éƒ¨ä¸‹è½½", lambda: all([
            downloader.download_hotpotqa(),
            downloader.download_docred(),
            downloader.download_2wikimultihopqa(),
            downloader.download_fever()
        ]))
    }
    
    print("è¯·é€‰æ‹©è¦ä¸‹è½½çš„æ•°æ®é›†:")
    for key, (name, _) in datasets.items():
        print(f"{key}. {name}")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
    
    if choice in datasets:
        name, download_func = datasets[choice]
        print(f"\nå¼€å§‹ä¸‹è½½ {name}...")
        
        try:
            success = download_func()
            if success:
                print(f"\nğŸ‰ {name} ä¸‹è½½å®Œæˆï¼")
                print("\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
                print("1. æ£€æŸ¥ data/ ç›®å½•ä¸­çš„æ–‡ä»¶")
                print("2. ä½¿ç”¨ä¼˜åŒ–é…ç½®è®­ç»ƒ:")
                print("   python train/train.py --config config/optimized_config.yaml --mode train")
            else:
                print(f"\nâŒ {name} ä¸‹è½½å¤±è´¥")
        except Exception as e:
            print(f"\nâŒ ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    main()
