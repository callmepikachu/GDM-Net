"""
è¿è¡Œ GDM-Net ç¤ºä¾‹çš„ç®€å•è„šæœ¬
"""

import torch
import torch.nn.functional as F
from transformers import BertTokenizer
from gdmnet import GDMNet


def run_inference_example():
    """è¿è¡Œæ¨ç†ç¤ºä¾‹"""
    print("ğŸ§  GDM-Net æ¨ç†ç¤ºä¾‹")
    print("=" * 40)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ“¦ åˆå§‹åŒ–æ¨¡å‹...")
    model = GDMNet(
        bert_model_name='bert-base-uncased',
        hidden_size=768,
        num_entities=10,
        num_relations=20,
        num_classes=5
    )
    model.eval()
    
    # åˆå§‹åŒ–åˆ†è¯å™¨
    print("ğŸ“ åˆå§‹åŒ–åˆ†è¯å™¨...")
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    
    # ç¤ºä¾‹æ–‡æ¡£å’ŒæŸ¥è¯¢
    examples = [
        {
            "document": "Apple Inc. is a technology company founded by Steve Jobs. Tim Cook is the current CEO.",
            "query": "Who is the CEO of Apple?"
        },
        {
            "document": "Microsoft Corporation was founded by Bill Gates. Satya Nadella is the current CEO.",
            "query": "Who founded Microsoft?"
        },
        {
            "document": "Tesla Inc. was founded by Elon Musk. The company focuses on electric vehicles.",
            "query": "What does Tesla focus on?"
        }
    ]
    
    print("ğŸ” å¼€å§‹æ¨ç†...")
    
    for i, example in enumerate(examples, 1):
        print(f"\n--- ç¤ºä¾‹ {i} ---")
        print(f"æ–‡æ¡£: {example['document']}")
        print(f"æŸ¥è¯¢: {example['query']}")
        
        # åˆ†è¯
        doc_encoding = tokenizer(
            example['document'],
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        query_encoding = tokenizer(
            example['query'],
            max_length=64,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(
                input_ids=doc_encoding['input_ids'],
                attention_mask=doc_encoding['attention_mask'],
                query=query_encoding['input_ids'],
                return_intermediate=True
            )
        
        # è·å–ç»“æœ
        logits = outputs['logits']
        probabilities = F.softmax(logits, dim=-1)
        prediction = torch.argmax(logits, dim=-1)
        
        print(f"é¢„æµ‹ç±»åˆ«: {prediction.item()}")
        print(f"ç½®ä¿¡åº¦: {probabilities.max().item():.3f}")
        print(f"æå–çš„å®ä½“æ•°é‡: {len(outputs['entities'][0])}")
        print(f"æå–çš„å…³ç³»æ•°é‡: {len(outputs['relations'][0])}")
        
        # æ˜¾ç¤ºæå–çš„å®ä½“ï¼ˆå¦‚æœæœ‰ï¼‰
        if outputs['entities'][0]:
            print("æå–çš„å®ä½“:")
            for j, entity in enumerate(outputs['entities'][0][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  - å®ä½“ {j+1}: ä½ç½® {entity['start']}-{entity['end']}, ç±»å‹ {entity['type']}, ç½®ä¿¡åº¦ {entity['confidence']:.3f}")
        
        # æ˜¾ç¤ºæå–çš„å…³ç³»ï¼ˆå¦‚æœæœ‰ï¼‰
        if outputs['relations'][0]:
            print("æå–çš„å…³ç³»:")
            for j, relation in enumerate(outputs['relations'][0][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"  - å…³ç³» {j+1}: å¤´å®ä½“ {relation['head']}, å°¾å®ä½“ {relation['tail']}, ç±»å‹ {relation['type']}, ç½®ä¿¡åº¦ {relation['confidence']:.3f}")


def check_training_progress():
    """æ£€æŸ¥è®­ç»ƒè¿›åº¦"""
    import os
    import glob
    
    print("\nğŸ“Š æ£€æŸ¥è®­ç»ƒè¿›åº¦...")
    
    # æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_files = glob.glob("checkpoints/*.ckpt")
    if checkpoint_files:
        print(f"æ‰¾åˆ° {len(checkpoint_files)} ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶:")
        for ckpt in checkpoint_files:
            size = os.path.getsize(ckpt) / (1024 * 1024)  # MB
            print(f"  - {os.path.basename(ckpt)} ({size:.1f} MB)")
    else:
        print("æš‚æ— æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆè®­ç»ƒå¯èƒ½ä»åœ¨è¿›è¡Œä¸­ï¼‰")
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
    log_dirs = glob.glob("logs/gdmnet/version_*")
    if log_dirs:
        print(f"æ‰¾åˆ° {len(log_dirs)} ä¸ªæ—¥å¿—ç›®å½•:")
        for log_dir in log_dirs:
            print(f"  - {log_dir}")
        print("\nğŸ’¡ æç¤º: è¿è¡Œ 'tensorboard --logdir logs/' æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
    else:
        print("æš‚æ— æ—¥å¿—æ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # è¿è¡Œæ¨ç†ç¤ºä¾‹
        run_inference_example()
        
        # æ£€æŸ¥è®­ç»ƒè¿›åº¦
        check_training_progress()
        
        print("\nğŸ‰ ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ“‹ åç»­æ“ä½œ:")
        print("1. ç­‰å¾…è®­ç»ƒå®Œæˆ")
        print("2. è¿è¡Œ 'tensorboard --logdir logs/' æŸ¥çœ‹è®­ç»ƒæ›²çº¿")
        print("3. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œç¤ºä¾‹æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
