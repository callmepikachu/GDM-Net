# High-Performance Single GPU Configuration
# Optimized for 48GB GPU memory - full model capacity

seed: 42

model:
  bert_model_name: "bert-base-uncased"
  hidden_size: 768  # å®Œæ•´BERTéšè—å±‚å¤§å°
  num_entities: 9
  num_relations: 10
  num_classes: 5
  gnn_type: "rgcn"
  num_gnn_layers: 3  # å¢åŠ GNNå±‚æ•°ä»¥æå‡æ€§èƒ½
  num_reasoning_hops: 4  # å¢åŠ æ¨ç†è·³æ•°
  fusion_method: "gate"
  dropout_rate: 0.2  # å¢åŠ dropoutå‡å°‘è¿‡æ‹Ÿåˆ
  freeze_bert: true  # å†»ç»“BERTå‚æ•°ï¼Œåªè®­ç»ƒä¸‹æ¸¸ä»»åŠ¡å±‚
  entity_loss_weight: 0.5  # å¢åŠ è¾…åŠ©æŸå¤±æƒé‡
  relation_loss_weight: 0.5  # å¢åŠ è¾…åŠ©æŸå¤±æƒé‡

data:
  train_path: "/root/autodl-fs/hotpot_train_v1.1.json"     # è®­ç»ƒé›†è·¯å¾„ (90,447æ ·æœ¬)
  val_path: "/root/autodl-fs/hotpot_dev_distractor_v1.json" # éªŒè¯é›†è·¯å¾„ (7,405æ ·æœ¬)
  test_path: "/root/autodl-fs/dev_fullwiki.json"           # æµ‹è¯•é›†è·¯å¾„
  pretokenized_dir: "/autodl-tmp/hotpotqa-pretokenized"    # ğŸš€ é¢„å¤„ç†ç¼“å­˜ç›®å½•
  max_length: 2048  # ğŸš€ æ‰©å¤§åˆ°2048åºåˆ—é•¿åº¦
  max_query_length: 256  # ğŸš€ ç›¸åº”æ‰©å¤§queryé•¿åº¦

training:
  max_epochs: 30  # è¿›ä¸€æ­¥å¢åŠ è®­ç»ƒè½®æ•°
  batch_size: 4   # ğŸš€ å‡å°batch sizeé€‚åº”2048åºåˆ—é•¿åº¦
  learning_rate: 1e-4  # è°ƒæ•´å­¦ä¹ ç‡å¹³è¡¡æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§
  num_workers: 16  # æœ€å¤§åŒ–æ•°æ®åŠ è½½å¹¶è¡Œåº¦
  accelerator: "gpu"
  devices: 1
  precision: 32
  gradient_clip_val: 0.5  # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
  accumulate_grad_batches: 2  # ğŸš€ å¢åŠ æ¢¯åº¦ç´¯ç§¯ä¿æŒæœ‰æ•ˆbatch size (4*2=8)
  val_check_interval: 0.5
  log_every_n_steps: 10  # è¿›ä¸€æ­¥å‡å°‘æ—¥å¿—é¢‘ç‡
  checkpoint_dir: "checkpoints"
  early_stopping: true
  patience: 15  # å¢åŠ è€å¿ƒå€¼é€‚åº”æ›´é•¿è®­ç»ƒ

logging:
  type: "tensorboard"
  save_dir: "logs"
  name: "gdmnet-high-performance"
