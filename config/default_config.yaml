# High-Performance Single GPU Configuration
# Optimized for 48GB GPU memory - full model capacity

seed: 42

model:
  bert_model_name: "bert-base-uncased"
  hidden_size: 768  # 完整BERT隐藏层大小
  num_entities: 9
  num_relations: 10
  num_classes: 5
  gnn_type: "rgcn"
  num_gnn_layers: 3  # 增加GNN层数以提升性能
  num_reasoning_hops: 4  # 增加推理跳数
  fusion_method: "gate"
  dropout_rate: 0.2  # 增加dropout减少过拟合
  freeze_bert: true  # 冻结BERT参数，只训练下游任务层
  entity_loss_weight: 0.5  # 增加辅助损失权重
  relation_loss_weight: 0.5  # 增加辅助损失权重

data:
  train_path: "/root/autodl-fs/hotpot_train_v1.1.json"     # 训练集路径 (90,447样本)
  val_path: "/root/autodl-fs/hotpot_dev_distractor_v1.json" # 验证集路径 (7,405样本)
  test_path: "/root/autodl-fs/dev_fullwiki.json"           # 测试集路径
  pretokenized_dir: "/autodl-tmp/hotpotqa-pretokenized"    # 🚀 预处理缓存目录
  max_length: 2048  # 🚀 扩大到2048序列长度
  max_query_length: 256  # 🚀 相应扩大query长度

training:
  max_epochs: 30  # 进一步增加训练轮数
  batch_size: 4   # 🚀 减小batch size适应2048序列长度
  learning_rate: 1e-4  # 调整学习率平衡收敛速度和稳定性
  num_workers: 16  # 最大化数据加载并行度
  accelerator: "gpu"
  devices: 1
  precision: 32
  gradient_clip_val: 0.5  # 更严格的梯度裁剪
  accumulate_grad_batches: 2  # 🚀 增加梯度累积保持有效batch size (4*2=8)
  val_check_interval: 0.5
  log_every_n_steps: 10  # 进一步减少日志频率
  checkpoint_dir: "checkpoints"
  early_stopping: true
  patience: 15  # 增加耐心值适应更长训练

logging:
  type: "tensorboard"
  save_dir: "logs"
  name: "gdmnet-high-performance"
