# ğŸ§  GDM-Net: Graph-Augmented Dual Memory Network

A neural architecture that combines explicit graph memory with dual-path processing for structured information extraction and multi-hop reasoning over documents.

## ğŸ“‹ ç›®å½•

1. [ç®€ä»‹](#ç®€ä»‹)
2. [æ¨¡å—æ¶æ„](#æ¨¡å—æ¶æ„)
3. [è¿è¡Œæµæ°´çº¿](#è¿è¡Œæµæ°´çº¿)
4. [å®‰è£…ä¾èµ–](#å®‰è£…ä¾èµ–)
5. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
6. [è®­ç»ƒæŒ‡å—](#è®­ç»ƒæŒ‡å—)
7. [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)

---

## ğŸ“– ç®€ä»‹

GDM-Net æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯å¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä¸“ä¸ºå¤šæ–‡æ¡£ç†è§£ä»»åŠ¡è®¾è®¡ï¼Œå¦‚å…³ç³»æŠ½å–ã€é—®ç­”å’Œæ¨ç†ã€‚å…¶æ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ï¼š

- âœ¨ **æ˜¾å¼å›¾å†…å­˜è¯»å†™**ï¼šåˆ©ç”¨å›¾ç»“æ„ï¼ˆå¦‚çŸ¥è¯†å›¾è°±ï¼‰ä½œä¸ºå¤–éƒ¨è®°å¿†
- ğŸ”— **åŒé€šè·¯å¤„ç†**ï¼šåŒæ—¶æ”¯æŒç»“æ„æŠ½å–å’Œè·¯å¾„æ¨ç†
- âš™ï¸ **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šæ‰€æœ‰æ¨¡å—å¯å¾®åˆ†ï¼Œæ”¯æŒè”åˆä¼˜åŒ–
- ğŸ”„ **é€šç”¨æ€§å¼º**ï¼šå…¼å®¹ Transformerã€GNNã€NeSy ç­‰å¤šç§ç»„ä»¶

---

## ğŸ§© æ¨¡å—æ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
+------------------+        +-------------------------+
|  å¤šæ–‡æ¡£è¾“å…¥æ®µè½  |---->---|  åŸºç¡€Encoder (e.g., BERT) |
+------------------+        +-------------------------+
                                     |
                                     v
                         +-----------------------+
                         | Structure Extractor   |<-----+
                         |  (Entity/Rel/Trigger) |      |
                         +-----------------------+      |
                                     |                  |
               +---------------------+------------------+
               |                                        |
        +--------------+                       +----------------+
        | å†™å…¥æ˜¾å¼å›¾è®°å¿† |<--------------------|  æ¨ç†è·¯å¾„æ¨¡å—  |
        |  (Graph Mem) |                       | (PathFinder)   |
        +--------------+                       +----------------+
               |                                        |
        +--------------+                       +----------------+
        | å›¾ç»“æ„è¯»å–æ¨¡å— |-------------------->|  æ¨ç†èåˆè¾“å‡º  |
        +--------------+                       +----------------+
```

### æ¨¡å—è¯¦è§£

| æ¨¡å— | åŠŸèƒ½ |
|------|------|
| **DocumentEncoder** | ä½¿ç”¨ BERT ç¼–ç è¾“å…¥æ–‡æ¡£ |
| **StructureExtractor** | æå–å®ä½“ã€å…³ç³»ç­‰ç»“æ„ä¿¡æ¯ |
| **GraphMemory** | å›¾ç¥ç»ç½‘ç»œï¼ˆRGCN/GATï¼‰ç»´æŠ¤å›¾ç»“æ„ |
| **GraphWriter** | å°†ç»“æ„ä¿¡æ¯å†™å…¥å›¾å†…å­˜ |
| **PathFinder** | åœ¨å›¾ä¸­æŸ¥æ‰¾æ¨ç†è·¯å¾„ |
| **GraphReader** | æ ¹æ®æŸ¥è¯¢ä»å›¾ä¸­è¯»å–ç›¸å…³ä¿¡æ¯ |
| **ReasoningFusion** | èåˆæ–‡æ¡£è¡¨ç¤ºå’Œå›¾è¡¨ç¤ºè¿›è¡Œæœ€ç»ˆé¢„æµ‹ |

---

## â–¶ï¸ è¿è¡Œæµæ°´çº¿

```mermaid
graph TD
    A[è¾“å…¥æ–‡æ¡£] --> B[BERTç¼–ç ]
    B --> C[ç»“æ„æŠ½å–<br/>å®ä½“/å…³ç³»]
    C --> D[å›¾å†™å…¥æ¨¡å—]
    D --> E[å›¾å†…å­˜<br/>RGCN/GAT]
    F[æŸ¥è¯¢è¾“å…¥] --> G[è·¯å¾„æŸ¥æ‰¾]
    G --> H[å›¾è¯»å–]
    E --> G
    E --> H
    H --> I[æ¨ç†èåˆ]
    B --> I
    I --> J[è¾“å‡ºé¢„æµ‹]
```

---

## ğŸ›  å®‰è£…ä¾èµ–

```bash
pip install torch transformers torch-geometric
pip install pytorch-lightning
pip install datasets
```

---

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### æ¨¡å‹åˆå§‹åŒ–

```python
from gdmnet import GDMNet

model = GDMNet(
    bert_model_name='bert-base-uncased',
    hidden_size=768,
    num_entities=10,
    num_relations=20,
    num_classes=5
)
```

### å‰å‘ä¼ æ’­

```python
import torch

# æ¨¡æ‹Ÿè¾“å…¥
input_ids = torch.randint(0, 30000, (2, 512))  # [batch_size, seq_len]
attention_mask = torch.ones_like(input_ids)
query = torch.randn(768)  # æŸ¥è¯¢å‘é‡

# å‰å‘ä¼ æ’­
logits = model(input_ids, attention_mask, query=query)
print(f"Output logits shape: {logits.shape}")
```

---

## ğŸ‹ï¸ è®­ç»ƒæŒ‡å—

### æ•°æ®æ ¼å¼è¦æ±‚

```json
{
  "documents": ["doc1 text", "doc2 text"],
  "entities": [{"span": [0, 3], "type": "PERSON"}, ...],
  "relations": [{"head": 0, "tail": 1, "type": "WORKS_FOR"}, ...],
  "query": "Who is the CEO of Apple?",
  "answer": "Tim Cook"
}
```

### è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
from torch.utils.data import DataLoader
import pytorch_lightning as pl

# å‡è®¾å·²å®šä¹‰ dataset å’Œ dataloader
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)

# åˆå§‹åŒ–æ¨¡å‹
model = GDMNet()

# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = pl.Trainer(
    max_epochs=10,
    gpus=1,
    precision=16
)

# å¼€å§‹è®­ç»ƒ
trainer.fit(model, train_loader)
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
gdm-net/
â”œâ”€â”€ README.md
â”œâ”€â”€ gdmnet/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ encoder.py          # DocumentEncoder
â”‚   â”œâ”€â”€ extractor.py        # StructureExtractor
â”‚   â”œâ”€â”€ graph_memory.py     # GraphMemory, GraphWriter
â”‚   â”œâ”€â”€ reasoning.py        # PathFinder, GraphReader, ReasoningFusion
â”‚   â””â”€â”€ model.py            # GDMNet ä¸»æ¨¡å‹
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ example_usage.py
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ dataset.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ config/
    â””â”€â”€ model_config.yaml
```

---

## ğŸ§ª æ¨¡å—ç»“æ„è‰å›¾ä»£ç 

### ä¸»æ¨¡å‹ç»“æ„

```python
import torch
import torch.nn as nn
from transformers import BertModel
from torch_geometric.nn import RGCNConv
from torch_geometric.data import Data

# ----------------------------
# 1. åŸºç¡€ Encoderï¼ˆBERTï¼‰
# ----------------------------
class DocumentEncoder(nn.Module):
    def __init__(self, model_name='bert-base-uncased'):
        super().__init__()
        self.bert = BertModel.from_pretrained(model_name)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state  # [B, seq_len, H]

# ----------------------------
# 2. ç»“æ„æŠ½å–æ¨¡å—
# ----------------------------
class StructureExtractor(nn.Module):
    def __init__(self, hidden_size, num_entities, num_relations):
        super().__init__()
        self.entity_classifier = nn.Linear(hidden_size, num_entities)
        self.relation_classifier = nn.Linear(hidden_size * 2, num_relations)

    def forward(self, sequence_output):
        entities = self.entity_classifier(sequence_output)
        # ç®€åŒ–çš„å…³ç³»æŠ½å–é€»è¾‘
        batch_size, seq_len, _ = sequence_output.shape
        relations = self.relation_classifier(
            torch.cat([
                sequence_output.unsqueeze(2).expand(-1, -1, seq_len, -1),
                sequence_output.unsqueeze(1).expand(-1, seq_len, -1, -1)
            ], dim=-1).view(batch_size, -1, sequence_output.size(-1) * 2)
        )
        return entities, relations

# ----------------------------
# 3. å›¾å†…å­˜æ¨¡å—
# ----------------------------
class GraphMemory(nn.Module):
    def __init__(self, node_dim=768, num_relations=10):
        super().__init__()
        self.gnn = RGCNConv(node_dim, node_dim, num_relations=num_relations)

    def forward(self, x, edge_index, edge_type):
        return self.gnn(x, edge_index, edge_type)

# ----------------------------
# 4. å›¾å†™å…¥æ¨¡å—
# ----------------------------
class GraphWriter(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.writer = nn.Linear(hidden_size, 768)

    def forward(self, entities, relations):
        # ç®€åŒ–ï¼šå®ä½“ä½œä¸ºèŠ‚ç‚¹ï¼Œå…³ç³»ä½œä¸ºè¾¹
        nodes = self.writer(entities.mean(dim=1))  # [B, H]
        # æ„é€ è¾¹çš„ä¼ªä»£ç 
        edge_index = torch.randint(0, nodes.size(0), (2, 10))  # ç¤ºä¾‹è¾¹
        edge_type = torch.randint(0, 5, (10,))  # ç¤ºä¾‹è¾¹ç±»å‹
        return nodes, edge_index, edge_type

# ----------------------------
# 5. æ¨ç†è·¯å¾„æ¨¡å—
# ----------------------------
class PathFinder(nn.Module):
    def __init__(self, gnn_model):
        super().__init__()
        self.gnn = gnn_model

    def forward(self, graph_data, query_emb):
        node_emb = self.gnn(graph_data.x, graph_data.edge_index, graph_data.edge_type)
        path_attn = torch.softmax(torch.matmul(node_emb, query_emb.unsqueeze(-1)), dim=0)
        path_repr = torch.sum(path_attn * node_emb, dim=0)
        return path_repr

# ----------------------------
# 6. å›¾è¯»å–æ¨¡å—
# ----------------------------
class GraphReader(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, node_emb, query):
        attn = torch.softmax(torch.matmul(node_emb, query.unsqueeze(-1)), dim=0)
        read_vec = torch.sum(attn * node_emb, dim=0)
        return read_vec

# ----------------------------
# 7. æ¨ç†èåˆæ¨¡å—
# ----------------------------
class ReasoningFusion(nn.Module):
    def __init__(self, hidden_size, num_classes):
        super().__init__()
        self.fusion = nn.Linear(hidden_size * 2, hidden_size)
        self.output_proj = nn.Linear(hidden_size, num_classes)

    def forward(self, doc_repr, graph_repr):
        fused = torch.cat([doc_repr, graph_repr], dim=-1)
        fused = self.fusion(fused)
        logits = self.output_proj(fused)
        return logits

# ----------------------------
# 8. GDM-Net ä¸»æ¨¡å‹
# ----------------------------
class GDMNet(nn.Module):
    def __init__(self, bert_model_name='bert-base-uncased', hidden_size=768, 
                 num_entities=10, num_relations=20, num_classes=5):
        super().__init__()
        self.encoder = DocumentEncoder(bert_model_name)
        self.structure_extractor = StructureExtractor(hidden_size, num_entities, num_relations)
        self.graph_writer = GraphWriter(hidden_size)
        self.graph_memory = GraphMemory(node_dim=hidden_size, num_relations=num_relations)
        self.path_finder = PathFinder(self.graph_memory)
        self.graph_reader = GraphReader()
        self.reasoning_fusion = ReasoningFusion(hidden_size, num_classes)

    def forward(self, input_ids, attention_mask, query=None):
        # ç¼–ç æ–‡æ¡£
        sequence_output = self.encoder(input_ids, attention_mask)

        # ç»“æ„æŠ½å–
        entities, relations = self.structure_extractor(sequence_output)

        # å†™å…¥å›¾ç»“æ„
        nodes, edge_index, edge_type = self.graph_writer(entities, relations)

        # å›¾ç»“æ„æ„å»º
        graph_data = Data(x=nodes, edge_index=edge_index, edge_type=edge_type)

        # å›¾æ¨ç†è·¯å¾„æŸ¥æ‰¾
        path_repr = self.path_finder(graph_data, query)

        # å›¾è¯»å–
        graph_repr = self.graph_reader(nodes, query)

        # èåˆæ–‡æ¡£å’Œå›¾è¡¨ç¤º
        doc_repr = torch.mean(sequence_output, dim=1)
        logits = self.reasoning_fusion(doc_repr, graph_repr + path_repr)

        return logits
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

- [DocRED: A Large-Scale Document-Level Relation Extraction Dataset](https://arxiv.org/abs/1906.06127)
- [HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering](https://arxiv.org/abs/1809.09600)
- [Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103)

---

## ğŸ“¬ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ–è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚

---

*GDM-Net - è®©å¤šæ–‡æ¡£ç†è§£æ›´åŠ æ™ºèƒ½ï¼ğŸ§ âœ¨*
