{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxyz123"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§  GDM-Net Google Colab è®­ç»ƒ\n",
        "\n",
        "Graph-Augmented Dual Memory Network for Multi-Document Understanding\n",
        "\n",
        "æœ¬ç¬”è®°æœ¬å°†å¸®åŠ©æ‚¨åœ¨Google Colabä¸Šè®­ç»ƒGDM-Netæ¨¡å‹ã€‚\n",
        "\n",
        "## ğŸ“‹ ä½¿ç”¨å‰å‡†å¤‡\n",
        "1. ç¡®ä¿é€‰æ‹©äº†GPUè¿è¡Œæ—¶ï¼šRuntime â†’ Change runtime type â†’ GPU\n",
        "2. å‡†å¤‡å¥½æ‚¨çš„æ•°æ®æ–‡ä»¶\n",
        "3. ä¸Šä¼ é¡¹ç›®ä»£ç æ–‡ä»¶"
      ],
      "metadata": {
        "id": "title_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”§ 1. ç¯å¢ƒæ£€æŸ¥å’Œè®¾ç½®"
      ],
      "metadata": {
        "id": "setup_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æ£€æŸ¥GPUå¯ç”¨æ€§\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"ğŸ” ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU available, using CPU\")\n",
        "\n",
        "# æ˜¾ç¤ºè¯¦ç»†GPUä¿¡æ¯\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£…ä¾èµ–åŒ…\n",
        "print(\"ğŸ“¦ å®‰è£…ä¾èµ–åŒ…...\")\n",
        "\n",
        "# å®‰è£…PyTorchå’Œç›¸å…³åŒ…\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch-geometric\n",
        "!pip install transformers>=4.20.0\n",
        "!pip install pytorch-lightning>=1.7.0\n",
        "!pip install datasets>=2.0.0\n",
        "!pip install PyYAML>=6.0\n",
        "!pip install tensorboard>=2.8.0\n",
        "!pip install wandb>=0.12.0\n",
        "!pip install tqdm>=4.64.0\n",
        "!pip install scikit-learn>=1.1.0\n",
        "!pip install matplotlib>=3.5.0\n",
        "!pip install seaborn>=0.11.0\n",
        "\n",
        "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“ 2. é¡¹ç›®æ–‡ä»¶å‡†å¤‡"
      ],
      "metadata": {
        "id": "files_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æŒ‚è½½Google Driveï¼ˆå¯é€‰ï¼‰\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# å¦‚æœæ‚¨çš„é¡¹ç›®æ–‡ä»¶åœ¨Google Driveä¸­ï¼Œå¯ä»¥å¤åˆ¶åˆ°Colab\n",
        "# !cp -r /content/drive/MyDrive/GDM-Net/* /content/\n",
        "\n",
        "print(\"âœ… Google Drive æŒ‚è½½å®Œæˆ\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„\n",
        "import os\n",
        "\n",
        "directories = [\n",
        "    'gdmnet',\n",
        "    'train', \n",
        "    'config',\n",
        "    'data',\n",
        "    'checkpoints',\n",
        "    'logs',\n",
        "    'examples'\n",
        "]\n",
        "\n",
        "for dir_name in directories:\n",
        "    os.makedirs(dir_name, exist_ok=True)\n",
        "    print(f\"âœ… åˆ›å»ºç›®å½•: {dir_name}\")\n",
        "\n",
        "print(\"\\nğŸ“ é¡¹ç›®ç»“æ„åˆ›å»ºå®Œæˆ\")"
      ],
      "metadata": {
        "id": "create_dirs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ 3. é…ç½®æ–‡ä»¶åˆ›å»º"
      ],
      "metadata": {
        "id": "config_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# åˆ›å»ºColabä¼˜åŒ–çš„é…ç½®æ–‡ä»¶\n",
        "colab_config = \"\"\"\n",
        "# GDM-Net Colab Configuration - GPU Optimized\n",
        "\n",
        "seed: 42\n",
        "\n",
        "model:\n",
        "  bert_model_name: \"bert-base-uncased\"\n",
        "  hidden_size: 768\n",
        "  num_entities: 8\n",
        "  num_relations: 4\n",
        "  num_classes: 5\n",
        "  gnn_type: \"rgcn\"\n",
        "  num_gnn_layers: 2\n",
        "  num_reasoning_hops: 3\n",
        "  fusion_method: \"gate\"\n",
        "  learning_rate: 2e-5\n",
        "  dropout_rate: 0.1\n",
        "\n",
        "data:\n",
        "  train_path: \"data/hotpotqa_train.json\"\n",
        "  val_path: \"data/hotpotqa_val.json\"\n",
        "  test_path: \"data/hotpotqa_val.json\"\n",
        "  max_length: 512\n",
        "  max_query_length: 64\n",
        "\n",
        "training:\n",
        "  max_epochs: 10\n",
        "  batch_size: 8\n",
        "  num_workers: 2\n",
        "  accelerator: \"gpu\"\n",
        "  devices: 1\n",
        "  precision: 16\n",
        "  gradient_clip_val: 1.0\n",
        "  accumulate_grad_batches: 1\n",
        "  val_check_interval: 0.5\n",
        "  log_every_n_steps: 50\n",
        "  checkpoint_dir: \"checkpoints\"\n",
        "  early_stopping: true\n",
        "  patience: 3\n",
        "\n",
        "logging:\n",
        "  type: \"tensorboard\"\n",
        "  save_dir: \"logs\"\n",
        "  name: \"gdmnet-colab\"\n",
        "\"\"\"\n",
        "\n",
        "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
        "with open('config/colab_config.yaml', 'w') as f:\n",
        "    f.write(colab_config.strip())\n",
        "\n",
        "print(\"âœ… Colabé…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ\")\n",
        "print(\"ğŸ“„ é…ç½®æ–‡ä»¶è·¯å¾„: config/colab_config.yaml\")"
      ],
      "metadata": {
        "id": "create_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š 4. æ•°æ®å‡†å¤‡\n",
        "\n",
        "**è¯·ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶åˆ° `data/` ç›®å½•ï¼Œæˆ–ä»Google Driveå¤åˆ¶ã€‚**"
      ],
      "metadata": {
        "id": "data_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
        "import json\n",
        "import os\n",
        "\n",
        "def check_data_files():\n",
        "    \"\"\"æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\"\"\"\n",
        "    data_files = [\n",
        "        'data/hotpotqa_train.json',\n",
        "        'data/hotpotqa_val.json'\n",
        "    ]\n",
        "    \n",
        "    for file_path in data_files:\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            print(f\"âœ… {file_path}: {len(data)} æ ·æœ¬\")\n",
        "            \n",
        "            # æ˜¾ç¤ºæ ·æœ¬\n",
        "            if data:\n",
        "                sample = data[0]\n",
        "                print(f\"   æ–‡æ¡£: {sample['document'][:100]}...\")\n",
        "                print(f\"   æŸ¥è¯¢: {sample['query']}\")\n",
        "                print(f\"   å®ä½“: {len(sample['entities'])}, å…³ç³»: {len(sample['relations'])}\")\n",
        "                print()\n",
        "        else:\n",
        "            print(f\"âŒ {file_path}: æ–‡ä»¶ä¸å­˜åœ¨\")\n",
        "\n",
        "# æ£€æŸ¥æ•°æ®\n",
        "check_data_files()\n",
        "\n",
        "# å¦‚æœæ²¡æœ‰æ•°æ®æ–‡ä»¶ï¼Œæä¾›ä¸Šä¼ é€‰é¡¹\n",
        "if not os.path.exists('data/hotpotqa_train.json'):\n",
        "    print(\"\\nğŸ“¤ è¯·ä¸Šä¼ æ•°æ®æ–‡ä»¶:\")\n",
        "    print(\"1. ä½¿ç”¨å·¦ä¾§æ–‡ä»¶é¢æ¿ä¸Šä¼ åˆ° data/ ç›®å½•\")\n",
        "    print(\"2. æˆ–ä»Google Driveå¤åˆ¶:\")\n",
        "    print(\"   !cp /content/drive/MyDrive/path/to/your/data/* ./data/\")"
      ],
      "metadata": {
        "id": "check_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  5. æ¨¡å‹ä»£ç éƒ¨ç½²\n",
        "\n",
        "**è¯·ç¡®ä¿å·²ä¸Šä¼ æ‰€æœ‰GDM-Netæ¨¡å‹æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•ã€‚**"
      ],
      "metadata": {
        "id": "model_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "\n",
        "def check_model_files():\n",
        "    \"\"\"æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\"\"\"\n",
        "    required_files = [\n",
        "        'gdmnet/__init__.py',\n",
        "        'gdmnet/model.py',\n",
        "        'gdmnet/encoder.py',\n",
        "        'gdmnet/extractor.py',\n",
        "        'gdmnet/graph_memory.py',\n",
        "        'gdmnet/reasoning.py',\n",
        "        'train/train.py',\n",
        "        'train/dataset.py'\n",
        "    ]\n",
        "    \n",
        "    all_exist = True\n",
        "    for file_path in required_files:\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"âœ… {file_path}\")\n",
        "        else:\n",
        "            print(f\"âŒ {file_path}\")\n",
        "            all_exist = False\n",
        "    \n",
        "    return all_exist\n",
        "\n",
        "# æ£€æŸ¥æ–‡ä»¶\n",
        "files_ok = check_model_files()\n",
        "\n",
        "if files_ok:\n",
        "    # æµ‹è¯•å¯¼å…¥\n",
        "    try:\n",
        "        from gdmnet import GDMNet\n",
        "        print(\"\\nâœ… GDM-Netæ¨¡å‹å¯¼å…¥æˆåŠŸ\")\n",
        "    except ImportError as e:\n",
        "        print(f\"\\nâŒ æ¨¡å‹å¯¼å…¥å¤±è´¥: {e}\")\n",
        "else:\n",
        "    print(\"\\nâŒ ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ä¸Šä¼ å®Œæ•´çš„é¡¹ç›®ä»£ç \")"
      ],
      "metadata": {
        "id": "check_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ‹ï¸ 6. å¼€å§‹è®­ç»ƒ"
      ],
      "metadata": {
        "id": "training_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# è®¾ç½®è®­ç»ƒç¯å¢ƒ\n",
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "print(\"ğŸš€ å¯åŠ¨GDM-Netè®­ç»ƒ...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "!python train/train.py --config config/colab_config.yaml --mode train\n",
        "\n",
        "print(\"\\nğŸ‰ è®­ç»ƒå®Œæˆï¼\")"
      ],
      "metadata": {
        "id": "start_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“ˆ 7. ç›‘æ§è®­ç»ƒè¿›åº¦"
      ],
      "metadata": {
        "id": "monitor_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# å¯åŠ¨TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/\n",
        "\n",
        "print(\"ğŸ“Š TensorBoardå·²å¯åŠ¨ï¼Œæ‚¨å¯ä»¥åœ¨ä¸Šæ–¹çœ‹åˆ°è®­ç»ƒæ›²çº¿\")"
      ],
      "metadata": {
        "id": "tensorboard"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ£€æŸ¥è®­ç»ƒè¿›åº¦\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def check_training_progress():\n",
        "    \"\"\"æ£€æŸ¥è®­ç»ƒè¿›åº¦\"\"\"\n",
        "    print(\"ğŸ“Š è®­ç»ƒè¿›åº¦æ£€æŸ¥\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    # æ£€æŸ¥æ£€æŸ¥ç‚¹\n",
        "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
        "    if checkpoints:\n",
        "        print(f\"ğŸ’¾ æ‰¾åˆ° {len(checkpoints)} ä¸ªæ£€æŸ¥ç‚¹:\")\n",
        "        for ckpt in sorted(checkpoints):\n",
        "            size = os.path.getsize(ckpt) / (1024*1024)\n",
        "            print(f\"   {os.path.basename(ckpt)} ({size:.1f} MB)\")\n",
        "        \n",
        "        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹\n",
        "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
        "        print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {os.path.basename(best_model)}\")\n",
        "    else:\n",
        "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
        "    \n",
        "    # æ£€æŸ¥æ—¥å¿—\n",
        "    log_dirs = glob.glob('logs/gdmnet-colab/version_*')\n",
        "    if log_dirs:\n",
        "        print(f\"\\nğŸ“‹ æ‰¾åˆ° {len(log_dirs)} ä¸ªæ—¥å¿—ç›®å½•\")\n",
        "    else:\n",
        "        print(\"\\nâŒ æœªæ‰¾åˆ°æ—¥å¿—ç›®å½•\")\n",
        "\n",
        "# æ£€æŸ¥è¿›åº¦\n",
        "check_training_progress()"
      ],
      "metadata": {
        "id": "check_progress"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§ª 8. æ¨¡å‹æµ‹è¯•å’Œæ¨ç†"
      ],
      "metadata": {
        "id": "inference_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
        "import torch\n",
        "import glob\n",
        "from gdmnet import GDMNet\n",
        "\n",
        "def load_best_model():\n",
        "    \"\"\"åŠ è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
        "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
        "    if not checkpoints:\n",
        "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
        "        return None\n",
        "    \n",
        "    # æ‰¾åˆ°éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹\n",
        "    best_model_path = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
        "    print(f\"ğŸ§  åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}\")\n",
        "    \n",
        "    # åŠ è½½æ¨¡å‹\n",
        "    model = GDMNet.load_from_checkpoint(best_model_path)\n",
        "    model.eval()\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        print(\"ğŸ”¥ æ¨¡å‹å·²ç§»è‡³GPU\")\n",
        "    \n",
        "    print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
        "    print(f\"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# åŠ è½½æ¨¡å‹\n",
        "trained_model = load_best_model()"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è¿è¡Œæ¨ç†ç¤ºä¾‹\n",
        "from transformers import BertTokenizer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def run_inference_demo(model):\n",
        "    \"\"\"è¿è¡Œæ¨ç†æ¼”ç¤º\"\"\"\n",
        "    if model is None:\n",
        "        print(\"âŒ æ¨¡å‹æœªåŠ è½½\")\n",
        "        return\n",
        "    \n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    \n",
        "    # ç¤ºä¾‹è¾“å…¥\n",
        "    examples = [\n",
        "        {\n",
        "            \"document\": \"Apple Inc. is a technology company founded by Steve Jobs. Tim Cook is the current CEO.\",\n",
        "            \"query\": \"Who is the CEO of Apple?\"\n",
        "        },\n",
        "        {\n",
        "            \"document\": \"Microsoft Corporation was founded by Bill Gates. Satya Nadella is the current CEO.\",\n",
        "            \"query\": \"Who founded Microsoft?\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    print(\"ğŸ” æ¨ç†æ¼”ç¤º\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    for i, example in enumerate(examples, 1):\n",
        "        print(f\"\\nğŸ“‹ ç¤ºä¾‹ {i}:\")\n",
        "        print(f\"æ–‡æ¡£: {example['document']}\")\n",
        "        print(f\"æŸ¥è¯¢: {example['query']}\")\n",
        "        \n",
        "        # ç¼–ç è¾“å…¥\n",
        "        doc_encoding = tokenizer(\n",
        "            example['document'],\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        query_encoding = tokenizer(\n",
        "            example['query'],\n",
        "            max_length=64,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        # ç§»è‡³GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
        "        if torch.cuda.is_available():\n",
        "            doc_encoding = {k: v.cuda() for k, v in doc_encoding.items()}\n",
        "            query_encoding = {k: v.cuda() for k, v in query_encoding.items()}\n",
        "        \n",
        "        # æ¨ç†\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=doc_encoding['input_ids'],\n",
        "                attention_mask=doc_encoding['attention_mask'],\n",
        "                query=query_encoding['input_ids'],\n",
        "                return_intermediate=True\n",
        "            )\n",
        "        \n",
        "        # æ˜¾ç¤ºç»“æœ\n",
        "        logits = outputs['logits']\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "        prediction = torch.argmax(logits, dim=-1)\n",
        "        confidence = probabilities.max()\n",
        "        \n",
        "        print(f\"ğŸ¯ é¢„æµ‹ç±»åˆ«: {prediction.item()}\")\n",
        "        print(f\"ğŸ“Š ç½®ä¿¡åº¦: {confidence.item():.3f}\")\n",
        "        print(f\"ğŸ” æå–å®ä½“: {len(outputs['entities'][0])}\")\n",
        "        print(f\"ğŸ”— æå–å…³ç³»: {len(outputs['relations'][0])}\")\n",
        "\n",
        "# è¿è¡Œæ¨ç†æ¼”ç¤º\n",
        "if 'trained_model' in locals() and trained_model is not None:\n",
        "    run_inference_demo(trained_model)\n",
        "else:\n",
        "    print(\"âš ï¸ è¯·å…ˆåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\")"
      ],
      "metadata": {
        "id": "inference_demo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¾ 9. ä¿å­˜å’Œä¸‹è½½ç»“æœ"
      ],
      "metadata": {
        "id": "save_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ä¿å­˜ç»“æœåˆ°Google Drive\n",
        "def save_to_drive():\n",
        "    \"\"\"ä¿å­˜è®­ç»ƒç»“æœåˆ°Google Drive\"\"\"\n",
        "    result_dir = '/content/drive/MyDrive/GDM-Net-Results'\n",
        "    os.makedirs(result_dir, exist_ok=True)\n",
        "    \n",
        "    print(\"ğŸ’¾ ä¿å­˜ç»“æœåˆ°Google Drive...\")\n",
        "    \n",
        "    # å¤åˆ¶æ£€æŸ¥ç‚¹\n",
        "    if os.path.exists('checkpoints'):\n",
        "        !cp -r checkpoints/* /content/drive/MyDrive/GDM-Net-Results/\n",
        "        print(\"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜\")\n",
        "    \n",
        "    # å¤åˆ¶æ—¥å¿—\n",
        "    if os.path.exists('logs'):\n",
        "        !cp -r logs/* /content/drive/MyDrive/GDM-Net-Results/\n",
        "        print(\"âœ… æ—¥å¿—å·²ä¿å­˜\")\n",
        "    \n",
        "    # ä¿å­˜é…ç½®\n",
        "    !cp config/colab_config.yaml /content/drive/MyDrive/GDM-Net-Results/\n",
        "    print(\"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜\")\n",
        "    \n",
        "    print(f\"\\nğŸ‰ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {result_dir}\")\n",
        "\n",
        "# æ‰§è¡Œä¿å­˜\n",
        "save_to_drive()"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ä¸‹è½½æœ€ä½³æ¨¡å‹åˆ°æœ¬åœ°\n",
        "from google.colab import files\n",
        "import glob\n",
        "\n",
        "def download_best_model():\n",
        "    \"\"\"ä¸‹è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
        "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
        "    if checkpoints:\n",
        "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
        "        print(f\"ğŸ“¥ ä¸‹è½½æœ€ä½³æ¨¡å‹: {best_model}\")\n",
        "        files.download(best_model)\n",
        "        print(\"âœ… ä¸‹è½½å®Œæˆ\")\n",
        "    else:\n",
        "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
        "\n",
        "# ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
        "# download_best_model()"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ‰ è®­ç»ƒå®Œæˆï¼\n",
        "\n",
        "æ­å–œæ‚¨æˆåŠŸåœ¨Google Colabä¸Šè®­ç»ƒäº†GDM-Netæ¨¡å‹ï¼\n",
        "\n",
        "### ğŸ“‹ åç»­æ­¥éª¤ï¼š\n",
        "1. æŸ¥çœ‹TensorBoardä¸­çš„è®­ç»ƒæ›²çº¿\n",
        "2. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†\n",
        "3. ä¿å­˜é‡è¦ç»“æœåˆ°Google Drive\n",
        "4. ä¸‹è½½æœ€ä½³æ¨¡å‹åˆ°æœ¬åœ°\n",
        "\n",
        "### ğŸ”§ è¿›ä¸€æ­¥ä¼˜åŒ–ï¼š\n",
        "- è°ƒæ•´è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰\n",
        "- å°è¯•ä¸åŒçš„èåˆç­–ç•¥\n",
        "- ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†\n",
        "- å®éªŒä¸åŒçš„GNNæ¶æ„\n",
        "\n",
        "### ğŸ“ è·å–å¸®åŠ©ï¼š\n",
        "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
        "1. GPUæ˜¯å¦æ­£ç¡®å¯ç”¨\n",
        "2. æ‰€æœ‰ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…\n",
        "3. æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®\n",
        "4. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸Šä¼ \n",
        "\n",
        "ç¥æ‚¨ç ”ç©¶é¡ºåˆ©ï¼ğŸš€"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
