{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyMxyz123"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 🚀 GDM-Net 多GPU Google Colab 训练\n",
    "\n",
    "Graph-Augmented Dual Memory Network for Multi-Document Understanding\n",
    "\n",
    "本笔记本将帮助您在Google Colab上使用**多GPU**训练GDM-Net模型。\n",
    "\n",
    "## 📋 使用前准备\n",
    "1. **选择高端GPU运行时**：Runtime → Change runtime type → GPU (推荐A100或V100)\n",
    "2. **检查多GPU可用性**：某些Colab Pro+账户可能有多GPU\n",
    "3. 准备好您的数据文件\n",
    "4. 上传项目代码文件\n",
    "\n",
    "## 🎯 多GPU优势\n",
    "- **更大批次大小**：多GPU可以处理更大的批次\n",
    "- **更快训练速度**：并行计算显著加速\n",
    "- **更长序列支持**：内存分布允许处理更长文档"
   ],
   "metadata": {
    "id": "title_cell"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🔧 1. 环境检查和设置"
   ],
   "metadata": {
    "id": "setup_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 多GPU环境检查\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"🔍 多GPU系统信息检查\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"🚀 检测到 {num_gpus} 个GPU\")\n",
    "\n",
    "    total_memory = 0\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        total_memory += gpu_memory\n",
    "        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "\n",
    "    print(f\"📊 总GPU内存: {total_memory:.1f} GB\")\n",
    "    print(f\"🔥 PyTorch版本: {torch.__version__}\")\n",
    "\n",
    "    if num_gpus > 1:\n",
    "        print(f\"✅ 多GPU训练可用！将启用分布式数据并行(DDP)\")\n",
    "        print(f\"📈 预期训练速度提升: ~{num_gpus * 0.8:.1f}倍\")\n",
    "    else:\n",
    "        print(f\"🔧 单GPU训练模式\")\n",
    "else:\n",
    "    print(\"❌ CUDA不可用，将使用CPU训练\")\n",
    "\n",
    "# 显示详细GPU信息\n",
    "print(f\"\\n🖥️ 详细GPU信息:\")\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "check_gpu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 完整修复PyTorch、transformers和NumPy环境\n",
    "print(\"🛠️ 完整修复PyTorch、transformers和NumPy环境...\")\n",
    "\n",
    "# 完全卸载可能冲突的包\n",
    "print(\"🧹 完全清理现有环境...\")\n",
    "!pip uninstall torch torchvision torchaudio transformers torch-geometric pytorch-lightning numpy -y -q\n",
    "\n",
    "# 清理pip缓存\n",
    "!pip cache purge\n",
    "\n",
    "# 首先安装兼容的NumPy版本\n",
    "print(\"📦 安装兼容的NumPy版本...\")\n",
    "!pip install \"numpy<2.0\"\n",
    "\n",
    "# 安装稳定版本的PyTorch\n",
    "print(\"📦 安装稳定版本的PyTorch...\")\n",
    "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# 安装稳定版本的transformers\n",
    "print(\"📦 安装稳定版本的transformers...\")\n",
    "!pip install transformers==4.30.0\n",
    "!pip install torch-geometric\n",
    "!pip install pytorch-lightning==1.9.0\n",
    "!pip install datasets>=2.0.0\n",
    "!pip install PyYAML>=6.0\n",
    "!pip install tensorboard>=2.8.0\n",
    "!pip install wandb>=0.12.0\n",
    "!pip install tqdm>=4.64.0\n",
    "!pip install scikit-learn>=1.1.0\n",
    "!pip install matplotlib>=3.5.0\n",
    "!pip install seaborn>=0.11.0\n",
    "\n",
    "print(\"✅ 依赖安装完成\")\n",
    "\n",
    "# 验证环境安装\n",
    "print(\"\\\\n🔍 验证环境安装...\")\n",
    "\n",
    "# 验证NumPy\n",
    "import numpy as np\n",
    "print(f\"✅ NumPy: {np.__version__}\")\n",
    "\n",
    "# 验证PyTorch\n",
    "import torch\n",
    "print(f\"✅ PyTorch: {torch.__version__}\")\n",
    "print(f\"✅ CUDA版本: {torch.version.cuda}\")\n",
    "print(f\"✅ CUDA可用: {torch.cuda.is_available()}\")\n",
    "\n",
    "# 验证torchvision（这是容易出问题的地方）\n",
    "try:\n",
    "    import torchvision\n",
    "    print(f\"✅ torchvision: {torchvision.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ torchvision导入失败: {e}\")\n",
    "    print(\"🔧 尝试修复...\")\n",
    "    !pip install --force-reinstall torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "    import torchvision\n",
    "    print(f\"✅ torchvision修复成功: {torchvision.__version__}\")\n",
    "\n",
    "# 验证transformers\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    print(\"✅ transformers导入成功\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ transformers导入失败: {e}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"✅ GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ],
   "metadata": {
    "id": "install_deps"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📁 2. 项目文件准备"
   ],
   "metadata": {
    "id": "files_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 挂载Google Drive（可选）\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 如果您的项目文件在Google Drive中，可以复制到Colab\n",
    "# !cp -r /content/drive/MyDrive/GDM-Net/* /content/\n",
    "\n",
    "print(\"✅ Google Drive 挂载完成\")"
   ],
   "metadata": {
    "id": "mount_drive"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 创建项目目录结构\n",
    "import os\n",
    "\n",
    "directories = [\n",
    "    'gdmnet',\n",
    "    'train', \n",
    "    'config',\n",
    "    'data',\n",
    "    'checkpoints',\n",
    "    'logs',\n",
    "    'examples'\n",
    "]\n",
    "\n",
    "for dir_name in directories:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    print(f\"✅ 创建目录: {dir_name}\")\n",
    "\n",
    "print(\"\\n📁 项目结构创建完成\")"
   ],
   "metadata": {
    "id": "create_dirs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ⚙️ 3. 配置文件创建"
   ],
   "metadata": {
    "id": "config_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 创建多GPU优化的HotpotQA配置文件\n",
    "print(\"⚙️ 创建多GPU优化配置...\")\n",
    "\n",
    "# 检查GPU数量并创建相应配置\n",
    "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "\n",
    "if num_gpus > 1:\n",
    "    print(f\"🚀 创建多GPU配置 ({num_gpus} GPUs)\")\n",
    "    colab_config = f\"\"\"\n",
    "# Multi-GPU HotpotQA Configuration for Google Colab\n",
    "# Optimized for {num_gpus} GPUs with distributed training\"\"\"\n",
    "else:\n",
    "    print(f\"🔧 创建单GPU配置\")\n",
    "    colab_config = \"\"\"\n",
    "# Single-GPU HotpotQA Configuration for Google Colab\n",
    "# Optimized for single GPU training\"\"\"\n",
    "\n",
    "colab_config += \"\"\"\n",
    "\n",
    "seed: 42\n",
    "\n",
    "model:\n",
    "  bert_model_name: \"bert-base-uncased\"\n",
    "  hidden_size: 768\n",
    "  num_entities: 8\n",
    "  num_relations: 4\n",
    "  num_classes: 5\n",
    "  gnn_type: \"rgcn\"\n",
    "  num_gnn_layers: 2\n",
    "  num_reasoning_hops: 3\n",
    "  fusion_method: \"gate\"\n",
    "  learning_rate: 2e-5\n",
    "  dropout_rate: 0.1\n",
    "\n",
    "data:\n",
    "  train_path: \"data/hotpotqa_official_train.json\"\n",
    "  val_path: \"data/hotpotqa_official_val.json\"\n",
    "  test_path: \"data/hotpotqa_official_val.json\"\n",
    "  max_length: 512\n",
    "  max_query_length: 64\n",
    "\n",
    "training:\n",
    "  max_epochs: 5\"\"\"\n",
    "\n",
    "# 根据GPU数量动态调整配置\n",
    "if num_gpus > 1:\n",
    "    # 多GPU配置\n",
    "    batch_size_per_gpu = 4\n",
    "    total_batch_size = batch_size_per_gpu * num_gpus\n",
    "    colab_config += f\"\"\"\n",
    "  batch_size: {batch_size_per_gpu}  # 每个GPU的批次大小\n",
    "  num_workers: {min(num_gpus * 2, 8)}  # 多GPU可以使用更多worker\n",
    "  accelerator: \"gpu\"\n",
    "  devices: {num_gpus}  # 使用所有GPU\n",
    "  strategy: \"ddp\"  # 分布式数据并行\n",
    "  precision: 32\n",
    "  gradient_clip_val: 1.0\n",
    "  accumulate_grad_batches: 1  # 多GPU不需要太多累积\n",
    "  val_check_interval: 0.25\n",
    "  log_every_n_steps: 50\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: true\n",
    "  patience: 3\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-multi-gpu-{num_gpus}\"\n",
    "\"\"\"\n",
    "    print(f\"📊 多GPU配置:\")\n",
    "    print(f\"  - GPU数量: {num_gpus}\")\n",
    "    print(f\"  - 每GPU批次大小: {batch_size_per_gpu}\")\n",
    "    print(f\"  - 总有效批次大小: {total_batch_size}\")\n",
    "    print(f\"  - 预期内存使用: ~{6/num_gpus:.1f}GB per GPU\")\n",
    "else:\n",
    "    # 单GPU配置\n",
    "    colab_config += \"\"\"\n",
    "  batch_size: 1  # 单GPU使用小批次以节省内存\n",
    "  num_workers: 2\n",
    "  accelerator: \"gpu\"\n",
    "  devices: 1\n",
    "  precision: 32  # GPU兼容性：使用32位精度\n",
    "  gradient_clip_val: 1.0\n",
    "  accumulate_grad_batches: 8  # 单GPU需要更多累积\n",
    "  val_check_interval: 0.25\n",
    "  log_every_n_steps: 100\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: true\n",
    "  patience: 2\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-single-gpu\"\n",
    "\"\"\"\n",
    "\n",
    "# 保存配置文件\n",
    "config_filename = f'config/multi_gpu_hotpotqa_config.yaml' if num_gpus > 1 else 'config/single_gpu_hotpotqa_config.yaml'\n",
    "\n",
    "with open(config_filename, 'w') as f:\n",
    "    f.write(colab_config.strip())\n",
    "\n",
    "print(\"✅ 多GPU优化配置文件创建完成\")\n",
    "print(f\"📄 配置文件路径: {config_filename}\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"🚀 多GPU训练配置 ({num_gpus} GPUs)\")\n",
    "    print(f\"📈 预期训练速度提升: ~{num_gpus * 0.8:.1f}倍\")\n",
    "else:\n",
    "    print(\"🔧 单GPU训练配置\")"
   ],
   "metadata": {
    "id": "create_config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📊 4. 数据准备\n",
    "\n",
    "**请上传您的数据文件到 `data/` 目录，或从Google Drive复制。**"
   ],
   "metadata": {
    "id": "data_title"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 检查数据文件\n",
    "import json\n",
    "import os\n",
    "\n",
    "def check_data_files():\n",
    "    \"\"\"检查官方HotpotQA数据文件是否存在\"\"\"\n",
    "    data_files = [\n",
    "        'data/hotpotqa_official_train.json',\n",
    "        'data/hotpotqa_official_val.json'\n",
    "    ]\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"✅ {file_path}: {len(data)} 样本\")\n",
    "            \n",
    "            # 显示样本\n",
    "            if data:\n",
    "                sample = data[0]\n",
    "                print(f\"   文档: {sample['document'][:100]}...\")\n",
    "                print(f\"   查询: {sample['query']}\")\n",
    "                print(f\"   实体: {len(sample['entities'])}, 关系: {len(sample['relations'])}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"❌ {file_path}: 文件不存在\")\n",
    "\n",
    "# 检查数据\n",
    "check_data_files()\n",
    "\n",
    "# 如果没有官方数据文件，提供获取选项\n",
    "if not os.path.exists('data/hotpotqa_official_train.json'):\n",
    "    print(\"\\n📤 获取官方HotpotQA数据集:\")\n",
    "    print(\"1. 从Google Drive复制:\")\n",
    "    print(\"   !cp /content/drive/MyDrive/GDM-Net/data/hotpotqa_official_*.json ./data/\")\n",
    "    print(\"2. 或重新下载:\")\n",
    "    print(\"   !python download_official_hotpotqa.py\")\n",
    "else:\n",
    "    print(\"\\n✅ 官方HotpotQA数据集已准备就绪！\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 验证官方数据集质量和格式\n",
    "def validate_official_dataset():\n",
    "    \"\"\"验证官方HotpotQA数据集的质量和格式\"\"\"\n",
    "    print(\"🔍 验证官方HotpotQA数据集...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not os.path.exists('data/hotpotqa_official_train.json'):\n",
    "        print(\"❌ 官方训练数据不存在\")\n",
    "        return False\n",
    "\n",
    "    with open('data/hotpotqa_official_train.json', 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    with open('data/hotpotqa_official_val.json', 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    print(f\"📊 数据集统计:\")\n",
    "    print(f\"  训练集: {len(train_data)} 样本\")\n",
    "    print(f\"  验证集: {len(val_data)} 样本\")\n",
    "\n",
    "    # 分析数据质量\n",
    "    sample = train_data[0]\n",
    "    print(f\"\\n📋 数据样本分析:\")\n",
    "    print(f\"  文档长度: {len(sample['document'])} 字符\")\n",
    "    print(f\"  查询长度: {len(sample['query'])} 字符\")\n",
    "    print(f\"  实体数量: {len(sample['entities'])}\")\n",
    "    print(f\"  关系数量: {len(sample['relations'])}\")\n",
    "    print(f\"  标签: {sample['label']}\")\n",
    "    print(f\"  数据源: {sample['metadata']['source']}\")\n",
    "\n",
    "    # 检查数据完整性\n",
    "    entity_types = set()\n",
    "    relation_types = set()\n",
    "    labels = set()\n",
    "\n",
    "    for item in train_data[:100]:  # 检查前100个样本\n",
    "        for entity in item['entities']:\n",
    "            entity_types.add(entity['type'])\n",
    "        for relation in item['relations']:\n",
    "            relation_types.add(relation['type'])\n",
    "        labels.add(item['label'])\n",
    "\n",
    "    print(f\"\\n🔢 数据范围检查:\")\n",
    "    print(f\"  实体类型范围: {min(entity_types) if entity_types else 'N/A'} - {max(entity_types) if entity_types else 'N/A'}\")\n",
    "    print(f\"  关系类型范围: {min(relation_types) if relation_types else 'N/A'} - {max(relation_types) if relation_types else 'N/A'}\")\n",
    "    print(f\"  标签范围: {min(labels)} - {max(labels)}\")\n",
    "\n",
    "    # 显示真实样本内容\n",
    "    print(f\"\\n📖 真实样本内容:\")\n",
    "    print(f\"文档: {sample['document'][:200]}...\")\n",
    "    print(f\"查询: {sample['query']}\")\n",
    "    print(f\"答案: {sample['metadata'].get('answer', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\n✅ 官方HotpotQA数据集验证完成\")\n",
    "    return True\n",
    "\n",
    "# 执行验证\n",
    "validate_official_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🧠 5. 模型代码部署\n",
    "\n",
    "**请确保已上传所有GDM-Net模型文件到对应目录。**"
   ],
   "metadata": {
    "id": "model_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 检查模型文件\n",
    "import sys\n",
    "sys.path.append('/content')\n",
    "\n",
    "def check_model_files():\n",
    "    \"\"\"检查模型文件是否存在\"\"\"\n",
    "    required_files = [\n",
    "        'gdmnet/__init__.py',\n",
    "        'gdmnet/model.py',\n",
    "        'gdmnet/encoder.py',\n",
    "        'gdmnet/extractor.py',\n",
    "        'gdmnet/graph_memory.py',\n",
    "        'gdmnet/reasoning.py',\n",
    "        'train/train.py',\n",
    "        'train/dataset.py'\n",
    "    ]\n",
    "    \n",
    "    all_exist = True\n",
    "    for file_path in required_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"✅ {file_path}\")\n",
    "        else:\n",
    "            print(f\"❌ {file_path}\")\n",
    "            all_exist = False\n",
    "    \n",
    "    return all_exist\n",
    "\n",
    "# 检查文件\n",
    "files_ok = check_model_files()\n",
    "\n",
    "if files_ok:\n",
    "    # 首先确保transformers正确安装\n",
    "    print(\"\\n🔧 验证transformers库...\")\n",
    "    try:\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        print(\"✅ transformers库正常\")\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ transformers导入失败: {e}\")\n",
    "        print(\"🔧 重新安装transformers...\")\n",
    "        !pip install --upgrade transformers>=4.20.0\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        print(\"✅ transformers重新安装成功\")\n",
    "\n",
    "    # 测试GDM-Net导入\n",
    "    try:\n",
    "        import sys\n",
    "        sys.path.append('/content')  # 确保路径正确\n",
    "        from gdmnet import GDMNet\n",
    "        print(\"✅ GDM-Net模型导入成功\")\n",
    "\n",
    "        # 测试模型创建\n",
    "        test_model = GDMNet(\n",
    "            bert_model_name='bert-base-uncased',\n",
    "            hidden_size=768,\n",
    "            num_entities=8,\n",
    "            num_relations=4,\n",
    "            num_classes=5\n",
    "        )\n",
    "        print(f\"✅ 模型创建成功 ({sum(p.numel() for p in test_model.parameters()):,} 参数)\")\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ GDM-Net导入失败: {e}\")\n",
    "        print(\"💡 请确保所有模型文件都已正确上传\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 模型创建失败: {e}\")\n",
    "        print(\"💡 可能是依赖版本不兼容，尝试重新安装依赖\")\n",
    "else:\n",
    "    print(\"\\n❌ 缺少必要的模型文件，请上传完整的项目代码\")"
   ],
   "metadata": {
    "id": "check_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🏋️ 6. 开始训练"
   ],
   "metadata": {
    "id": "training_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 设置训练环境\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# 多GPU训练启动\n",
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "config_file = f'config/multi_gpu_hotpotqa_config.yaml' if num_gpus > 1 else 'config/single_gpu_hotpotqa_config.yaml'\n",
    "\n",
    "print(\"🚀 启动GDM-Net多GPU训练...\")\n",
    "print(\"🎯 使用真实Wikipedia数据进行多跳推理训练\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"🔥 多GPU加速训练 ({num_gpus} GPUs)\")\n",
    "    print(f\"📊 分布式数据并行 (DDP)\")\n",
    "    print(f\"⚡ 预期速度提升: ~{num_gpus * 0.8:.1f}倍\")\n",
    "else:\n",
    "    print(\"🔧 单GPU训练模式\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 设置环境变量以优化多GPU训练\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "if num_gpus > 1:\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'  # 多GPU通信调试\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # 异步执行\n",
    "\n",
    "# 启动训练\n",
    "exec_cmd = f\"python train/train.py --config {config_file} --mode train\"\n",
    "print(f\"🎯 执行命令: {exec_cmd}\")\n",
    "!{exec_cmd}\n",
    "\n",
    "print(f\"\\n🎉 多GPU HotpotQA数据集训练完成！\")\n",
    "print(\"📊 训练结果具有学术研究价值，可与论文基线对比\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"🚀 多GPU训练加速效果已体现在训练时间中\")"
   ],
   "metadata": {
    "id": "start_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📈 7. 监控训练进度"
   ],
   "metadata": {
    "id": "monitor_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 启动TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/\n",
    "\n",
    "print(\"📊 TensorBoard已启动，您可以在上方看到训练曲线\")"
   ],
   "metadata": {
    "id": "tensorboard"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 多GPU性能监控\n",
    "def check_multi_gpu_performance():\n",
    "    \"\"\"检查多GPU训练性能\"\"\"\n",
    "    print(\"🚀 多GPU性能监控\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"📊 GPU使用情况 ({num_gpus} GPUs):\")\n",
    "\n",
    "        for i in range(num_gpus):\n",
    "            # GPU内存使用\n",
    "            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "\n",
    "            print(f\"  GPU {i}: {memory_allocated:.1f}GB / {memory_total:.1f}GB 使用中\")\n",
    "            print(f\"         {memory_reserved:.1f}GB 已预留\")\n",
    "\n",
    "            # GPU利用率（需要nvidia-ml-py包，Colab通常没有）\n",
    "            try:\n",
    "                import pynvml\n",
    "                pynvml.nvmlInit()\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                print(f\"         GPU利用率: {util.gpu}%, 内存利用率: {util.memory}%\")\n",
    "            except:\n",
    "                print(f\"         (无法获取利用率信息)\")\n",
    "\n",
    "        # 显示nvidia-smi\n",
    "        print(f\"\\n🖥️ 详细GPU状态:\")\n",
    "        !nvidia-smi\n",
    "    else:\n",
    "        print(\"❌ CUDA不可用\")\n",
    "\n",
    "# 执行多GPU性能检查\n",
    "check_multi_gpu_performance()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 检查训练进度\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def check_training_progress():\n",
    "    \"\"\"检查训练进度\"\"\"\n",
    "    print(\"📊 训练进度检查\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # 检查检查点\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        print(f\"💾 找到 {len(checkpoints)} 个检查点:\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            size = os.path.getsize(ckpt) / (1024*1024)\n",
    "            print(f\"   {os.path.basename(ckpt)} ({size:.1f} MB)\")\n",
    "        \n",
    "        # 找到最佳模型\n",
    "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "        print(f\"\\n🏆 最佳模型: {os.path.basename(best_model)}\")\n",
    "    else:\n",
    "        print(\"❌ 未找到检查点文件\")\n",
    "    \n",
    "    # 检查日志\n",
    "    log_dirs = glob.glob('logs/gdmnet-colab/version_*')\n",
    "    if log_dirs:\n",
    "        print(f\"\\n📋 找到 {len(log_dirs)} 个日志目录\")\n",
    "    else:\n",
    "        print(\"\\n❌ 未找到日志目录\")\n",
    "\n",
    "# 检查进度\n",
    "check_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🧪 8. 模型测试和推理"
   ],
   "metadata": {
    "id": "inference_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 加载训练好的模型\n",
    "import torch\n",
    "import glob\n",
    "from gdmnet import GDMNet\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"加载最佳模型\"\"\"\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if not checkpoints:\n",
    "        print(\"❌ 未找到检查点文件\")\n",
    "        return None\n",
    "    \n",
    "    # 找到验证损失最低的模型\n",
    "    best_model_path = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "    print(f\"🧠 加载最佳模型: {best_model_path}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    model = GDMNet.load_from_checkpoint(best_model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"🔥 模型已移至GPU\")\n",
    "    \n",
    "    print(f\"✅ 模型加载成功\")\n",
    "    print(f\"📊 模型参数: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 加载模型\n",
    "trained_model = load_best_model()"
   ],
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 运行推理示例\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_inference_demo(model):\n",
    "    \"\"\"运行推理演示\"\"\"\n",
    "    if model is None:\n",
    "        print(\"❌ 模型未加载\")\n",
    "        return\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # 示例输入\n",
    "    examples = [\n",
    "        {\n",
    "            \"document\": \"Apple Inc. is a technology company founded by Steve Jobs. Tim Cook is the current CEO.\",\n",
    "            \"query\": \"Who is the CEO of Apple?\"\n",
    "        },\n",
    "        {\n",
    "            \"document\": \"Microsoft Corporation was founded by Bill Gates. Satya Nadella is the current CEO.\",\n",
    "            \"query\": \"Who founded Microsoft?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🔍 推理演示\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\n📋 示例 {i}:\")\n",
    "        print(f\"文档: {example['document']}\")\n",
    "        print(f\"查询: {example['query']}\")\n",
    "        \n",
    "        # 编码输入\n",
    "        doc_encoding = tokenizer(\n",
    "            example['document'],\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        query_encoding = tokenizer(\n",
    "            example['query'],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 移至GPU（如果可用）\n",
    "        if torch.cuda.is_available():\n",
    "            doc_encoding = {k: v.cuda() for k, v in doc_encoding.items()}\n",
    "            query_encoding = {k: v.cuda() for k, v in query_encoding.items()}\n",
    "        \n",
    "        # 推理\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=doc_encoding['input_ids'],\n",
    "                attention_mask=doc_encoding['attention_mask'],\n",
    "                query=query_encoding['input_ids'],\n",
    "                return_intermediate=True\n",
    "            )\n",
    "        \n",
    "        # 显示结果\n",
    "        logits = outputs['logits']\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "        confidence = probabilities.max()\n",
    "        \n",
    "        print(f\"🎯 预测类别: {prediction.item()}\")\n",
    "        print(f\"📊 置信度: {confidence.item():.3f}\")\n",
    "        print(f\"🔍 提取实体: {len(outputs['entities'][0])}\")\n",
    "        print(f\"🔗 提取关系: {len(outputs['relations'][0])}\")\n",
    "\n",
    "# 运行推理演示\n",
    "if 'trained_model' in locals() and trained_model is not None:\n",
    "    run_inference_demo(trained_model)\n",
    "else:\n",
    "    print(\"⚠️ 请先加载训练好的模型\")"
   ],
   "metadata": {
    "id": "inference_demo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 💾 9. 保存和下载结果"
   ],
   "metadata": {
    "id": "save_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 保存结果到Google Drive\n",
    "def save_to_drive():\n",
    "    \"\"\"保存训练结果到Google Drive\"\"\"\n",
    "    result_dir = '/content/drive/MyDrive/GDM-Net-Results'\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"💾 保存结果到Google Drive...\")\n",
    "    \n",
    "    # 复制检查点\n",
    "    if os.path.exists('checkpoints'):\n",
    "        !cp -r checkpoints/* /content/drive/MyDrive/GDM-Net-Results/\n",
    "        print(\"✅ 检查点已保存\")\n",
    "    \n",
    "    # 复制日志\n",
    "    if os.path.exists('logs'):\n",
    "        !cp -r logs/* /content/drive/MyDrive/GDM-Net-Results/\n",
    "        print(\"✅ 日志已保存\")\n",
    "    \n",
    "    # 保存配置\n",
    "    !cp config/colab_config.yaml /content/drive/MyDrive/GDM-Net-Results/\n",
    "    print(\"✅ 配置文件已保存\")\n",
    "    \n",
    "    print(f\"\\n🎉 所有结果已保存到: {result_dir}\")\n",
    "\n",
    "# 执行保存\n",
    "save_to_drive()"
   ],
   "metadata": {
    "id": "save_results"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 下载最佳模型到本地\n",
    "from google.colab import files\n",
    "import glob\n",
    "\n",
    "def download_best_model():\n",
    "    \"\"\"下载最佳模型\"\"\"\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "        print(f\"📥 下载最佳模型: {best_model}\")\n",
    "        files.download(best_model)\n",
    "        print(\"✅ 下载完成\")\n",
    "    else:\n",
    "        print(\"❌ 未找到检查点文件\")\n",
    "\n",
    "# 下载模型（可选）\n",
    "# download_best_model()"
   ],
   "metadata": {
    "id": "download_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🎉 多GPU训练完成！\n",
    "\n",
    "恭喜您成功在Google Colab上使用多GPU训练了GDM-Net模型！\n",
    "\n",
    "### 🚀 多GPU训练成果：\n",
    "- **并行计算**：充分利用了多GPU资源\n",
    "- **训练加速**：相比单GPU有显著速度提升\n",
    "- **内存优化**：多GPU分布式内存使用\n",
    "- **学术级结果**：可与论文基线直接对比\n",
    "\n",
    "### 📋 后续步骤：\n",
    "1. 查看TensorBoard中的训练曲线\n",
    "2. 分析多GPU训练效果\n",
    "3. 使用训练好的模型进行推理\n",
    "4. 保存重要结果到Google Drive\n",
    "5. 下载最佳模型到本地\n",
    "\n",
    "### 🔧 多GPU优化建议：\n",
    "- **批次大小调优**：根据GPU数量调整批次大小\n",
    "- **学习率缩放**：多GPU训练可能需要调整学习率\n",
    "- **通信优化**：使用更高效的分布式策略\n",
    "- **负载均衡**：确保各GPU负载均匀\n",
    "\n",
    "### 📊 性能对比：\n",
    "| 配置 | GPU数量 | 批次大小 | 训练速度 | 内存使用 |\n",
    "|------|---------|----------|----------|----------|\n",
    "| 单GPU | 1 | 1×8累积 | 基准 | 12GB |\n",
    "| 双GPU | 2 | 4×2 | ~1.8倍 | 6GB×2 |\n",
    "| 四GPU | 4 | 4×4 | ~3.5倍 | 3GB×4 |\n",
    "\n",
    "### 🎯 多GPU HotpotQA训练性能预期：\n",
    "\n",
    "**多GPU训练配置**：\n",
    "- **单GPU**: batch_size=1, 累积=8, 内存~12GB\n",
    "- **双GPU**: batch_size=4×2, 累积=1, 内存~6GB×2\n",
    "- **四GPU**: batch_size=4×4, 累积=1, 内存~3GB×4\n",
    "\n",
    "**预期性能指标**：\n",
    "- **准确率**：55-65%（与学术论文基线对比）\n",
    "- **训练时间**：\n",
    "  - 单GPU: 1-2小时\n",
    "  - 双GPU: 35-70分钟 (~1.8倍加速)\n",
    "  - 四GPU: 20-40分钟 (~3.5倍加速)\n",
    "- **收敛速度**：通常在3-4个epoch收敛\n",
    "\n",
    "**多GPU优势**：\n",
    "- ✅ **并行计算**：显著减少训练时间\n",
    "- ✅ **内存分布**：支持更大批次和更长序列\n",
    "- ✅ **扩展性好**：GPU数量增加，性能线性提升\n",
    "- ✅ **学术价值**：结果可与论文基线直接对比\n",
    "\n",
    "### 🔧 多GPU故障排除：\n",
    "如果遇到问题，请检查：\n",
    "1. **多GPU可用性**：确保Colab提供多GPU\n",
    "2. **NCCL通信**：检查GPU间通信是否正常\n",
    "3. **内存分布**：确保各GPU内存使用均匀\n",
    "4. **批次大小**：根据GPU数量调整批次大小\n",
    "5. **分布式策略**：确保DDP策略正确配置\n",
    "\n",
    "### 🎉 恭喜！\n",
    "您现在拥有一个**完全支持多GPU的GDM-Net实现**，它：\n",
    "- 🚀 **多GPU加速**：充分利用并行计算资源\n",
    "- 📊 **学术级结果**：可发表的研究成果\n",
    "- 🔧 **高度可配置**：自动适应不同GPU配置\n",
    "- 📈 **性能监控**：完整的多GPU性能分析\n",
    "\n",
    "祝您多GPU训练顺利！🚀🚀🚀"
   ],
   "metadata": {
    "id": "conclusion"
   }
  }
 ]
}
