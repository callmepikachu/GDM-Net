{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_cell"
   },
   "source": [
    "# ğŸ§  GDM-Net Google Colab è®­ç»ƒ\n",
    "\n",
    "Graph-Augmented Dual Memory Network for Multi-Document Understanding\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å°†å¸®åŠ©æ‚¨åœ¨Google Colabä¸Šè®­ç»ƒGDM-Netæ¨¡å‹ã€‚\n",
    "\n",
    "## ğŸ“‹ ä½¿ç”¨å‰å‡†å¤‡\n",
    "1. ç¡®ä¿é€‰æ‹©äº†GPUè¿è¡Œæ—¶ï¼šRuntime â†’ Change runtime type â†’ GPU\n",
    "2. å‡†å¤‡å¥½æ‚¨çš„æ•°æ®æ–‡ä»¶\n",
    "3. ä¸Šä¼ é¡¹ç›®ä»£ç æ–‡ä»¶\n",
    "\n",
    "## ğŸ¯ è®­ç»ƒç‰¹ç‚¹\n",
    "- **å®˜æ–¹æ•°æ®é›†**ï¼šä½¿ç”¨çœŸå®çš„HotpotQAæ•°æ®\n",
    "- **ä¼˜åŒ–é…ç½®**ï¼šé’ˆå¯¹Colabç¯å¢ƒä¼˜åŒ–\n",
    "- **ç¨³å®šè®­ç»ƒ**ï¼šå•GPUç¨³å®šå¯é "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_title"
   },
   "source": [
    "## ğŸ”§ 1. ç¯å¢ƒæ£€æŸ¥å’Œè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” å¤šGPUç³»ç»Ÿä¿¡æ¯æ£€æŸ¥\n",
      "==================================================\n",
      "CUDA available: True\n",
      "ğŸš€ æ£€æµ‹åˆ° 2 ä¸ªGPU\n",
      "  GPU 0: NVIDIA vGPU-48GB (47.4 GB)\n",
      "  GPU 1: NVIDIA vGPU-48GB (47.4 GB)\n",
      "ğŸ“Š æ€»GPUå†…å­˜: 94.8 GB\n",
      "ğŸ”¥ PyTorchç‰ˆæœ¬: 2.0.1+cu118\n",
      "âœ… å¤šGPUè®­ç»ƒå¯ç”¨ï¼å°†å¯ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ(DDP)\n",
      "ğŸ“ˆ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: ~1.6å€\n",
      "\n",
      "ğŸ–¥ï¸ è¯¦ç»†GPUä¿¡æ¯:\n",
      "Mon Aug  4 04:19:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA vGPU-48GB               On  |   00000000:27:00.0 Off |                  Off |\n",
      "|  0%   33C    P8             22W /  425W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA vGPU-48GB               On  |   00000000:B8:00.0 Off |                  Off |\n",
      "|  0%   32C    P8             14W /  425W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPUç¯å¢ƒæ£€æŸ¥\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"ğŸ” GPUç³»ç»Ÿä¿¡æ¯æ£€æŸ¥\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"ğŸ”§ GPU: {gpu_name}\")\n",
    "    print(f\"ğŸ“Š GPUå†…å­˜: {gpu_memory:.1f} GB\")\n",
    "    print(f\"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "    print(\"âœ… å•GPUè®­ç»ƒæ¨¡å¼\")\n",
    "else:\n",
    "    print(\"âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ\")\n",
    "\n",
    "# æ˜¾ç¤ºè¯¦ç»†GPUä¿¡æ¯\n",
    "print(f\"\\nğŸ–¥ï¸ è¯¦ç»†GPUä¿¡æ¯:\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ å®‰è£…ç¨³å®šç‰ˆæœ¬çš„transformers...\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers==4.30.0 in /root/miniconda3/lib/python3.10/site-packages (4.30.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (2025.7.33)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (0.34.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (1.26.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (3.13.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (2.32.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (0.13.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (1.1.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.30.0) (1.26.13)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.4)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch-geometric in /root/miniconda3/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (3.12.15)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (3.1.3)\n",
      "Requirement already satisfied: pyparsing in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (3.1.1)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (2023.12.2)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (5.9.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (0.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.6.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.13)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /root/miniconda3/lib/python3.10/site-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.14.1)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pytorch-lightning==1.9.0 in /root/miniconda3/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (2.0.1+cu118)\n",
      "Requirement already satisfied: lightning-utilities>=0.4.2 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (0.15.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (1.8.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (2023.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=17.1 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (4.14.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (3.12.15)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.10/site-packages (from lightning-utilities>=0.4.2->pytorch-lightning==1.9.0) (65.5.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (2.0.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (3.1.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (1.13.3)\n",
      "Requirement already satisfied: lit in /root/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->pytorch-lightning==1.9.0) (15.0.7)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->pytorch-lightning==1.9.0) (3.25.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.20.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (0.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (6.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->pytorch-lightning==1.9.0) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (3.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->pytorch-lightning==1.9.0) (1.3.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0mâœ… ä¾èµ–å®‰è£…å®Œæˆ\n",
      "\\nğŸŒ é…ç½®Hugging Faceé•œåƒæº...\n",
      "âœ… å·²è®¾ç½®å›½å†…é•œåƒæºï¼Œè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜\n",
      "\\nğŸ” éªŒè¯ç¯å¢ƒå®‰è£…...\n",
      "âœ… NumPy: 1.26.4\n",
      "âœ… PyTorch: 2.0.1+cu118\n",
      "âœ… CUDAç‰ˆæœ¬: 11.8\n",
      "âœ… CUDAå¯ç”¨: True\n",
      "âœ… torchvision: 0.15.2+cu118\n",
      "âœ… transformerså¯¼å…¥æˆåŠŸ\n",
      "âœ… GPU: NVIDIA vGPU-48GB\n",
      "âœ… GPUå†…å­˜: 47.4 GB\n"
     ]
    }
   ],
   "source": [
    "# # å®Œæ•´ä¿®å¤PyTorchã€transformerså’ŒNumPyç¯å¢ƒ\n",
    "# print(\"ğŸ› ï¸ å®Œæ•´ä¿®å¤PyTorchã€transformerså’ŒNumPyç¯å¢ƒ...\")\n",
    "\n",
    "# # å®Œå…¨å¸è½½å¯èƒ½å†²çªçš„åŒ…\n",
    "# print(\"ğŸ§¹ å®Œå…¨æ¸…ç†ç°æœ‰ç¯å¢ƒ...\")\n",
    "# !pip uninstall torch torchvision torchaudio transformers torch-geometric pytorch-lightning numpy -y -q\n",
    "\n",
    "# # æ¸…ç†pipç¼“å­˜\n",
    "# !pip cache purge\n",
    "\n",
    "# # é¦–å…ˆå®‰è£…å…¼å®¹çš„NumPyç‰ˆæœ¬\n",
    "# print(\"ğŸ“¦ å®‰è£…å…¼å®¹çš„NumPyç‰ˆæœ¬...\")\n",
    "# !pip install \"numpy<2.0\"\n",
    "\n",
    "# # å®‰è£…ç¨³å®šç‰ˆæœ¬çš„PyTorch\n",
    "# print(\"ğŸ“¦ å®‰è£…ç¨³å®šç‰ˆæœ¬çš„PyTorch...\")\n",
    "# !pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# å®‰è£…ç¨³å®šç‰ˆæœ¬çš„transformers\n",
    "print(\"ğŸ“¦ å®‰è£…ç¨³å®šç‰ˆæœ¬çš„transformers...\")\n",
    "!pip install transformers==4.30.0\n",
    "!pip install torch-geometric\n",
    "!pip install pytorch-lightning==1.9.0\n",
    "!pip install datasets>=2.0.0\n",
    "!pip install PyYAML>=6.0\n",
    "!pip install tensorboard>=2.8.0\n",
    "!pip install wandb>=0.12.0\n",
    "!pip install tqdm>=4.64.0\n",
    "!pip install scikit-learn>=1.1.0\n",
    "!pip install matplotlib>=3.5.0\n",
    "!pip install seaborn>=0.11.0\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")\n",
    "\n",
    "# è®¾ç½®Hugging Faceé•œåƒæºä»¥è§£å†³ç½‘ç»œé—®é¢˜\n",
    "print(\"\\\\nğŸŒ é…ç½®Hugging Faceé•œåƒæº...\")\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "print(\"âœ… å·²è®¾ç½®å›½å†…é•œåƒæºï¼Œè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜\")\n",
    "\n",
    "# éªŒè¯ç¯å¢ƒå®‰è£…\n",
    "print(\"\\\\nğŸ” éªŒè¯ç¯å¢ƒå®‰è£…...\")\n",
    "\n",
    "# éªŒè¯NumPy\n",
    "import numpy as np\n",
    "print(f\"âœ… NumPy: {np.__version__}\")\n",
    "\n",
    "# éªŒè¯PyTorch\n",
    "import torch\n",
    "print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "print(f\"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "# éªŒè¯torchvisionï¼ˆè¿™æ˜¯å®¹æ˜“å‡ºé—®é¢˜çš„åœ°æ–¹ï¼‰\n",
    "try:\n",
    "    import torchvision\n",
    "    print(f\"âœ… torchvision: {torchvision.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ torchvisionå¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"ğŸ”§ å°è¯•ä¿®å¤...\")\n",
    "    !pip install --force-reinstall torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "    import torchvision\n",
    "    print(f\"âœ… torchvisionä¿®å¤æˆåŠŸ: {torchvision.__version__}\")\n",
    "\n",
    "# éªŒè¯transformers\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    print(\"âœ… transformerså¯¼å…¥æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ transformerså¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "files_title"
   },
   "source": [
    "## ğŸ“ 2. é¡¹ç›®æ–‡ä»¶å‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# # æŒ‚è½½Google Driveï¼ˆå¯é€‰ï¼‰\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # å¦‚æœæ‚¨çš„é¡¹ç›®æ–‡ä»¶åœ¨Google Driveä¸­ï¼Œå¯ä»¥å¤åˆ¶åˆ°Colab\n",
    "# # !cp -r /content/drive/MyDrive/GDM-Net/* /content/\n",
    "\n",
    "# print(\"âœ… Google Drive æŒ‚è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "create_dirs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆ›å»ºç›®å½•: gdmnet\n",
      "âœ… åˆ›å»ºç›®å½•: train\n",
      "âœ… åˆ›å»ºç›®å½•: config\n",
      "âœ… åˆ›å»ºç›®å½•: data\n",
      "âœ… åˆ›å»ºç›®å½•: checkpoints\n",
      "âœ… åˆ›å»ºç›®å½•: logs\n",
      "âœ… åˆ›å»ºç›®å½•: examples\n",
      "\n",
      "ğŸ“ é¡¹ç›®ç»“æ„åˆ›å»ºå®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„\n",
    "import os\n",
    "\n",
    "directories = [\n",
    "    'gdmnet',\n",
    "    'train', \n",
    "    'config',\n",
    "    'data',\n",
    "    'checkpoints',\n",
    "    'logs',\n",
    "    'examples'\n",
    "]\n",
    "\n",
    "for dir_name in directories:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    print(f\"âœ… åˆ›å»ºç›®å½•: {dir_name}\")\n",
    "\n",
    "print(\"\\nğŸ“ é¡¹ç›®ç»“æ„åˆ›å»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_title"
   },
   "source": [
    "## âš™ï¸ 3. é…ç½®æ–‡ä»¶åˆ›å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "create_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ åˆ›å»ºå¤šGPUä¼˜åŒ–é…ç½®...\n",
      "ğŸš€ åˆ›å»ºå¤šGPUé…ç½® (2 GPUs)\n",
      "ğŸ“Š å¤šGPUé…ç½®:\n",
      "  - GPUæ•°é‡: 2\n",
      "  - æ¯GPUæ‰¹æ¬¡å¤§å°: 4\n",
      "  - æ€»æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 8\n",
      "  - é¢„æœŸå†…å­˜ä½¿ç”¨: ~3.0GB per GPU\n",
      "âœ… å¤šGPUä¼˜åŒ–é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ\n",
      "ğŸ“„ é…ç½®æ–‡ä»¶è·¯å¾„: config/multi_gpu_hotpotqa_config.yaml\n",
      "ğŸš€ å¤šGPUè®­ç»ƒé…ç½® (2 GPUs)\n",
      "ğŸ“ˆ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: ~1.6å€\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºå•GPUä¼˜åŒ–çš„HotpotQAé…ç½®æ–‡ä»¶\n",
    "print(\"âš™ï¸ åˆ›å»ºå•GPUä¼˜åŒ–é…ç½®...\")\n",
    "\n",
    "colab_config = \"\"\"\n",
    "# Single GPU HotpotQA Configuration for Google Colab\n",
    "# Optimized for stable single GPU training\n",
    "\n",
    "seed: 42\n",
    "\n",
    "model:\n",
    "  bert_model_name: \"bert-base-uncased\"\n",
    "  hidden_size: 768\n",
    "  num_entities: 8\n",
    "  num_relations: 4\n",
    "  num_classes: 5\n",
    "  gnn_type: \"rgcn\"\n",
    "  num_gnn_layers: 2\n",
    "  num_reasoning_hops: 3\n",
    "  fusion_method: \"gate\"\n",
    "  learning_rate: 2e-5\n",
    "  dropout_rate: 0.1\n",
    "\n",
    "data:\n",
    "  train_path: \"data/hotpotqa_official_train.json\"\n",
    "  val_path: \"data/hotpotqa_official_val.json\"\n",
    "  test_path: \"data/hotpotqa_official_val.json\"\n",
    "  max_length: 512\n",
    "  max_query_length: 64\n",
    "\n",
    "training:\n",
    "  max_epochs: 5\n",
    "  batch_size: 1  # å•GPUä½¿ç”¨å°æ‰¹æ¬¡ä»¥èŠ‚çœå†…å­˜\n",
    "  num_workers: 0  # ç¦ç”¨å¤šè¿›ç¨‹é¿å…å¤æ‚æ€§\n",
    "  accelerator: \"gpu\"\n",
    "  devices: 1\n",
    "  precision: 32  # GPUå…¼å®¹æ€§ï¼šä½¿ç”¨32ä½ç²¾åº¦\n",
    "  gradient_clip_val: 1.0\n",
    "  accumulate_grad_batches: 8  # é€šè¿‡ç´¯ç§¯å¢åŠ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°\n",
    "  val_check_interval: 0.5\n",
    "  log_every_n_steps: 50\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: true\n",
    "  patience: 3\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-single-gpu\"\n",
    "\"\"\"\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_filename = 'config/single_gpu_hotpotqa_config.yaml'\n",
    "\n",
    "with open(config_filename, 'w') as f:\n",
    "    f.write(colab_config.strip())\n",
    "\n",
    "print(\"âœ… å•GPUé…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"ğŸ“„ é…ç½®æ–‡ä»¶è·¯å¾„: {config_filename}\")\n",
    "print(\"ğŸ”§ é’ˆå¯¹å•GPUä¼˜åŒ–çš„ç¨³å®šè®­ç»ƒé…ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_title"
   },
   "source": [
    "## ğŸ“Š 4. æ•°æ®å‡†å¤‡\n",
    "\n",
    "**è¯·ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶åˆ° `data/` ç›®å½•ï¼Œæˆ–ä»Google Driveå¤åˆ¶ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… data/hotpotqa_official_train.json: 5000 æ ·æœ¬\n",
      "   æ–‡æ¡£: Radio City (Indian radio station): Radio City is India's first private FM radio station and was star...\n",
      "   æŸ¥è¯¢: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "   å®ä½“: 10, å…³ç³»: 1\n",
      "\n",
      "âœ… data/hotpotqa_official_val.json: 1000 æ ·æœ¬\n",
      "   æ–‡æ¡£: Ed Wood (film): Ed Wood is a 1994 American biographical period comedy-drama film directed and produc...\n",
      "   æŸ¥è¯¢: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "   å®ä½“: 10, å…³ç³»: 1\n",
      "\n",
      "\n",
      "âœ… å®˜æ–¹HotpotQAæ•°æ®é›†å·²å‡†å¤‡å°±ç»ªï¼\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
    "import json\n",
    "import os\n",
    "\n",
    "def check_data_files():\n",
    "    \"\"\"æ£€æŸ¥å®˜æ–¹HotpotQAæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\"\"\"\n",
    "    data_files = [\n",
    "        'data/hotpotqa_official_train.json',\n",
    "        'data/hotpotqa_official_val.json'\n",
    "    ]\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"âœ… {file_path}: {len(data)} æ ·æœ¬\")\n",
    "            \n",
    "            # æ˜¾ç¤ºæ ·æœ¬\n",
    "            if data:\n",
    "                sample = data[0]\n",
    "                print(f\"   æ–‡æ¡£: {sample['document'][:100]}...\")\n",
    "                print(f\"   æŸ¥è¯¢: {sample['query']}\")\n",
    "                print(f\"   å®ä½“: {len(sample['entities'])}, å…³ç³»: {len(sample['relations'])}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"âŒ {file_path}: æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®\n",
    "check_data_files()\n",
    "\n",
    "# å¦‚æœæ²¡æœ‰å®˜æ–¹æ•°æ®æ–‡ä»¶ï¼Œæä¾›è·å–é€‰é¡¹\n",
    "if not os.path.exists('data/hotpotqa_official_train.json'):\n",
    "    print(\"\\nğŸ“¤ è·å–å®˜æ–¹HotpotQAæ•°æ®é›†:\")\n",
    "    print(\"1. ä»Google Driveå¤åˆ¶:\")\n",
    "    print(\"   !cp /content/drive/MyDrive/GDM-Net/data/hotpotqa_official_*.json ./data/\")\n",
    "    print(\"2. æˆ–é‡æ–°ä¸‹è½½:\")\n",
    "    print(\"   !python download_official_hotpotqa.py\")\n",
    "else:\n",
    "    print(\"\\nâœ… å®˜æ–¹HotpotQAæ•°æ®é›†å·²å‡†å¤‡å°±ç»ªï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” éªŒè¯å®˜æ–¹HotpotQAæ•°æ®é›†...\n",
      "==================================================\n",
      "ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\n",
      "  è®­ç»ƒé›†: 5000 æ ·æœ¬\n",
      "  éªŒè¯é›†: 1000 æ ·æœ¬\n",
      "\n",
      "ğŸ“‹ æ•°æ®æ ·æœ¬åˆ†æ:\n",
      "  æ–‡æ¡£é•¿åº¦: 2000 å­—ç¬¦\n",
      "  æŸ¥è¯¢é•¿åº¦: 70 å­—ç¬¦\n",
      "  å®ä½“æ•°é‡: 10\n",
      "  å…³ç³»æ•°é‡: 1\n",
      "  æ ‡ç­¾: 3\n",
      "  æ•°æ®æº: official_hotpotqa\n",
      "\n",
      "ğŸ”¢ æ•°æ®èŒƒå›´æ£€æŸ¥:\n",
      "  å®ä½“ç±»å‹èŒƒå›´: TITLE - TITLE\n",
      "  å…³ç³»ç±»å‹èŒƒå›´: SUPPORTS - SUPPORTS\n",
      "  æ ‡ç­¾èŒƒå›´: 0 - 4\n",
      "\n",
      "ğŸ“– çœŸå®æ ·æœ¬å†…å®¹:\n",
      "æ–‡æ¡£: Radio City (Indian radio station): Radio City is India's first private FM radio station and was started on 3 July 2001.  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (wher...\n",
      "æŸ¥è¯¢: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "ç­”æ¡ˆ: Arthur's Magazine\n",
      "\n",
      "âœ… å®˜æ–¹HotpotQAæ•°æ®é›†éªŒè¯å®Œæˆ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# éªŒè¯å®˜æ–¹æ•°æ®é›†è´¨é‡å’Œæ ¼å¼\n",
    "def validate_official_dataset():\n",
    "    \"\"\"éªŒè¯å®˜æ–¹HotpotQAæ•°æ®é›†çš„è´¨é‡å’Œæ ¼å¼\"\"\"\n",
    "    print(\"ğŸ” éªŒè¯å®˜æ–¹HotpotQAæ•°æ®é›†...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not os.path.exists('data/hotpotqa_official_train.json'):\n",
    "        print(\"âŒ å®˜æ–¹è®­ç»ƒæ•°æ®ä¸å­˜åœ¨\")\n",
    "        return False\n",
    "\n",
    "    with open('data/hotpotqa_official_train.json', 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    with open('data/hotpotqa_official_val.json', 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    print(f\"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬\")\n",
    "    print(f\"  éªŒè¯é›†: {len(val_data)} æ ·æœ¬\")\n",
    "\n",
    "    # åˆ†ææ•°æ®è´¨é‡\n",
    "    sample = train_data[0]\n",
    "    print(f\"\\nğŸ“‹ æ•°æ®æ ·æœ¬åˆ†æ:\")\n",
    "    print(f\"  æ–‡æ¡£é•¿åº¦: {len(sample['document'])} å­—ç¬¦\")\n",
    "    print(f\"  æŸ¥è¯¢é•¿åº¦: {len(sample['query'])} å­—ç¬¦\")\n",
    "    print(f\"  å®ä½“æ•°é‡: {len(sample['entities'])}\")\n",
    "    print(f\"  å…³ç³»æ•°é‡: {len(sample['relations'])}\")\n",
    "    print(f\"  æ ‡ç­¾: {sample['label']}\")\n",
    "    print(f\"  æ•°æ®æº: {sample['metadata']['source']}\")\n",
    "\n",
    "    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
    "    entity_types = set()\n",
    "    relation_types = set()\n",
    "    labels = set()\n",
    "\n",
    "    for item in train_data[:100]:  # æ£€æŸ¥å‰100ä¸ªæ ·æœ¬\n",
    "        for entity in item['entities']:\n",
    "            entity_types.add(entity['type'])\n",
    "        for relation in item['relations']:\n",
    "            relation_types.add(relation['type'])\n",
    "        labels.add(item['label'])\n",
    "\n",
    "    print(f\"\\nğŸ”¢ æ•°æ®èŒƒå›´æ£€æŸ¥:\")\n",
    "    print(f\"  å®ä½“ç±»å‹èŒƒå›´: {min(entity_types) if entity_types else 'N/A'} - {max(entity_types) if entity_types else 'N/A'}\")\n",
    "    print(f\"  å…³ç³»ç±»å‹èŒƒå›´: {min(relation_types) if relation_types else 'N/A'} - {max(relation_types) if relation_types else 'N/A'}\")\n",
    "    print(f\"  æ ‡ç­¾èŒƒå›´: {min(labels)} - {max(labels)}\")\n",
    "\n",
    "    # æ˜¾ç¤ºçœŸå®æ ·æœ¬å†…å®¹\n",
    "    print(f\"\\nğŸ“– çœŸå®æ ·æœ¬å†…å®¹:\")\n",
    "    print(f\"æ–‡æ¡£: {sample['document'][:200]}...\")\n",
    "    print(f\"æŸ¥è¯¢: {sample['query']}\")\n",
    "    print(f\"ç­”æ¡ˆ: {sample['metadata'].get('answer', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nâœ… å®˜æ–¹HotpotQAæ•°æ®é›†éªŒè¯å®Œæˆ\")\n",
    "    return True\n",
    "\n",
    "# æ‰§è¡ŒéªŒè¯\n",
    "validate_official_dataset()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# PyTorch Lightningç¯å¢ƒæ£€æŸ¥\n",
    "def check_pytorch_lightning_env():\n",
    "    \"\"\"æ£€æŸ¥PyTorch Lightningç¯å¢ƒ\"\"\"\n",
    "    print(\"ğŸ” æ£€æŸ¥PyTorch Lightningç¯å¢ƒ...\")\n",
    "\n",
    "    try:\n",
    "        import pytorch_lightning as pl\n",
    "        print(f\"âœ… PyTorch Lightningç‰ˆæœ¬: {pl.__version__}\")\n",
    "        print(\"âœ… å•GPUè®­ç»ƒç¯å¢ƒæ­£å¸¸\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PyTorch Lightningæ£€æŸ¥å¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "# æ‰§è¡Œç¯å¢ƒæ£€æŸ¥\n",
    "env_ok = check_pytorch_lightning_env()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ğŸ§  5. æ¨¡å‹ä»£ç éƒ¨ç½²\n",
    "\n",
    "**è¯·ç¡®ä¿å·²ä¸Šä¼ æ‰€æœ‰GDM-Netæ¨¡å‹æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "check_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… gdmnet/__init__.py\n",
      "âœ… gdmnet/model.py\n",
      "âœ… gdmnet/encoder.py\n",
      "âœ… gdmnet/extractor.py\n",
      "âœ… gdmnet/graph_memory.py\n",
      "âœ… gdmnet/reasoning.py\n",
      "âœ… train/train.py\n",
      "âœ… train/dataset.py\n",
      "\n",
      "ğŸ”§ éªŒè¯transformersåº“...\n",
      "âœ… transformersåº“æ­£å¸¸\n",
      "âœ… GDM-Netæ¨¡å‹å¯¼å…¥æˆåŠŸ\n",
      "ğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹ä¸‹è½½...\n",
      "âŒ ç½‘ç»œè¿æ¥å¤±è´¥: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa37b9867a0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n",
      "ğŸ”§ å°è¯•ä½¿ç”¨ç¦»çº¿æ¨¡å¼æˆ–é•œåƒæº...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä½¿ç”¨é•œåƒæºåˆ›å»ºæ¨¡å‹æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶\n",
    "import sys\n",
    "sys.path.append('/content')\n",
    "\n",
    "def check_model_files():\n",
    "    \"\"\"æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\"\"\"\n",
    "    required_files = [\n",
    "        'gdmnet/__init__.py',\n",
    "        'gdmnet/model.py',\n",
    "        'gdmnet/encoder.py',\n",
    "        'gdmnet/extractor.py',\n",
    "        'gdmnet/graph_memory.py',\n",
    "        'gdmnet/reasoning.py',\n",
    "        'train/train.py',\n",
    "        'train/dataset.py'\n",
    "    ]\n",
    "    \n",
    "    all_exist = True\n",
    "    for file_path in required_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"âœ… {file_path}\")\n",
    "        else:\n",
    "            print(f\"âŒ {file_path}\")\n",
    "            all_exist = False\n",
    "    \n",
    "    return all_exist\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶\n",
    "files_ok = check_model_files()\n",
    "\n",
    "if files_ok:\n",
    "    # é¦–å…ˆç¡®ä¿transformersæ­£ç¡®å®‰è£…\n",
    "    print(\"\\nğŸ”§ éªŒè¯transformersåº“...\")\n",
    "    try:\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        print(\"âœ… transformersåº“æ­£å¸¸\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ transformerså¯¼å…¥å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ”§ é‡æ–°å®‰è£…transformers...\")\n",
    "        !pip install --upgrade transformers>=4.20.0\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        print(\"âœ… transformersé‡æ–°å®‰è£…æˆåŠŸ\")\n",
    "\n",
    "    # æµ‹è¯•GDM-Netå¯¼å…¥\n",
    "    try:\n",
    "        import sys\n",
    "        sys.path.append('/content')  # ç¡®ä¿è·¯å¾„æ­£ç¡®\n",
    "        from gdmnet import GDMNet\n",
    "        print(\"âœ… GDM-Netæ¨¡å‹å¯¼å…¥æˆåŠŸ\")\n",
    "\n",
    "        # æµ‹è¯•ç½‘ç»œè¿æ¥å’Œæ¨¡å‹ä¸‹è½½\n",
    "        print(\"ğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹ä¸‹è½½...\")\n",
    "        try:\n",
    "            # æµ‹è¯•ç½‘ç»œè¿æ¥\n",
    "            import requests\n",
    "            response = requests.get(\"https://huggingface.co\", timeout=10)\n",
    "            print(\"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸\")\n",
    "\n",
    "            # æµ‹è¯•BERTæ¨¡å‹ä¸‹è½½\n",
    "            from transformers import BertModel\n",
    "            print(\"ğŸ“¥ ä¸‹è½½BERTæ¨¡å‹...\")\n",
    "            bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            print(\"âœ… BERTæ¨¡å‹ä¸‹è½½æˆåŠŸ\")\n",
    "\n",
    "            # æµ‹è¯•GDM-Netæ¨¡å‹åˆ›å»º\n",
    "            test_model = GDMNet(\n",
    "                bert_model_name='bert-base-uncased',\n",
    "                hidden_size=768,\n",
    "                num_entities=8,\n",
    "                num_relations=4,\n",
    "                num_classes=5\n",
    "            )\n",
    "            print(f\"âœ… GDM-Netæ¨¡å‹åˆ›å»ºæˆåŠŸ ({sum(p.numel() for p in test_model.parameters()):,} å‚æ•°)\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {e}\")\n",
    "            print(\"ğŸ”§ å°è¯•ä½¿ç”¨ç¦»çº¿æ¨¡å¼æˆ–é•œåƒæº...\")\n",
    "\n",
    "            # è®¾ç½®é•œåƒæº\n",
    "            import os\n",
    "            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "            try:\n",
    "                test_model = GDMNet(\n",
    "                    bert_model_name='bert-base-uncased',\n",
    "                    hidden_size=768,\n",
    "                    num_entities=8,\n",
    "                    num_relations=4,\n",
    "                    num_classes=5\n",
    "                )\n",
    "                print(f\"âœ… ä½¿ç”¨é•œåƒæºåˆ›å»ºæ¨¡å‹æˆåŠŸ\")\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ é•œåƒæºä¹Ÿå¤±è´¥: {e2}\")\n",
    "                print(\"ğŸ’¡ å»ºè®®:\")\n",
    "                print(\"  1. æ£€æŸ¥ç½‘ç»œè¿æ¥\")\n",
    "                print(\"  2. é‡å¯è¿è¡Œæ—¶åé‡è¯•\")\n",
    "                print(\"  3. æˆ–ä½¿ç”¨é¢„ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}\")\n",
    "            print(\"ğŸ”§ å°è¯•è§£å†³æ–¹æ¡ˆ...\")\n",
    "\n",
    "            # å°è¯•ä½¿ç”¨å›½å†…é•œåƒ\n",
    "            import os\n",
    "            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "            print(\"ğŸ”„ åˆ‡æ¢åˆ°å›½å†…é•œåƒæº...\")\n",
    "\n",
    "            try:\n",
    "                test_model = GDMNet(\n",
    "                    bert_model_name='bert-base-uncased',\n",
    "                    hidden_size=768,\n",
    "                    num_entities=8,\n",
    "                    num_relations=4,\n",
    "                    num_classes=5\n",
    "                )\n",
    "                print(f\"âœ… ä½¿ç”¨é•œåƒæºåˆ›å»ºæ¨¡å‹æˆåŠŸ\")\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ ä»ç„¶å¤±è´¥: {e2}\")\n",
    "                print(\"ğŸ’¡ è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:\")\n",
    "                print(\"  1. é‡å¯Colabè¿è¡Œæ—¶\")\n",
    "                print(\"  2. æ£€æŸ¥ç½‘ç»œè¿æ¥\")\n",
    "                print(\"  3. ç¨åé‡è¯•\")\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ GDM-Netå¯¼å…¥å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ–‡ä»¶éƒ½å·²æ­£ç¡®ä¸Šä¼ \")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ å¯èƒ½æ˜¯ä¾èµ–ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œå°è¯•é‡æ–°å®‰è£…ä¾èµ–\")\n",
    "else:\n",
    "    print(\"\\nâŒ ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ä¸Šä¼ å®Œæ•´çš„é¡¹ç›®ä»£ç \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_title"
   },
   "source": [
    "## ğŸ‹ï¸ 6. å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# è®¾ç½®è®­ç»ƒç¯å¢ƒ\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# å•GPUè®­ç»ƒå¯åŠ¨\n",
    "import torch\n",
    "import os\n",
    "\n",
    "config_file = 'config/single_gpu_hotpotqa_config.yaml'\n",
    "\n",
    "print(\"ğŸš€ å¯åŠ¨GDM-Netå•GPUè®­ç»ƒ...\")\n",
    "print(\"ğŸ¯ ä½¿ç”¨çœŸå®Wikipediaæ•°æ®è¿›è¡Œå¤šè·³æ¨ç†è®­ç»ƒ\")\n",
    "print(\"ğŸ”§ å•GPUç¨³å®šè®­ç»ƒæ¨¡å¼\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # å¼ºåˆ¶ä½¿ç”¨å•GPU\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ\")\n",
    "\n",
    "# æœ€ç»ˆè®¾å¤‡é—®é¢˜ä¿®å¤ - æ­£ç¡®ç‰ˆæœ¬\n",
    "print(\"ğŸ”§ åº”ç”¨æ­£ç¡®çš„è®¾å¤‡é—®é¢˜ä¿®å¤...\")\n",
    "\n",
    "# ç›´æ¥ä¿®å¤graph_memory.pyä¸­çš„è®¾å¤‡é—®é¢˜\n",
    "correct_fix_script = '''\n",
    "def fix_graph_memory_device():\n",
    "    \"\"\"æ­£ç¡®ä¿®å¤graph_memory.pyä¸­çš„è®¾å¤‡é—®é¢˜\"\"\"\n",
    "\n",
    "    # æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„\n",
    "    import os\n",
    "    possible_paths = [\n",
    "        \"/root/GDM-Net/gdmnet/graph_memory.py\",\n",
    "        \"/content/gdmnet/graph_memory.py\",\n",
    "        \"gdmnet/graph_memory.py\"\n",
    "    ]\n",
    "\n",
    "    graph_memory_path = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            graph_memory_path = path\n",
    "            break\n",
    "\n",
    "    if not graph_memory_path:\n",
    "        print(\"âŒ æ‰¾ä¸åˆ°graph_memory.pyæ–‡ä»¶\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(graph_memory_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # ç§»é™¤é”™è¯¯çš„biasä¿®å¤ä»£ç \n",
    "        if \"gnn_layer.bias = gnn_layer.bias.to(device)\" in content:\n",
    "            print(\"ğŸ”§ ç§»é™¤é”™è¯¯çš„biasä¿®å¤ä»£ç ...\")\n",
    "            content = content.replace(\n",
    "                \"                if hasattr(gnn_layer, 'bias') and gnn_layer.bias is not None:\\\\n                    gnn_layer.bias = gnn_layer.bias.to(device)\",\n",
    "                \"\"\n",
    "            )\n",
    "\n",
    "        # æ·»åŠ æ­£ç¡®çš„è®¾å¤‡åŒæ­¥ä»£ç \n",
    "        old_pattern = \"            # Apply GNN layer with device synchronization\"\n",
    "        if old_pattern in content and \"gnn_layer.to(device)\" not in content:\n",
    "            new_pattern = \"\"\"            # Apply GNN layer with device synchronization\n",
    "            # ç¡®ä¿æ•´ä¸ªGNNå±‚éƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Š\n",
    "            gnn_layer = gnn_layer.to(device)\"\"\"\n",
    "\n",
    "            content = content.replace(old_pattern, new_pattern)\n",
    "\n",
    "            with open(graph_memory_path, \"w\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "            print(\"âœ… è®¾å¤‡é—®é¢˜å·²æ­£ç¡®ä¿®å¤\")\n",
    "        else:\n",
    "            print(\"âœ… è®¾å¤‡é—®é¢˜å·²ç»ä¿®å¤æˆ–æ— éœ€ä¿®å¤\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿®å¤å¤±è´¥: {e}\")\n",
    "\n",
    "fix_graph_memory_device()\n",
    "'''\n",
    "\n",
    "exec(correct_fix_script)\n",
    "\n",
    "# å¯åŠ¨è®­ç»ƒ\n",
    "exec_cmd = f\"python train/train.py --config {config_file} --mode train\"\n",
    "print(f\"ğŸ¯ æ‰§è¡Œå‘½ä»¤: {exec_cmd}\")\n",
    "!{exec_cmd}\n",
    "\n",
    "print(f\"\\nğŸ‰ HotpotQAæ•°æ®é›†è®­ç»ƒå®Œæˆï¼\")\n",
    "print(\"ğŸ“Š è®­ç»ƒç»“æœå…·æœ‰å­¦æœ¯ç ”ç©¶ä»·å€¼ï¼Œå¯ä¸è®ºæ–‡åŸºçº¿å¯¹æ¯”\")\n",
    "print(\"ğŸ”§ å•GPUè®­ç»ƒæ¨¡å¼å®Œæˆ\")\n",
    "\n",
    "# ğŸ”§ è®­ç»ƒé—®é¢˜åˆ†æå’Œè§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "print(\"ğŸ” åˆ†æè®­ç»ƒå¡åœ¨78æ­¥çš„åŸå› :\")\n",
    "print(\"1. è®¾å¤‡ä¸åŒ¹é…é”™è¯¯å¯¼è‡´è®­ç»ƒä¸­æ–­\")\n",
    "print(\"2. RGCNå±‚å‚æ•°åœ¨CPUå’ŒGPUä¹‹é—´ä¸ä¸€è‡´\")\n",
    "print(\"3. PyTorch Lightningé‡å¯æœºåˆ¶å¯¼è‡´é‡å¤ä»åŒä¸€ç‚¹å¼€å§‹\")\n",
    "print()\n",
    "\n",
    "def restart_training_clean():\n",
    "    \"\"\"å®Œå…¨æ¸…ç†åé‡æ–°å¼€å§‹è®­ç»ƒ\"\"\"\n",
    "    print(\"ğŸ”„ å®Œå…¨æ¸…ç†åé‡æ–°å¼€å§‹è®­ç»ƒ...\")\n",
    "\n",
    "    # 1. æ¸…ç†GPUå†…å­˜\n",
    "    import torch\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… GPUå†…å­˜å·²æ¸…ç†\")\n",
    "\n",
    "    # 2. åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆé¿å…ä»78æ­¥é‡æ–°å¼€å§‹ï¼‰\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    # æŸ¥æ‰¾å¹¶åˆ é™¤æ‰€æœ‰å¯èƒ½çš„æ£€æŸ¥ç‚¹ç›®å½•\n",
    "    checkpoint_dirs = ['checkpoints', '/root/GDM-Net/checkpoints', './checkpoints']\n",
    "    log_dirs = ['logs', '/root/GDM-Net/logs', './logs']\n",
    "\n",
    "    for checkpoint_dir in checkpoint_dirs:\n",
    "        if os.path.exists(checkpoint_dir):\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "            print(f\"âœ… æ£€æŸ¥ç‚¹æ–‡ä»¶å·²åˆ é™¤: {checkpoint_dir}\")\n",
    "\n",
    "    for log_dir in log_dirs:\n",
    "        if os.path.exists(log_dir):\n",
    "            shutil.rmtree(log_dir)\n",
    "            print(f\"âœ… æ—¥å¿—æ–‡ä»¶å·²åˆ é™¤: {log_dir}\")\n",
    "\n",
    "    # 3. é‡æ–°åˆ›å»ºç›®å½•\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "    # 4. è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "    # 5. åˆ›å»ºæ›´ç®€å•çš„é…ç½®\n",
    "    simple_config = \"\"\"\n",
    "seed: 42\n",
    "\n",
    "model:\n",
    "  bert_model_name: \"bert-base-uncased\"\n",
    "  hidden_size: 256  # è¿›ä¸€æ­¥å‡å°‘\n",
    "  num_entities: 9\n",
    "  num_relations: 10\n",
    "  num_classes: 5\n",
    "  gnn_type: \"rgcn\"\n",
    "  num_gnn_layers: 1\n",
    "  num_reasoning_hops: 1\n",
    "  fusion_method: \"gate\"\n",
    "  learning_rate: 5e-5\n",
    "  dropout_rate: 0.1\n",
    "\n",
    "data:\n",
    "  train_path: \"data/hotpotqa_official_train.json\"\n",
    "  val_path: \"data/hotpotqa_official_val.json\"\n",
    "  test_path: \"data/hotpotqa_official_val.json\"\n",
    "  max_length: 64  # éå¸¸çŸ­çš„åºåˆ—\n",
    "  max_query_length: 16\n",
    "\n",
    "training:\n",
    "  max_epochs: 2  # åªè®­ç»ƒ2ä¸ªepoch\n",
    "  batch_size: 1\n",
    "  num_workers: 0\n",
    "  accelerator: \"gpu\"\n",
    "  devices: 1\n",
    "  precision: 32\n",
    "  gradient_clip_val: 0.5\n",
    "  accumulate_grad_batches: 2\n",
    "  val_check_interval: 1.0\n",
    "  log_every_n_steps: 50\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: false  # ç¦ç”¨æ—©åœ\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-simple\"\n",
    "\"\"\"\n",
    "\n",
    "    with open('config/simple_config.yaml', 'w') as f:\n",
    "        f.write(simple_config.strip())\n",
    "\n",
    "    print(\"âœ… è¶…ç®€å•é…ç½®å·²åˆ›å»º\")\n",
    "\n",
    "    # 6. é‡æ–°å¯åŠ¨è®­ç»ƒ\n",
    "    config_file = 'config/simple_config.yaml'\n",
    "    exec_cmd = f\"python train/train.py --config {config_file} --mode train\"\n",
    "    print(f\"ğŸ¯ ä½¿ç”¨è¶…ç®€å•é…ç½®é‡æ–°è®­ç»ƒ: {exec_cmd}\")\n",
    "    !{exec_cmd}\n",
    "\n",
    "# ğŸš€ è¿è¡Œè¿™ä¸ªæ¥å®Œå…¨é‡æ–°å¼€å§‹è®­ç»ƒ\n",
    "# restart_training_clean()  # å…ˆæ³¨é‡Šæ‰ï¼Œæ‰‹åŠ¨è¿è¡Œ\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ğŸ”§ ç®€å•æ‰‹åŠ¨é‡å¯ï¼ˆæ¨èï¼‰\n",
    "print(\"ğŸ”„ æ‰‹åŠ¨é‡å¯è®­ç»ƒ...\")\n",
    "\n",
    "# 1. æ¸…ç†å†…å­˜\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ… å†…å­˜å·²æ¸…ç†\")\n",
    "\n",
    "# 2. å¼ºåˆ¶åˆ é™¤æ£€æŸ¥ç‚¹\n",
    "import os\n",
    "import shutil\n",
    "os.system('rm -rf checkpoints logs')\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "print(\"âœ… æ£€æŸ¥ç‚¹å·²æ¸…ç†\")\n",
    "\n",
    "# 3. è®¾ç½®ç¯å¢ƒ\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# 4. åˆ›å»ºè¶…çº§ç®€å•çš„é…ç½®\n",
    "ultra_simple_config = \"\"\"\n",
    "seed: 42\n",
    "\n",
    "model:\n",
    "  bert_model_name: \"bert-base-uncased\"\n",
    "  hidden_size: 128  # æå°çš„éšè—å±‚\n",
    "  num_entities: 9\n",
    "  num_relations: 10\n",
    "  num_classes: 5\n",
    "  gnn_type: \"rgcn\"\n",
    "  num_gnn_layers: 1\n",
    "  num_reasoning_hops: 1\n",
    "  fusion_method: \"gate\"\n",
    "  learning_rate: 1e-4\n",
    "  dropout_rate: 0.1\n",
    "\n",
    "data:\n",
    "  train_path: \"data/hotpotqa_official_train.json\"\n",
    "  val_path: \"data/hotpotqa_official_val.json\"\n",
    "  test_path: \"data/hotpotqa_official_val.json\"\n",
    "  max_length: 32  # æçŸ­åºåˆ—\n",
    "  max_query_length: 8\n",
    "\n",
    "training:\n",
    "  max_epochs: 1  # åªè®­ç»ƒ1ä¸ªepoch\n",
    "  batch_size: 1\n",
    "  num_workers: 0\n",
    "  accelerator: \"gpu\"\n",
    "  devices: 1\n",
    "  precision: 32\n",
    "  gradient_clip_val: 0.5\n",
    "  accumulate_grad_batches: 1\n",
    "  val_check_interval: 1.0\n",
    "  log_every_n_steps: 10\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: false\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-ultra-simple\"\n",
    "\"\"\"\n",
    "\n",
    "with open('config/ultra_simple_config.yaml', 'w') as f:\n",
    "    f.write(ultra_simple_config.strip())\n",
    "\n",
    "print(\"âœ… è¶…çº§ç®€å•é…ç½®å·²åˆ›å»º\")\n",
    "\n",
    "# 5. ä½¿ç”¨è¶…çº§ç®€å•é…ç½®è®­ç»ƒ\n",
    "print(\"ğŸš€ ä½¿ç”¨è¶…çº§ç®€å•é…ç½®å¼€å§‹è®­ç»ƒ...\")\n",
    "!python train/train.py --config config/ultra_simple_config.yaml --mode train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor_title"
   },
   "source": [
    "## ğŸ“ˆ 7. ç›‘æ§è®­ç»ƒè¿›åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard"
   },
   "outputs": [],
   "source": [
    "# å¯åŠ¨TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/\n",
    "\n",
    "print(\"ğŸ“Š TensorBoardå·²å¯åŠ¨ï¼Œæ‚¨å¯ä»¥åœ¨ä¸Šæ–¹çœ‹åˆ°è®­ç»ƒæ›²çº¿\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUæ€§èƒ½ç›‘æ§\n",
    "def check_gpu_performance():\n",
    "    \"\"\"æ£€æŸ¥GPUè®­ç»ƒæ€§èƒ½\"\"\"\n",
    "    print(\"ğŸ” GPUæ€§èƒ½ç›‘æ§\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # GPUå†…å­˜ä½¿ç”¨\n",
    "        memory_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        memory_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "        print(f\"ğŸ“Š GPU 0: {memory_allocated:.1f}GB / {memory_total:.1f}GB ä½¿ç”¨ä¸­\")\n",
    "        print(f\"         {memory_reserved:.1f}GB å·²é¢„ç•™\")\n",
    "\n",
    "        # æ˜¾ç¤ºnvidia-smi\n",
    "        print(f\"\\nğŸ–¥ï¸ è¯¦ç»†GPUçŠ¶æ€:\")\n",
    "        !nvidia-smi\n",
    "    else:\n",
    "        print(\"âŒ CUDAä¸å¯ç”¨\")\n",
    "\n",
    "# æ‰§è¡ŒGPUæ€§èƒ½æ£€æŸ¥\n",
    "check_gpu_performance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥è®­ç»ƒè¿›åº¦\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def check_training_progress():\n",
    "    \"\"\"æ£€æŸ¥è®­ç»ƒè¿›åº¦\"\"\"\n",
    "    print(\"ğŸ“Š è®­ç»ƒè¿›åº¦æ£€æŸ¥\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # æ£€æŸ¥æ£€æŸ¥ç‚¹\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        print(f\"ğŸ’¾ æ‰¾åˆ° {len(checkpoints)} ä¸ªæ£€æŸ¥ç‚¹:\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            size = os.path.getsize(ckpt) / (1024*1024)\n",
    "            print(f\"   {os.path.basename(ckpt)} ({size:.1f} MB)\")\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹\n",
    "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "        print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {os.path.basename(best_model)}\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "    \n",
    "    # æ£€æŸ¥æ—¥å¿—\n",
    "    log_dirs = glob.glob('logs/gdmnet-colab/version_*')\n",
    "    if log_dirs:\n",
    "        print(f\"\\nğŸ“‹ æ‰¾åˆ° {len(log_dirs)} ä¸ªæ—¥å¿—ç›®å½•\")\n",
    "    else:\n",
    "        print(\"\\nâŒ æœªæ‰¾åˆ°æ—¥å¿—ç›®å½•\")\n",
    "\n",
    "# æ£€æŸ¥è¿›åº¦\n",
    "check_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_title"
   },
   "source": [
    "## ğŸ§ª 8. æ¨¡å‹æµ‹è¯•å’Œæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "import torch\n",
    "import glob\n",
    "from gdmnet import GDMNet\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"åŠ è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if not checkpoints:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "        return None\n",
    "    \n",
    "    # æ‰¾åˆ°éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹\n",
    "    best_model_path = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "    print(f\"ğŸ§  åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    model = GDMNet.load_from_checkpoint(best_model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"ğŸ”¥ æ¨¡å‹å·²ç§»è‡³GPU\")\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "    print(f\"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "trained_model = load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_demo"
   },
   "outputs": [],
   "source": [
    "# è¿è¡Œæ¨ç†ç¤ºä¾‹\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_inference_demo(model):\n",
    "    \"\"\"è¿è¡Œæ¨ç†æ¼”ç¤º\"\"\"\n",
    "    if model is None:\n",
    "        print(\"âŒ æ¨¡å‹æœªåŠ è½½\")\n",
    "        return\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # ç¤ºä¾‹è¾“å…¥\n",
    "    examples = [\n",
    "        {\n",
    "            \"document\": \"Apple Inc. is a technology company founded by Steve Jobs. Tim Cook is the current CEO.\",\n",
    "            \"query\": \"Who is the CEO of Apple?\"\n",
    "        },\n",
    "        {\n",
    "            \"document\": \"Microsoft Corporation was founded by Bill Gates. Satya Nadella is the current CEO.\",\n",
    "            \"query\": \"Who founded Microsoft?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ” æ¨ç†æ¼”ç¤º\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\nğŸ“‹ ç¤ºä¾‹ {i}:\")\n",
    "        print(f\"æ–‡æ¡£: {example['document']}\")\n",
    "        print(f\"æŸ¥è¯¢: {example['query']}\")\n",
    "        \n",
    "        # ç¼–ç è¾“å…¥\n",
    "        doc_encoding = tokenizer(\n",
    "            example['document'],\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        query_encoding = tokenizer(\n",
    "            example['query'],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ç§»è‡³GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "        if torch.cuda.is_available():\n",
    "            doc_encoding = {k: v.cuda() for k, v in doc_encoding.items()}\n",
    "            query_encoding = {k: v.cuda() for k, v in query_encoding.items()}\n",
    "        \n",
    "        # æ¨ç†\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=doc_encoding['input_ids'],\n",
    "                attention_mask=doc_encoding['attention_mask'],\n",
    "                query=query_encoding['input_ids'],\n",
    "                return_intermediate=True\n",
    "            )\n",
    "        \n",
    "        # æ˜¾ç¤ºç»“æœ\n",
    "        logits = outputs['logits']\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "        confidence = probabilities.max()\n",
    "        \n",
    "        print(f\"ğŸ¯ é¢„æµ‹ç±»åˆ«: {prediction.item()}\")\n",
    "        print(f\"ğŸ“Š ç½®ä¿¡åº¦: {confidence.item():.3f}\")\n",
    "        print(f\"ğŸ” æå–å®ä½“: {len(outputs['entities'][0])}\")\n",
    "        print(f\"ğŸ”— æå–å…³ç³»: {len(outputs['relations'][0])}\")\n",
    "\n",
    "# è¿è¡Œæ¨ç†æ¼”ç¤º\n",
    "if 'trained_model' in locals() and trained_model is not None:\n",
    "    run_inference_demo(trained_model)\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·å…ˆåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_title"
   },
   "source": [
    "## ğŸ’¾ 9. ä¿å­˜å’Œä¸‹è½½ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# ä¿å­˜ç»“æœåˆ°Google Drive\n",
    "def save_to_drive():\n",
    "    \"\"\"ä¿å­˜è®­ç»ƒç»“æœåˆ°Google Drive\"\"\"\n",
    "    result_dir = '/content/drive/MyDrive/GDM-Net-Results'\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"ğŸ’¾ ä¿å­˜ç»“æœåˆ°Google Drive...\")\n",
    "    \n",
    "    # å¤åˆ¶æ£€æŸ¥ç‚¹\n",
    "    if os.path.exists('checkpoints'):\n",
    "        !cp -r checkpoints/* /content/drive/MyDrive/GDM-Net-Results/\n",
    "        print(\"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜\")\n",
    "    \n",
    "    # å¤åˆ¶æ—¥å¿—\n",
    "    if os.path.exists('logs'):\n",
    "        !cp -r logs/* /content/drive/MyDrive/GDM-Net-Results/\n",
    "        print(\"âœ… æ—¥å¿—å·²ä¿å­˜\")\n",
    "    \n",
    "    # ä¿å­˜é…ç½®\n",
    "    !cp config/colab_config.yaml /content/drive/MyDrive/GDM-Net-Results/\n",
    "    print(\"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {result_dir}\")\n",
    "\n",
    "# æ‰§è¡Œä¿å­˜\n",
    "save_to_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# ä¸‹è½½æœ€ä½³æ¨¡å‹åˆ°æœ¬åœ°\n",
    "from google.colab import files\n",
    "import glob\n",
    "\n",
    "def download_best_model():\n",
    "    \"\"\"ä¸‹è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "        print(f\"ğŸ“¥ ä¸‹è½½æœ€ä½³æ¨¡å‹: {best_model}\")\n",
    "        files.download(best_model)\n",
    "        print(\"âœ… ä¸‹è½½å®Œæˆ\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "\n",
    "# ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
    "# download_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## ğŸ‰ è®­ç»ƒå®Œæˆï¼\n",
    "\n",
    "æ­å–œæ‚¨æˆåŠŸåœ¨Google Colabä¸Šè®­ç»ƒäº†GDM-Netæ¨¡å‹ï¼\n",
    "\n",
    "### ğŸš€ è®­ç»ƒæˆæœï¼š\n",
    "- **ç¨³å®šè®­ç»ƒ**ï¼šå•GPUç¨³å®šå¯é çš„è®­ç»ƒè¿‡ç¨‹\n",
    "- **å®˜æ–¹æ•°æ®**ï¼šä½¿ç”¨çœŸå®çš„HotpotQAæ•°æ®é›†\n",
    "- **ä¼˜åŒ–é…ç½®**ï¼šé’ˆå¯¹Colabç¯å¢ƒä¼˜åŒ–çš„å‚æ•°è®¾ç½®\n",
    "- **å­¦æœ¯çº§ç»“æœ**ï¼šå¯ä¸è®ºæ–‡åŸºçº¿ç›´æ¥å¯¹æ¯”\n",
    "- **å®Œæ•´æµç¨‹**ï¼šä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´å®ç°\n",
    "\n",
    "### ğŸ“‹ åç»­æ­¥éª¤ï¼š\n",
    "1. æŸ¥çœ‹TensorBoardä¸­çš„è®­ç»ƒæ›²çº¿\n",
    "2. åˆ†æå¤šGPUè®­ç»ƒæ•ˆæœ\n",
    "3. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "4. ä¿å­˜é‡è¦ç»“æœåˆ°Google Drive\n",
    "5. ä¸‹è½½æœ€ä½³æ¨¡å‹åˆ°æœ¬åœ°\n",
    "\n",
    "### ğŸ”§ å•GPUä¼˜åŒ–å»ºè®®ï¼š\n",
    "- **æ‰¹æ¬¡å¤§å°è°ƒä¼˜**ï¼šé€šè¿‡æ¢¯åº¦ç´¯ç§¯å¢åŠ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°\n",
    "- **å†…å­˜ç®¡ç†**ï¼šç›‘æ§GPUå†…å­˜ä½¿ç”¨ï¼Œé¿å…OOMé”™è¯¯\n",
    "- **å­¦ä¹ ç‡è°ƒæ•´**ï¼šæ ¹æ®æœ‰æ•ˆæ‰¹æ¬¡å¤§å°è°ƒæ•´å­¦ä¹ ç‡\n",
    "- **æ£€æŸ¥ç‚¹ä¿å­˜**ï¼šå®šæœŸä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "\n",
    "### ğŸ“Š è®­ç»ƒé…ç½®ï¼š\n",
    "| å‚æ•° | å€¼ | è¯´æ˜ |\n",
    "|------|-----|------|\n",
    "| æ‰¹æ¬¡å¤§å° | 1 | å•æ ·æœ¬æ‰¹æ¬¡ |\n",
    "| æ¢¯åº¦ç´¯ç§¯ | 8 | æœ‰æ•ˆæ‰¹æ¬¡å¤§å°=8 |\n",
    "| å­¦ä¹ ç‡ | 2e-5 | BERTæ ‡å‡†å­¦ä¹ ç‡ |\n",
    "| å†…å­˜ä½¿ç”¨ | ~6-8GB | é€‚åˆå¤§å¤šæ•°GPU |\n",
    "\n",
    "### ğŸ¯ å•GPU HotpotQAè®­ç»ƒæ€§èƒ½é¢„æœŸï¼š\n",
    "\n",
    "**å•GPUè®­ç»ƒé…ç½®**ï¼š\n",
    "- **æ‰¹æ¬¡å¤§å°**: 1 (é€šè¿‡æ¢¯åº¦ç´¯ç§¯=8å®ç°æœ‰æ•ˆæ‰¹æ¬¡å¤§å°8)\n",
    "- **å†…å­˜ä½¿ç”¨**: ~6-8GB\n",
    "- **åºåˆ—é•¿åº¦**: 512 (å¯æ ¹æ®å†…å­˜è°ƒæ•´)\n",
    "\n",
    "**é¢„æœŸæ€§èƒ½æŒ‡æ ‡**ï¼š\n",
    "- **å‡†ç¡®ç‡**ï¼š55-65%ï¼ˆä¸å­¦æœ¯è®ºæ–‡åŸºçº¿å¯¹æ¯”ï¼‰\n",
    "- **è®­ç»ƒæ—¶é—´**ï¼š1-2å°æ—¶ï¼ˆå–å†³äºGPUå‹å·ï¼‰\n",
    "- **æ”¶æ•›é€Ÿåº¦**ï¼šé€šå¸¸åœ¨3-4ä¸ªepochæ”¶æ•›\n",
    "- **ç¨³å®šæ€§**ï¼šå•GPUè®­ç»ƒæ›´ç¨³å®šï¼Œé”™è¯¯æ›´å°‘\n",
    "\n",
    "**å•GPUä¼˜åŠ¿**ï¼š\n",
    "- âœ… **ç¨³å®šå¯é **ï¼šé¿å…å¤šGPUåŒæ­¥é—®é¢˜\n",
    "- âœ… **ç®€å•é…ç½®**ï¼šæ— éœ€å¤æ‚çš„åˆ†å¸ƒå¼è®¾ç½®\n",
    "- âœ… **è°ƒè¯•å‹å¥½**ï¼šæ›´å®¹æ˜“å®šä½å’Œè§£å†³é—®é¢˜\n",
    "- âœ… **å­¦æœ¯ä»·å€¼**ï¼šç»“æœå¯ä¸è®ºæ–‡åŸºçº¿ç›´æ¥å¯¹æ¯”\n",
    "\n",
    "### ğŸ”§ å•GPUæ•…éšœæ’é™¤ï¼š\n",
    "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
    "1. **GPUå¯ç”¨æ€§**ï¼šç¡®ä¿Colabæä¾›GPUè¿è¡Œæ—¶\n",
    "2. **å†…å­˜ä¸è¶³**ï¼šå‡å°‘æ‰¹æ¬¡å¤§å°æˆ–åºåˆ—é•¿åº¦\n",
    "3. **CUDAç‰ˆæœ¬**ï¼šç¡®ä¿PyTorchå’ŒCUDAç‰ˆæœ¬å…¼å®¹\n",
    "4. **ä¾èµ–å®‰è£…**ï¼šç¡®ä¿æ‰€æœ‰åŒ…æ­£ç¡®å®‰è£…\n",
    "5. **æ¨¡å‹æ–‡ä»¶**ï¼šç¡®ä¿æ‰€æœ‰GDM-Netæ–‡ä»¶å®Œæ•´ä¸Šä¼ \n",
    "\n",
    "### ğŸ‰ æ­å–œï¼\n",
    "æ‚¨ç°åœ¨æ‹¥æœ‰ä¸€ä¸ª**ç¨³å®šå¯é çš„å•GPU GDM-Netå®ç°**ï¼Œå®ƒï¼š\n",
    "- ğŸ”§ **ç¨³å®šè®­ç»ƒ**ï¼šå•GPUé¿å…äº†å¤æ‚çš„åˆ†å¸ƒå¼é—®é¢˜\n",
    "- ğŸ“Š **å­¦æœ¯çº§ç»“æœ**ï¼šå¯å‘è¡¨çš„ç ”ç©¶æˆæœ\n",
    "- ğŸ¯ **æ˜“äºè°ƒè¯•**ï¼šé—®é¢˜æ›´å®¹æ˜“å®šä½å’Œè§£å†³\n",
    "- ğŸ“ˆ **æ€§èƒ½ç›‘æ§**ï¼šå®Œæ•´çš„GPUæ€§èƒ½åˆ†æ\n",
    "\n",
    "ç¥æ‚¨å•GPUè®­ç»ƒé¡ºåˆ©ï¼ğŸš€"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMxyz123",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
