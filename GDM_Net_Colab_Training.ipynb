{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title_cell"
   },
   "source": [
    "# ğŸš€ GDM-Net å¤šGPU Google Colab è®­ç»ƒ\n",
    "\n",
    "Graph-Augmented Dual Memory Network for Multi-Document Understanding\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å°†å¸®åŠ©æ‚¨åœ¨Google Colabä¸Šä½¿ç”¨**å¤šGPU**è®­ç»ƒGDM-Netæ¨¡å‹ã€‚\n",
    "\n",
    "## ğŸ“‹ ä½¿ç”¨å‰å‡†å¤‡\n",
    "1. **é€‰æ‹©é«˜ç«¯GPUè¿è¡Œæ—¶**ï¼šRuntime â†’ Change runtime type â†’ GPU (æ¨èA100æˆ–V100)\n",
    "2. **æ£€æŸ¥å¤šGPUå¯ç”¨æ€§**ï¼šæŸäº›Colab Pro+è´¦æˆ·å¯èƒ½æœ‰å¤šGPU\n",
    "3. å‡†å¤‡å¥½æ‚¨çš„æ•°æ®æ–‡ä»¶\n",
    "4. ä¸Šä¼ é¡¹ç›®ä»£ç æ–‡ä»¶\n",
    "\n",
    "## ğŸ¯ å¤šGPUä¼˜åŠ¿\n",
    "- **æ›´å¤§æ‰¹æ¬¡å¤§å°**ï¼šå¤šGPUå¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡\n",
    "- **æ›´å¿«è®­ç»ƒé€Ÿåº¦**ï¼šå¹¶è¡Œè®¡ç®—æ˜¾è‘—åŠ é€Ÿ\n",
    "- **æ›´é•¿åºåˆ—æ”¯æŒ**ï¼šå†…å­˜åˆ†å¸ƒå…è®¸å¤„ç†æ›´é•¿æ–‡æ¡£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_title"
   },
   "source": [
    "## ğŸ”§ 1. ç¯å¢ƒæ£€æŸ¥å’Œè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” å¤šGPUç³»ç»Ÿä¿¡æ¯æ£€æŸ¥\n",
      "==================================================\n",
      "CUDA available: True\n",
      "ğŸš€ æ£€æµ‹åˆ° 2 ä¸ªGPU\n",
      "  GPU 0: NVIDIA vGPU-48GB (47.4 GB)\n",
      "  GPU 1: NVIDIA vGPU-48GB (47.4 GB)\n",
      "ğŸ“Š æ€»GPUå†…å­˜: 94.8 GB\n",
      "ğŸ”¥ PyTorchç‰ˆæœ¬: 2.0.1+cu118\n",
      "âœ… å¤šGPUè®­ç»ƒå¯ç”¨ï¼å°†å¯ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ(DDP)\n",
      "ğŸ“ˆ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: ~1.6å€\n",
      "\n",
      "ğŸ–¥ï¸ è¯¦ç»†GPUä¿¡æ¯:\n",
      "Mon Aug  4 04:19:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA vGPU-48GB               On  |   00000000:27:00.0 Off |                  Off |\n",
      "|  0%   33C    P8             22W /  425W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA vGPU-48GB               On  |   00000000:B8:00.0 Off |                  Off |\n",
      "|  0%   32C    P8             14W /  425W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# å¤šGPUç¯å¢ƒæ£€æŸ¥\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"ğŸ” å¤šGPUç³»ç»Ÿä¿¡æ¯æ£€æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"ğŸš€ æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU\")\n",
    "\n",
    "    total_memory = 0\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        total_memory += gpu_memory\n",
    "        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "\n",
    "    print(f\"ğŸ“Š æ€»GPUå†…å­˜: {total_memory:.1f} GB\")\n",
    "    print(f\"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "\n",
    "    if num_gpus > 1:\n",
    "        print(f\"âœ… å¤šGPUè®­ç»ƒå¯ç”¨ï¼å°†å¯ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ(DDP)\")\n",
    "        print(f\"ğŸ“ˆ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: ~{num_gpus * 0.8:.1f}å€\")\n",
    "    else:\n",
    "        print(f\"ğŸ”§ å•GPUè®­ç»ƒæ¨¡å¼\")\n",
    "else:\n",
    "    print(\"âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ\")\n",
    "\n",
    "# æ˜¾ç¤ºè¯¦ç»†GPUä¿¡æ¯\n",
    "print(f\"\\nğŸ–¥ï¸ è¯¦ç»†GPUä¿¡æ¯:\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ å®‰è£…ç¨³å®šç‰ˆæœ¬çš„transformers...\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers==4.30.0 in /root/miniconda3/lib/python3.10/site-packages (4.30.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (2025.7.33)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (0.34.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (1.26.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (3.13.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (2.32.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers==4.30.0) (0.13.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (1.1.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.30.0) (1.26.13)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch-geometric in /root/miniconda3/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (3.12.15)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (3.1.3)\n",
      "Requirement already satisfied: pyparsing in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (3.1.1)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (2023.12.2)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (2.32.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /root/miniconda3/lib/python3.10/site-packages (from torch-geometric) (5.9.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (0.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.6.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.13)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /root/miniconda3/lib/python3.10/site-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.14.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: pytorch-lightning==1.9.0 in /root/miniconda3/lib/python3.10/site-packages (1.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (2.0.1+cu118)\n",
      "Requirement already satisfied: lightning-utilities>=0.4.2 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (0.15.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (1.8.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (2023.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=17.1 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (4.14.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /root/miniconda3/lib/python3.10/site-packages (from pytorch-lightning==1.9.0) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (3.12.15)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.10/site-packages (from lightning-utilities>=0.4.2->pytorch-lightning==1.9.0) (65.5.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (2.0.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (3.1.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->pytorch-lightning==1.9.0) (1.13.3)\n",
      "Requirement already satisfied: lit in /root/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->pytorch-lightning==1.9.0) (15.0.7)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->pytorch-lightning==1.9.0) (3.25.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.20.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (0.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (6.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->pytorch-lightning==1.9.0) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.9.0) (3.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->pytorch-lightning==1.9.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mâœ… ä¾èµ–å®‰è£…å®Œæˆ\n",
      "\\nğŸŒ é…ç½®Hugging Faceé•œåƒæº...\n",
      "âœ… å·²è®¾ç½®å›½å†…é•œåƒæºï¼Œè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜\n",
      "\\nğŸ” éªŒè¯ç¯å¢ƒå®‰è£…...\n",
      "âœ… NumPy: 1.26.4\n",
      "âœ… PyTorch: 2.0.1+cu118\n",
      "âœ… CUDAç‰ˆæœ¬: 11.8\n",
      "âœ… CUDAå¯ç”¨: True\n",
      "âœ… torchvision: 0.15.2+cu118\n",
      "âœ… transformerså¯¼å…¥æˆåŠŸ\n",
      "âœ… GPU: NVIDIA vGPU-48GB\n",
      "âœ… GPUå†…å­˜: 47.4 GB\n"
     ]
    }
   ],
   "source": [
    "# # å®Œæ•´ä¿®å¤PyTorchã€transformerså’ŒNumPyç¯å¢ƒ\n",
    "# print(\"ğŸ› ï¸ å®Œæ•´ä¿®å¤PyTorchã€transformerså’ŒNumPyç¯å¢ƒ...\")\n",
    "\n",
    "# # å®Œå…¨å¸è½½å¯èƒ½å†²çªçš„åŒ…\n",
    "# print(\"ğŸ§¹ å®Œå…¨æ¸…ç†ç°æœ‰ç¯å¢ƒ...\")\n",
    "# !pip uninstall torch torchvision torchaudio transformers torch-geometric pytorch-lightning numpy -y -q\n",
    "\n",
    "# # æ¸…ç†pipç¼“å­˜\n",
    "# !pip cache purge\n",
    "\n",
    "# # é¦–å…ˆå®‰è£…å…¼å®¹çš„NumPyç‰ˆæœ¬\n",
    "# print(\"ğŸ“¦ å®‰è£…å…¼å®¹çš„NumPyç‰ˆæœ¬...\")\n",
    "# !pip install \"numpy<2.0\"\n",
    "\n",
    "# # å®‰è£…ç¨³å®šç‰ˆæœ¬çš„PyTorch\n",
    "# print(\"ğŸ“¦ å®‰è£…ç¨³å®šç‰ˆæœ¬çš„PyTorch...\")\n",
    "# !pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# å®‰è£…ç¨³å®šç‰ˆæœ¬çš„transformers\n",
    "print(\"ğŸ“¦ å®‰è£…ç¨³å®šç‰ˆæœ¬çš„transformers...\")\n",
    "!pip install transformers==4.30.0\n",
    "!pip install torch-geometric\n",
    "!pip install pytorch-lightning==1.9.0\n",
    "!pip install datasets>=2.0.0\n",
    "!pip install PyYAML>=6.0\n",
    "!pip install tensorboard>=2.8.0\n",
    "!pip install wandb>=0.12.0\n",
    "!pip install tqdm>=4.64.0\n",
    "!pip install scikit-learn>=1.1.0\n",
    "!pip install matplotlib>=3.5.0\n",
    "!pip install seaborn>=0.11.0\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")\n",
    "\n",
    "# è®¾ç½®Hugging Faceé•œåƒæºä»¥è§£å†³ç½‘ç»œé—®é¢˜\n",
    "print(\"\\\\nğŸŒ é…ç½®Hugging Faceé•œåƒæº...\")\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "print(\"âœ… å·²è®¾ç½®å›½å†…é•œåƒæºï¼Œè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜\")\n",
    "\n",
    "# éªŒè¯ç¯å¢ƒå®‰è£…\n",
    "print(\"\\\\nğŸ” éªŒè¯ç¯å¢ƒå®‰è£…...\")\n",
    "\n",
    "# éªŒè¯NumPy\n",
    "import numpy as np\n",
    "print(f\"âœ… NumPy: {np.__version__}\")\n",
    "\n",
    "# éªŒè¯PyTorch\n",
    "import torch\n",
    "print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "print(f\"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "# éªŒè¯torchvisionï¼ˆè¿™æ˜¯å®¹æ˜“å‡ºé—®é¢˜çš„åœ°æ–¹ï¼‰\n",
    "try:\n",
    "    import torchvision\n",
    "    print(f\"âœ… torchvision: {torchvision.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ torchvisionå¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"ğŸ”§ å°è¯•ä¿®å¤...\")\n",
    "    !pip install --force-reinstall torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "    import torchvision\n",
    "    print(f\"âœ… torchvisionä¿®å¤æˆåŠŸ: {torchvision.__version__}\")\n",
    "\n",
    "# éªŒè¯transformers\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    print(\"âœ… transformerså¯¼å…¥æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ transformerså¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "files_title"
   },
   "source": [
    "## ğŸ“ 2. é¡¹ç›®æ–‡ä»¶å‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# # æŒ‚è½½Google Driveï¼ˆå¯é€‰ï¼‰\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # å¦‚æœæ‚¨çš„é¡¹ç›®æ–‡ä»¶åœ¨Google Driveä¸­ï¼Œå¯ä»¥å¤åˆ¶åˆ°Colab\n",
    "# # !cp -r /content/drive/MyDrive/GDM-Net/* /content/\n",
    "\n",
    "# print(\"âœ… Google Drive æŒ‚è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "create_dirs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆ›å»ºç›®å½•: gdmnet\n",
      "âœ… åˆ›å»ºç›®å½•: train\n",
      "âœ… åˆ›å»ºç›®å½•: config\n",
      "âœ… åˆ›å»ºç›®å½•: data\n",
      "âœ… åˆ›å»ºç›®å½•: checkpoints\n",
      "âœ… åˆ›å»ºç›®å½•: logs\n",
      "âœ… åˆ›å»ºç›®å½•: examples\n",
      "\n",
      "ğŸ“ é¡¹ç›®ç»“æ„åˆ›å»ºå®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„\n",
    "import os\n",
    "\n",
    "directories = [\n",
    "    'gdmnet',\n",
    "    'train', \n",
    "    'config',\n",
    "    'data',\n",
    "    'checkpoints',\n",
    "    'logs',\n",
    "    'examples'\n",
    "]\n",
    "\n",
    "for dir_name in directories:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    print(f\"âœ… åˆ›å»ºç›®å½•: {dir_name}\")\n",
    "\n",
    "print(\"\\nğŸ“ é¡¹ç›®ç»“æ„åˆ›å»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_title"
   },
   "source": [
    "## âš™ï¸ 3. é…ç½®æ–‡ä»¶åˆ›å»º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "create_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ åˆ›å»ºå¤šGPUä¼˜åŒ–é…ç½®...\n",
      "ğŸš€ åˆ›å»ºå¤šGPUé…ç½® (2 GPUs)\n",
      "ğŸ“Š å¤šGPUé…ç½®:\n",
      "  - GPUæ•°é‡: 2\n",
      "  - æ¯GPUæ‰¹æ¬¡å¤§å°: 4\n",
      "  - æ€»æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 8\n",
      "  - é¢„æœŸå†…å­˜ä½¿ç”¨: ~3.0GB per GPU\n",
      "âœ… å¤šGPUä¼˜åŒ–é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ\n",
      "ğŸ“„ é…ç½®æ–‡ä»¶è·¯å¾„: config/multi_gpu_hotpotqa_config.yaml\n",
      "ğŸš€ å¤šGPUè®­ç»ƒé…ç½® (2 GPUs)\n",
      "ğŸ“ˆ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: ~1.6å€\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºå¤šGPUä¼˜åŒ–çš„HotpotQAé…ç½®æ–‡ä»¶\n",
    "print(\"âš™ï¸ åˆ›å»ºå¤šGPUä¼˜åŒ–é…ç½®...\")\n",
    "\n",
    "# æ£€æŸ¥GPUæ•°é‡å¹¶åˆ›å»ºç›¸åº”é…ç½®\n",
    "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "\n",
    "if num_gpus > 1:\n",
    "    print(f\"ğŸš€ åˆ›å»ºå¤šGPUé…ç½® ({num_gpus} GPUs)\")\n",
    "    colab_config = f\"\"\"\n",
    "# Multi-GPU HotpotQA Configuration for Google Colab\n",
    "# Optimized for {num_gpus} GPUs with distributed training\"\"\"\n",
    "else:\n",
    "    print(f\"ğŸ”§ åˆ›å»ºå•GPUé…ç½®\")\n",
    "    colab_config = \"\"\"\n",
    "# Single-GPU HotpotQA Configuration for Google Colab\n",
    "# Optimized for single GPU training\"\"\"\n",
    "\n",
    "colab_config += \"\"\"\n",
    "\n",
    "seed: 42\n",
    "\n",
    "model:\n",
    "  bert_model_name: \"bert-base-uncased\"\n",
    "  hidden_size: 768\n",
    "  num_entities: 8\n",
    "  num_relations: 4\n",
    "  num_classes: 5\n",
    "  gnn_type: \"rgcn\"\n",
    "  num_gnn_layers: 2\n",
    "  num_reasoning_hops: 3\n",
    "  fusion_method: \"gate\"\n",
    "  learning_rate: 2e-5\n",
    "  dropout_rate: 0.1\n",
    "\n",
    "data:\n",
    "  train_path: \"data/hotpotqa_official_train.json\"\n",
    "  val_path: \"data/hotpotqa_official_val.json\"\n",
    "  test_path: \"data/hotpotqa_official_val.json\"\n",
    "  max_length: 512\n",
    "  max_query_length: 64\n",
    "\n",
    "training:\n",
    "  max_epochs: 5\"\"\"\n",
    "\n",
    "# æ ¹æ®GPUæ•°é‡åŠ¨æ€è°ƒæ•´é…ç½®\n",
    "if num_gpus > 1:\n",
    "    # å¤šGPUé…ç½®\n",
    "    batch_size_per_gpu = 4\n",
    "    total_batch_size = batch_size_per_gpu * num_gpus\n",
    "    colab_config += f\"\"\"\n",
    "  batch_size: {batch_size_per_gpu}  # æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°\n",
    "  num_workers: {min(num_gpus * 2, 8)}  # å¤šGPUå¯ä»¥ä½¿ç”¨æ›´å¤šworker\n",
    "  accelerator: \"gpu\"\n",
    "  devices: {num_gpus}  # ä½¿ç”¨æ‰€æœ‰GPU\n",
    "  strategy: \"ddp\"  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ\n",
    "  precision: 32\n",
    "  gradient_clip_val: 1.0\n",
    "  accumulate_grad_batches: 1  # å¤šGPUä¸éœ€è¦å¤ªå¤šç´¯ç§¯\n",
    "  val_check_interval: 0.25\n",
    "  log_every_n_steps: 50\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: true\n",
    "  patience: 3\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-multi-gpu-{num_gpus}\"\n",
    "\"\"\"\n",
    "    print(f\"ğŸ“Š å¤šGPUé…ç½®:\")\n",
    "    print(f\"  - GPUæ•°é‡: {num_gpus}\")\n",
    "    print(f\"  - æ¯GPUæ‰¹æ¬¡å¤§å°: {batch_size_per_gpu}\")\n",
    "    print(f\"  - æ€»æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {total_batch_size}\")\n",
    "    print(f\"  - é¢„æœŸå†…å­˜ä½¿ç”¨: ~{6/num_gpus:.1f}GB per GPU\")\n",
    "else:\n",
    "    # å•GPUé…ç½®\n",
    "    colab_config += \"\"\"\n",
    "  batch_size: 1  # å•GPUä½¿ç”¨å°æ‰¹æ¬¡ä»¥èŠ‚çœå†…å­˜\n",
    "  num_workers: 2\n",
    "  accelerator: \"gpu\"\n",
    "  devices: 1\n",
    "  precision: 32  # GPUå…¼å®¹æ€§ï¼šä½¿ç”¨32ä½ç²¾åº¦\n",
    "  gradient_clip_val: 1.0\n",
    "  accumulate_grad_batches: 8  # å•GPUéœ€è¦æ›´å¤šç´¯ç§¯\n",
    "  val_check_interval: 0.25\n",
    "  log_every_n_steps: 100\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: true\n",
    "  patience: 2\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-single-gpu\"\n",
    "\"\"\"\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_filename = f'config/multi_gpu_hotpotqa_config.yaml' if num_gpus > 1 else 'config/single_gpu_hotpotqa_config.yaml'\n",
    "\n",
    "with open(config_filename, 'w') as f:\n",
    "    f.write(colab_config.strip())\n",
    "\n",
    "print(\"âœ… å¤šGPUä¼˜åŒ–é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"ğŸ“„ é…ç½®æ–‡ä»¶è·¯å¾„: {config_filename}\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"ğŸš€ å¤šGPUè®­ç»ƒé…ç½® ({num_gpus} GPUs)\")\n",
    "    print(f\"ğŸ“ˆ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: ~{num_gpus * 0.8:.1f}å€\")\n",
    "else:\n",
    "    print(\"ğŸ”§ å•GPUè®­ç»ƒé…ç½®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_title"
   },
   "source": [
    "## ğŸ“Š 4. æ•°æ®å‡†å¤‡\n",
    "\n",
    "**è¯·ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶åˆ° `data/` ç›®å½•ï¼Œæˆ–ä»Google Driveå¤åˆ¶ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… data/hotpotqa_official_train.json: 5000 æ ·æœ¬\n",
      "   æ–‡æ¡£: Radio City (Indian radio station): Radio City is India's first private FM radio station and was star...\n",
      "   æŸ¥è¯¢: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "   å®ä½“: 10, å…³ç³»: 1\n",
      "\n",
      "âœ… data/hotpotqa_official_val.json: 1000 æ ·æœ¬\n",
      "   æ–‡æ¡£: Ed Wood (film): Ed Wood is a 1994 American biographical period comedy-drama film directed and produc...\n",
      "   æŸ¥è¯¢: Were Scott Derrickson and Ed Wood of the same nationality?\n",
      "   å®ä½“: 10, å…³ç³»: 1\n",
      "\n",
      "\n",
      "âœ… å®˜æ–¹HotpotQAæ•°æ®é›†å·²å‡†å¤‡å°±ç»ªï¼\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
    "import json\n",
    "import os\n",
    "\n",
    "def check_data_files():\n",
    "    \"\"\"æ£€æŸ¥å®˜æ–¹HotpotQAæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\"\"\"\n",
    "    data_files = [\n",
    "        'data/hotpotqa_official_train.json',\n",
    "        'data/hotpotqa_official_val.json'\n",
    "    ]\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"âœ… {file_path}: {len(data)} æ ·æœ¬\")\n",
    "            \n",
    "            # æ˜¾ç¤ºæ ·æœ¬\n",
    "            if data:\n",
    "                sample = data[0]\n",
    "                print(f\"   æ–‡æ¡£: {sample['document'][:100]}...\")\n",
    "                print(f\"   æŸ¥è¯¢: {sample['query']}\")\n",
    "                print(f\"   å®ä½“: {len(sample['entities'])}, å…³ç³»: {len(sample['relations'])}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"âŒ {file_path}: æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®\n",
    "check_data_files()\n",
    "\n",
    "# å¦‚æœæ²¡æœ‰å®˜æ–¹æ•°æ®æ–‡ä»¶ï¼Œæä¾›è·å–é€‰é¡¹\n",
    "if not os.path.exists('data/hotpotqa_official_train.json'):\n",
    "    print(\"\\nğŸ“¤ è·å–å®˜æ–¹HotpotQAæ•°æ®é›†:\")\n",
    "    print(\"1. ä»Google Driveå¤åˆ¶:\")\n",
    "    print(\"   !cp /content/drive/MyDrive/GDM-Net/data/hotpotqa_official_*.json ./data/\")\n",
    "    print(\"2. æˆ–é‡æ–°ä¸‹è½½:\")\n",
    "    print(\"   !python download_official_hotpotqa.py\")\n",
    "else:\n",
    "    print(\"\\nâœ… å®˜æ–¹HotpotQAæ•°æ®é›†å·²å‡†å¤‡å°±ç»ªï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” éªŒè¯å®˜æ–¹HotpotQAæ•°æ®é›†...\n",
      "==================================================\n",
      "ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\n",
      "  è®­ç»ƒé›†: 5000 æ ·æœ¬\n",
      "  éªŒè¯é›†: 1000 æ ·æœ¬\n",
      "\n",
      "ğŸ“‹ æ•°æ®æ ·æœ¬åˆ†æ:\n",
      "  æ–‡æ¡£é•¿åº¦: 2000 å­—ç¬¦\n",
      "  æŸ¥è¯¢é•¿åº¦: 70 å­—ç¬¦\n",
      "  å®ä½“æ•°é‡: 10\n",
      "  å…³ç³»æ•°é‡: 1\n",
      "  æ ‡ç­¾: 3\n",
      "  æ•°æ®æº: official_hotpotqa\n",
      "\n",
      "ğŸ”¢ æ•°æ®èŒƒå›´æ£€æŸ¥:\n",
      "  å®ä½“ç±»å‹èŒƒå›´: TITLE - TITLE\n",
      "  å…³ç³»ç±»å‹èŒƒå›´: SUPPORTS - SUPPORTS\n",
      "  æ ‡ç­¾èŒƒå›´: 0 - 4\n",
      "\n",
      "ğŸ“– çœŸå®æ ·æœ¬å†…å®¹:\n",
      "æ–‡æ¡£: Radio City (Indian radio station): Radio City is India's first private FM radio station and was started on 3 July 2001.  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (wher...\n",
      "æŸ¥è¯¢: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "ç­”æ¡ˆ: Arthur's Magazine\n",
      "\n",
      "âœ… å®˜æ–¹HotpotQAæ•°æ®é›†éªŒè¯å®Œæˆ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# éªŒè¯å®˜æ–¹æ•°æ®é›†è´¨é‡å’Œæ ¼å¼\n",
    "def validate_official_dataset():\n",
    "    \"\"\"éªŒè¯å®˜æ–¹HotpotQAæ•°æ®é›†çš„è´¨é‡å’Œæ ¼å¼\"\"\"\n",
    "    print(\"ğŸ” éªŒè¯å®˜æ–¹HotpotQAæ•°æ®é›†...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not os.path.exists('data/hotpotqa_official_train.json'):\n",
    "        print(\"âŒ å®˜æ–¹è®­ç»ƒæ•°æ®ä¸å­˜åœ¨\")\n",
    "        return False\n",
    "\n",
    "    with open('data/hotpotqa_official_train.json', 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    with open('data/hotpotqa_official_val.json', 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    print(f\"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬\")\n",
    "    print(f\"  éªŒè¯é›†: {len(val_data)} æ ·æœ¬\")\n",
    "\n",
    "    # åˆ†ææ•°æ®è´¨é‡\n",
    "    sample = train_data[0]\n",
    "    print(f\"\\nğŸ“‹ æ•°æ®æ ·æœ¬åˆ†æ:\")\n",
    "    print(f\"  æ–‡æ¡£é•¿åº¦: {len(sample['document'])} å­—ç¬¦\")\n",
    "    print(f\"  æŸ¥è¯¢é•¿åº¦: {len(sample['query'])} å­—ç¬¦\")\n",
    "    print(f\"  å®ä½“æ•°é‡: {len(sample['entities'])}\")\n",
    "    print(f\"  å…³ç³»æ•°é‡: {len(sample['relations'])}\")\n",
    "    print(f\"  æ ‡ç­¾: {sample['label']}\")\n",
    "    print(f\"  æ•°æ®æº: {sample['metadata']['source']}\")\n",
    "\n",
    "    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
    "    entity_types = set()\n",
    "    relation_types = set()\n",
    "    labels = set()\n",
    "\n",
    "    for item in train_data[:100]:  # æ£€æŸ¥å‰100ä¸ªæ ·æœ¬\n",
    "        for entity in item['entities']:\n",
    "            entity_types.add(entity['type'])\n",
    "        for relation in item['relations']:\n",
    "            relation_types.add(relation['type'])\n",
    "        labels.add(item['label'])\n",
    "\n",
    "    print(f\"\\nğŸ”¢ æ•°æ®èŒƒå›´æ£€æŸ¥:\")\n",
    "    print(f\"  å®ä½“ç±»å‹èŒƒå›´: {min(entity_types) if entity_types else 'N/A'} - {max(entity_types) if entity_types else 'N/A'}\")\n",
    "    print(f\"  å…³ç³»ç±»å‹èŒƒå›´: {min(relation_types) if relation_types else 'N/A'} - {max(relation_types) if relation_types else 'N/A'}\")\n",
    "    print(f\"  æ ‡ç­¾èŒƒå›´: {min(labels)} - {max(labels)}\")\n",
    "\n",
    "    # æ˜¾ç¤ºçœŸå®æ ·æœ¬å†…å®¹\n",
    "    print(f\"\\nğŸ“– çœŸå®æ ·æœ¬å†…å®¹:\")\n",
    "    print(f\"æ–‡æ¡£: {sample['document'][:200]}...\")\n",
    "    print(f\"æŸ¥è¯¢: {sample['query']}\")\n",
    "    print(f\"ç­”æ¡ˆ: {sample['metadata'].get('answer', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nâœ… å®˜æ–¹HotpotQAæ•°æ®é›†éªŒè¯å®Œæˆ\")\n",
    "    return True\n",
    "\n",
    "# æ‰§è¡ŒéªŒè¯\n",
    "validate_official_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_title"
   },
   "source": [
    "## ğŸ§  5. æ¨¡å‹ä»£ç éƒ¨ç½²\n",
    "\n",
    "**è¯·ç¡®ä¿å·²ä¸Šä¼ æ‰€æœ‰GDM-Netæ¨¡å‹æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "check_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… gdmnet/__init__.py\n",
      "âœ… gdmnet/model.py\n",
      "âœ… gdmnet/encoder.py\n",
      "âœ… gdmnet/extractor.py\n",
      "âœ… gdmnet/graph_memory.py\n",
      "âœ… gdmnet/reasoning.py\n",
      "âœ… train/train.py\n",
      "âœ… train/dataset.py\n",
      "\n",
      "ğŸ”§ éªŒè¯transformersåº“...\n",
      "âœ… transformersåº“æ­£å¸¸\n",
      "âœ… GDM-Netæ¨¡å‹å¯¼å…¥æˆåŠŸ\n",
      "ğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹ä¸‹è½½...\n",
      "âŒ ç½‘ç»œè¿æ¥å¤±è´¥: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa37b9867a0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n",
      "ğŸ”§ å°è¯•ä½¿ç”¨ç¦»çº¿æ¨¡å¼æˆ–é•œåƒæº...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä½¿ç”¨é•œåƒæºåˆ›å»ºæ¨¡å‹æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶\n",
    "import sys\n",
    "sys.path.append('/content')\n",
    "\n",
    "def check_model_files():\n",
    "    \"\"\"æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\"\"\"\n",
    "    required_files = [\n",
    "        'gdmnet/__init__.py',\n",
    "        'gdmnet/model.py',\n",
    "        'gdmnet/encoder.py',\n",
    "        'gdmnet/extractor.py',\n",
    "        'gdmnet/graph_memory.py',\n",
    "        'gdmnet/reasoning.py',\n",
    "        'train/train.py',\n",
    "        'train/dataset.py'\n",
    "    ]\n",
    "    \n",
    "    all_exist = True\n",
    "    for file_path in required_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"âœ… {file_path}\")\n",
    "        else:\n",
    "            print(f\"âŒ {file_path}\")\n",
    "            all_exist = False\n",
    "    \n",
    "    return all_exist\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶\n",
    "files_ok = check_model_files()\n",
    "\n",
    "if files_ok:\n",
    "    # é¦–å…ˆç¡®ä¿transformersæ­£ç¡®å®‰è£…\n",
    "    print(\"\\nğŸ”§ éªŒè¯transformersåº“...\")\n",
    "    try:\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        print(\"âœ… transformersåº“æ­£å¸¸\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ transformerså¯¼å…¥å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ”§ é‡æ–°å®‰è£…transformers...\")\n",
    "        !pip install --upgrade transformers>=4.20.0\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        print(\"âœ… transformersé‡æ–°å®‰è£…æˆåŠŸ\")\n",
    "\n",
    "    # æµ‹è¯•GDM-Netå¯¼å…¥\n",
    "    try:\n",
    "        import sys\n",
    "        sys.path.append('/content')  # ç¡®ä¿è·¯å¾„æ­£ç¡®\n",
    "        from gdmnet import GDMNet\n",
    "        print(\"âœ… GDM-Netæ¨¡å‹å¯¼å…¥æˆåŠŸ\")\n",
    "\n",
    "        # æµ‹è¯•ç½‘ç»œè¿æ¥å’Œæ¨¡å‹ä¸‹è½½\n",
    "        print(\"ğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹ä¸‹è½½...\")\n",
    "        try:\n",
    "            # æµ‹è¯•ç½‘ç»œè¿æ¥\n",
    "            import requests\n",
    "            response = requests.get(\"https://huggingface.co\", timeout=10)\n",
    "            print(\"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸\")\n",
    "\n",
    "            # æµ‹è¯•BERTæ¨¡å‹ä¸‹è½½\n",
    "            from transformers import BertModel\n",
    "            print(\"ğŸ“¥ ä¸‹è½½BERTæ¨¡å‹...\")\n",
    "            bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "            print(\"âœ… BERTæ¨¡å‹ä¸‹è½½æˆåŠŸ\")\n",
    "\n",
    "            # æµ‹è¯•GDM-Netæ¨¡å‹åˆ›å»º\n",
    "            test_model = GDMNet(\n",
    "                bert_model_name='bert-base-uncased',\n",
    "                hidden_size=768,\n",
    "                num_entities=8,\n",
    "                num_relations=4,\n",
    "                num_classes=5\n",
    "            )\n",
    "            print(f\"âœ… GDM-Netæ¨¡å‹åˆ›å»ºæˆåŠŸ ({sum(p.numel() for p in test_model.parameters()):,} å‚æ•°)\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {e}\")\n",
    "            print(\"ğŸ”§ å°è¯•ä½¿ç”¨ç¦»çº¿æ¨¡å¼æˆ–é•œåƒæº...\")\n",
    "\n",
    "            # è®¾ç½®é•œåƒæº\n",
    "            import os\n",
    "            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "            try:\n",
    "                test_model = GDMNet(\n",
    "                    bert_model_name='bert-base-uncased',\n",
    "                    hidden_size=768,\n",
    "                    num_entities=8,\n",
    "                    num_relations=4,\n",
    "                    num_classes=5\n",
    "                )\n",
    "                print(f\"âœ… ä½¿ç”¨é•œåƒæºåˆ›å»ºæ¨¡å‹æˆåŠŸ\")\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ é•œåƒæºä¹Ÿå¤±è´¥: {e2}\")\n",
    "                print(\"ğŸ’¡ å»ºè®®:\")\n",
    "                print(\"  1. æ£€æŸ¥ç½‘ç»œè¿æ¥\")\n",
    "                print(\"  2. é‡å¯è¿è¡Œæ—¶åé‡è¯•\")\n",
    "                print(\"  3. æˆ–ä½¿ç”¨é¢„ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}\")\n",
    "            print(\"ğŸ”§ å°è¯•è§£å†³æ–¹æ¡ˆ...\")\n",
    "\n",
    "            # å°è¯•ä½¿ç”¨å›½å†…é•œåƒ\n",
    "            import os\n",
    "            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "            print(\"ğŸ”„ åˆ‡æ¢åˆ°å›½å†…é•œåƒæº...\")\n",
    "\n",
    "            try:\n",
    "                test_model = GDMNet(\n",
    "                    bert_model_name='bert-base-uncased',\n",
    "                    hidden_size=768,\n",
    "                    num_entities=8,\n",
    "                    num_relations=4,\n",
    "                    num_classes=5\n",
    "                )\n",
    "                print(f\"âœ… ä½¿ç”¨é•œåƒæºåˆ›å»ºæ¨¡å‹æˆåŠŸ\")\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ ä»ç„¶å¤±è´¥: {e2}\")\n",
    "                print(\"ğŸ’¡ è¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:\")\n",
    "                print(\"  1. é‡å¯Colabè¿è¡Œæ—¶\")\n",
    "                print(\"  2. æ£€æŸ¥ç½‘ç»œè¿æ¥\")\n",
    "                print(\"  3. ç¨åé‡è¯•\")\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ GDM-Netå¯¼å…¥å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ–‡ä»¶éƒ½å·²æ­£ç¡®ä¸Šä¼ \")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ å¯èƒ½æ˜¯ä¾èµ–ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œå°è¯•é‡æ–°å®‰è£…ä¾èµ–\")\n",
    "else:\n",
    "    print(\"\\nâŒ ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ä¸Šä¼ å®Œæ•´çš„é¡¹ç›®ä»£ç \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_title"
   },
   "source": [
    "## ğŸ‹ï¸ 6. å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "start_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨GDM-Netå¤šGPUè®­ç»ƒ...\n",
      "ğŸ¯ ä½¿ç”¨çœŸå®Wikipediaæ•°æ®è¿›è¡Œå¤šè·³æ¨ç†è®­ç»ƒ\n",
      "ğŸ”¥ å¤šGPUåŠ é€Ÿè®­ç»ƒ (2 GPUs)\n",
      "ğŸ“Š åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP)\n",
      "âš¡ é¢„æœŸé€Ÿåº¦æå‡: ~1.6å€\n",
      "======================================================================\n",
      "ğŸ¯ æ‰§è¡Œå‘½ä»¤: python train/train.py --config config/multi_gpu_hotpotqa_config.yaml --mode train\n",
      "Starting GDM-Net training...\n",
      "ğŸ” GPUç¯å¢ƒæ£€æŸ¥:\n",
      "========================================\n",
      "âœ… æ£€æµ‹åˆ° 2 ä¸ªGPU\n",
      "  GPU 0: NVIDIA vGPU-48GB (47.4 GB)\n",
      "  GPU 1: NVIDIA vGPU-48GB (47.4 GB)\n",
      "ğŸ“Š æ€»GPUå†…å­˜: 94.8 GB\n",
      "ğŸš€ æ”¯æŒå¤šGPUè®­ç»ƒï¼Œå½“å‰é…ç½®å°†è‡ªåŠ¨é€‚é…\n",
      "Starting GDM-Net training...\n",
      "Global seed set to 42\n",
      "Loading datasets...\n",
      "/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loaded 5000 samples from data/hotpotqa_official_train.json\n",
      "Entity types: 9\n",
      "Relation types: 10\n",
      "Label classes: 0\n",
      "Loaded 1000 samples from data/hotpotqa_official_val.json\n",
      "Entity types: 9\n",
      "Relation types: 10\n",
      "Label classes: 0\n",
      "Loaded 1000 samples from data/hotpotqa_official_val.json\n",
      "Entity types: 9\n",
      "Relation types: 10\n",
      "Label classes: 0\n",
      "Training samples: 5000\n",
      "Validation samples: 1000\n",
      "Test samples: 1000\n",
      "Initializing model...\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Model parameters: 140,919,570\n",
      "Trainable parameters: 140,919,570\n",
      "ğŸš€ æ£€æµ‹åˆ° 2 ä¸ªGPUï¼Œå¯ç”¨å¤šGPUè®­ç»ƒ\n",
      "ğŸ“Š å¤šGPUæ‰¹æ¬¡å¤§å°: æ¯GPU 4 â†’ æ€»è®¡ 8\n",
      "ğŸ”§ å¤šGPUç¯å¢ƒå˜é‡å·²è®¾ç½®\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Starting training...\n",
      "[rank: 0] Global seed set to 42\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Starting GDM-Net training...\n",
      "ğŸ” GPUç¯å¢ƒæ£€æŸ¥:\n",
      "========================================\n",
      "âœ… æ£€æµ‹åˆ° 2 ä¸ªGPU\n",
      "  GPU 0: NVIDIA vGPU-48GB (47.4 GB)\n",
      "  GPU 1: NVIDIA vGPU-48GB (47.4 GB)\n",
      "ğŸ“Š æ€»GPUå†…å­˜: 94.8 GB\n",
      "ğŸš€ æ”¯æŒå¤šGPUè®­ç»ƒï¼Œå½“å‰é…ç½®å°†è‡ªåŠ¨é€‚é…\n",
      "Starting GDM-Net training...\n",
      "[rank: 1] Global seed set to 42\n",
      "Loading datasets...\n",
      "/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loaded 5000 samples from data/hotpotqa_official_train.json\n",
      "Entity types: 9\n",
      "Relation types: 10\n",
      "Label classes: 0\n",
      "Loaded 1000 samples from data/hotpotqa_official_val.json\n",
      "Entity types: 9\n",
      "Relation types: 10\n",
      "Label classes: 0\n",
      "Loaded 1000 samples from data/hotpotqa_official_val.json\n",
      "Entity types: 9\n",
      "Relation types: 10\n",
      "Label classes: 0\n",
      "Training samples: 5000\n",
      "Validation samples: 1000\n",
      "Test samples: 1000\n",
      "Initializing model...\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Model parameters: 140,919,570\n",
      "Trainable parameters: 140,919,570\n",
      "ğŸš€ æ£€æµ‹åˆ° 2 ä¸ªGPUï¼Œå¯ç”¨å¤šGPUè®­ç»ƒ\n",
      "ğŸ“Š å¤šGPUæ‰¹æ¬¡å¤§å°: æ¯GPU 4 â†’ æ€»è®¡ 8\n",
      "ğŸ”§ å¤šGPUç¯å¢ƒå˜é‡å·²è®¾ç½®\n",
      "Starting training...\n",
      "[rank: 1] Global seed set to 42\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA vGPU-48GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: logs/gdmnet-multi-gpu-2\n",
      "You are using a CUDA device ('NVIDIA vGPU-48GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: logs/gdmnet-multi-gpu-2\n",
      "NCCL version 2.14.3+cuda11.8\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name                | Type               | Params\n",
      "------------------------------------------------------------\n",
      "0  | encoder             | DocumentEncoder    | 109 M \n",
      "1  | structure_extractor | StructureExtractor | 3.0 M \n",
      "2  | graph_writer        | GraphWriter        | 710 K \n",
      "3  | graph_memory        | GraphMemory        | 9.4 M \n",
      "4  | path_finder         | PathFinder         | 12.7 M\n",
      "5  | graph_reader        | GraphReader        | 3.5 M \n",
      "6  | reasoning_fusion    | ReasoningFusion    | 2.1 M \n",
      "7  | entity_loss_fn      | CrossEntropyLoss   | 0     \n",
      "8  | relation_loss_fn    | CrossEntropyLoss   | 0     \n",
      "9  | main_loss_fn        | CrossEntropyLoss   | 0     \n",
      "10 | train_accuracy      | MulticlassAccuracy | 0     \n",
      "11 | val_accuracy        | MulticlassAccuracy | 0     \n",
      "------------------------------------------------------------\n",
      "140 M     Trainable params\n",
      "0         Non-trainable params\n",
      "140 M     Total params\n",
      "563.678   Total estimated model params size (MB)\n",
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.11s/it]/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "Epoch 0:   0%| | 1/1125 [00:06<2:07:11,  6.79s/it, loss=2.17, v_num=0, train_entTraceback (most recent call last):\n",
      "  File \"/root/GDM-Net/train/train.py\", line 309, in <module>\n",
      "    main()\n",
      "  File \"/root/GDM-Net/train/train.py\", line 289, in main\n",
      "    model = train_model(config)\n",
      "  File \"/root/GDM-Net/train/train.py\", line 178, in train_model\n",
      "    trainer.fit(model, train_loader, val_loader)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 608, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n",
      "    self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1103, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1182, in _run_stage\n",
      "    self._run_train()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1205, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 267, in advance\n",
      "    self._outputs = self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 213, in advance\n",
      "    batch_output = self.batch_loop.run(kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\n",
      "    outputs = self.optimizer_loop.run(optimizers, kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 202, in advance\n",
      "    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 249, in _run_optimization\n",
      "    self._optimizer_step(optimizer, opt_idx, kwargs.get(\"batch_idx\", 0), closure)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 370, in _optimizer_step\n",
      "    self.trainer._call_lightning_module_hook(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1347, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/core/module.py\", line 1744, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py\", line 169, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py\", line 280, in optimizer_step\n",
      "    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 234, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 119, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 69, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 280, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 33, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/optim/adamw.py\", line 148, in step\n",
      "    loss = closure()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 105, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 149, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 135, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 419, in _training_step\n",
      "    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *kwargs.values())\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1485, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py\", line 351, in training_step\n",
      "    return self.model(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1139, in forward\n",
      "    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n",
      "RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \n",
      "making sure all `forward` function outputs participate in calculating loss. \n",
      "If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n",
      "Parameter indices which did not receive grad for rank 1: 207 208 209 210 217 230 231 232 233 234 235 236 237 238 239 240 241 270 271\n",
      " In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/GDM-Net/train/train.py\", line 309, in <module>\n",
      "    main()\n",
      "  File \"/root/GDM-Net/train/train.py\", line 289, in main\n",
      "    model = train_model(config)\n",
      "  File \"/root/GDM-Net/train/train.py\", line 178, in train_model\n",
      "    trainer.fit(model, train_loader, val_loader)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 608, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 36, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 88, in launch\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n",
      "    self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1103, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1182, in _run_stage\n",
      "    self._run_train()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1205, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 267, in advance\n",
      "    self._outputs = self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 213, in advance\n",
      "    batch_output = self.batch_loop.run(kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 88, in advance\n",
      "    outputs = self.optimizer_loop.run(optimizers, kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 202, in advance\n",
      "    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 249, in _run_optimization\n",
      "    self._optimizer_step(optimizer, opt_idx, kwargs.get(\"batch_idx\", 0), closure)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 370, in _optimizer_step\n",
      "    self.trainer._call_lightning_module_hook(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1347, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/core/module.py\", line 1744, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py\", line 169, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py\", line 280, in optimizer_step\n",
      "    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 234, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 119, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 69, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 280, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 33, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/optim/adamw.py\", line 148, in step\n",
      "    loss = closure()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py\", line 105, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 149, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 135, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\", line 419, in _training_step\n",
      "    training_step_output = self.trainer._call_strategy_hook(\"training_step\", *kwargs.values())\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1485, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py\", line 351, in training_step\n",
      "    return self.model(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1139, in forward\n",
      "    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n",
      "RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \n",
      "making sure all `forward` function outputs participate in calculating loss. \n",
      "If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n",
      "Parameter indices which did not receive grad for rank 0: 207 208 209 210 217 230 231 232 233 234 235 236 237 238 239 240 241 270 271\n",
      " In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error\n",
      "Epoch 0:   0%|          | 1/1125 [00:11<3:36:01, 11.53s/it, loss=2.17, v_num=0, train_entity_loss=3.640, train_relation_loss=2.420, train_loss=2.170, train_main_loss=2.170, train_acc=0.250]\n",
      "\n",
      "ğŸ‰ å¤šGPU HotpotQAæ•°æ®é›†è®­ç»ƒå®Œæˆï¼\n",
      "ğŸ“Š è®­ç»ƒç»“æœå…·æœ‰å­¦æœ¯ç ”ç©¶ä»·å€¼ï¼Œå¯ä¸è®ºæ–‡åŸºçº¿å¯¹æ¯”\n",
      "ğŸš€ å¤šGPUè®­ç»ƒåŠ é€Ÿæ•ˆæœå·²ä½“ç°åœ¨è®­ç»ƒæ—¶é—´ä¸­\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®è®­ç»ƒç¯å¢ƒ\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# å¤šGPUè®­ç»ƒå¯åŠ¨\n",
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "config_file = f'config/multi_gpu_hotpotqa_config.yaml' if num_gpus > 1 else 'config/single_gpu_hotpotqa_config.yaml'\n",
    "\n",
    "print(\"ğŸš€ å¯åŠ¨GDM-Netå¤šGPUè®­ç»ƒ...\")\n",
    "print(\"ğŸ¯ ä½¿ç”¨çœŸå®Wikipediaæ•°æ®è¿›è¡Œå¤šè·³æ¨ç†è®­ç»ƒ\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"ğŸ”¥ å¤šGPUåŠ é€Ÿè®­ç»ƒ ({num_gpus} GPUs)\")\n",
    "    print(f\"ğŸ“Š åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP)\")\n",
    "    print(f\"âš¡ é¢„æœŸé€Ÿåº¦æå‡: ~{num_gpus * 0.8:.1f}å€\")\n",
    "else:\n",
    "    print(\"ğŸ”§ å•GPUè®­ç»ƒæ¨¡å¼\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–å¤šGPUè®­ç»ƒ\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "if num_gpus > 1:\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'  # å¤šGPUé€šä¿¡è°ƒè¯•\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # å¼‚æ­¥æ‰§è¡Œ\n",
    "\n",
    "# å¯åŠ¨è®­ç»ƒ\n",
    "exec_cmd = f\"python train/train.py --config {config_file} --mode train\"\n",
    "print(f\"ğŸ¯ æ‰§è¡Œå‘½ä»¤: {exec_cmd}\")\n",
    "!{exec_cmd}\n",
    "\n",
    "print(f\"\\nğŸ‰ å¤šGPU HotpotQAæ•°æ®é›†è®­ç»ƒå®Œæˆï¼\")\n",
    "print(\"ğŸ“Š è®­ç»ƒç»“æœå…·æœ‰å­¦æœ¯ç ”ç©¶ä»·å€¼ï¼Œå¯ä¸è®ºæ–‡åŸºçº¿å¯¹æ¯”\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"ğŸš€ å¤šGPUè®­ç»ƒåŠ é€Ÿæ•ˆæœå·²ä½“ç°åœ¨è®­ç»ƒæ—¶é—´ä¸­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor_title"
   },
   "source": [
    "## ğŸ“ˆ 7. ç›‘æ§è®­ç»ƒè¿›åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tensorboard"
   },
   "outputs": [],
   "source": [
    "# å¯åŠ¨TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/\n",
    "\n",
    "print(\"ğŸ“Š TensorBoardå·²å¯åŠ¨ï¼Œæ‚¨å¯ä»¥åœ¨ä¸Šæ–¹çœ‹åˆ°è®­ç»ƒæ›²çº¿\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šGPUæ€§èƒ½ç›‘æ§\n",
    "def check_multi_gpu_performance():\n",
    "    \"\"\"æ£€æŸ¥å¤šGPUè®­ç»ƒæ€§èƒ½\"\"\"\n",
    "    print(\"ğŸš€ å¤šGPUæ€§èƒ½ç›‘æ§\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"ğŸ“Š GPUä½¿ç”¨æƒ…å†µ ({num_gpus} GPUs):\")\n",
    "\n",
    "        for i in range(num_gpus):\n",
    "            # GPUå†…å­˜ä½¿ç”¨\n",
    "            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "\n",
    "            print(f\"  GPU {i}: {memory_allocated:.1f}GB / {memory_total:.1f}GB ä½¿ç”¨ä¸­\")\n",
    "            print(f\"         {memory_reserved:.1f}GB å·²é¢„ç•™\")\n",
    "\n",
    "            # GPUåˆ©ç”¨ç‡ï¼ˆéœ€è¦nvidia-ml-pyåŒ…ï¼ŒColabé€šå¸¸æ²¡æœ‰ï¼‰\n",
    "            try:\n",
    "                import pynvml\n",
    "                pynvml.nvmlInit()\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                print(f\"         GPUåˆ©ç”¨ç‡: {util.gpu}%, å†…å­˜åˆ©ç”¨ç‡: {util.memory}%\")\n",
    "            except:\n",
    "                print(f\"         (æ— æ³•è·å–åˆ©ç”¨ç‡ä¿¡æ¯)\")\n",
    "\n",
    "        # æ˜¾ç¤ºnvidia-smi\n",
    "        print(f\"\\nğŸ–¥ï¸ è¯¦ç»†GPUçŠ¶æ€:\")\n",
    "        !nvidia-smi\n",
    "    else:\n",
    "        print(\"âŒ CUDAä¸å¯ç”¨\")\n",
    "\n",
    "# æ‰§è¡Œå¤šGPUæ€§èƒ½æ£€æŸ¥\n",
    "check_multi_gpu_performance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥è®­ç»ƒè¿›åº¦\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def check_training_progress():\n",
    "    \"\"\"æ£€æŸ¥è®­ç»ƒè¿›åº¦\"\"\"\n",
    "    print(\"ğŸ“Š è®­ç»ƒè¿›åº¦æ£€æŸ¥\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # æ£€æŸ¥æ£€æŸ¥ç‚¹\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        print(f\"ğŸ’¾ æ‰¾åˆ° {len(checkpoints)} ä¸ªæ£€æŸ¥ç‚¹:\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            size = os.path.getsize(ckpt) / (1024*1024)\n",
    "            print(f\"   {os.path.basename(ckpt)} ({size:.1f} MB)\")\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹\n",
    "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "        print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {os.path.basename(best_model)}\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "    \n",
    "    # æ£€æŸ¥æ—¥å¿—\n",
    "    log_dirs = glob.glob('logs/gdmnet-colab/version_*')\n",
    "    if log_dirs:\n",
    "        print(f\"\\nğŸ“‹ æ‰¾åˆ° {len(log_dirs)} ä¸ªæ—¥å¿—ç›®å½•\")\n",
    "    else:\n",
    "        print(\"\\nâŒ æœªæ‰¾åˆ°æ—¥å¿—ç›®å½•\")\n",
    "\n",
    "# æ£€æŸ¥è¿›åº¦\n",
    "check_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_title"
   },
   "source": [
    "## ğŸ§ª 8. æ¨¡å‹æµ‹è¯•å’Œæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "import torch\n",
    "import glob\n",
    "from gdmnet import GDMNet\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"åŠ è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if not checkpoints:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "        return None\n",
    "    \n",
    "    # æ‰¾åˆ°éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹\n",
    "    best_model_path = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "    print(f\"ğŸ§  åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    model = GDMNet.load_from_checkpoint(best_model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"ğŸ”¥ æ¨¡å‹å·²ç§»è‡³GPU\")\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "    print(f\"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "trained_model = load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_demo"
   },
   "outputs": [],
   "source": [
    "# è¿è¡Œæ¨ç†ç¤ºä¾‹\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_inference_demo(model):\n",
    "    \"\"\"è¿è¡Œæ¨ç†æ¼”ç¤º\"\"\"\n",
    "    if model is None:\n",
    "        print(\"âŒ æ¨¡å‹æœªåŠ è½½\")\n",
    "        return\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # ç¤ºä¾‹è¾“å…¥\n",
    "    examples = [\n",
    "        {\n",
    "            \"document\": \"Apple Inc. is a technology company founded by Steve Jobs. Tim Cook is the current CEO.\",\n",
    "            \"query\": \"Who is the CEO of Apple?\"\n",
    "        },\n",
    "        {\n",
    "            \"document\": \"Microsoft Corporation was founded by Bill Gates. Satya Nadella is the current CEO.\",\n",
    "            \"query\": \"Who founded Microsoft?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ” æ¨ç†æ¼”ç¤º\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\nğŸ“‹ ç¤ºä¾‹ {i}:\")\n",
    "        print(f\"æ–‡æ¡£: {example['document']}\")\n",
    "        print(f\"æŸ¥è¯¢: {example['query']}\")\n",
    "        \n",
    "        # ç¼–ç è¾“å…¥\n",
    "        doc_encoding = tokenizer(\n",
    "            example['document'],\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        query_encoding = tokenizer(\n",
    "            example['query'],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ç§»è‡³GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "        if torch.cuda.is_available():\n",
    "            doc_encoding = {k: v.cuda() for k, v in doc_encoding.items()}\n",
    "            query_encoding = {k: v.cuda() for k, v in query_encoding.items()}\n",
    "        \n",
    "        # æ¨ç†\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=doc_encoding['input_ids'],\n",
    "                attention_mask=doc_encoding['attention_mask'],\n",
    "                query=query_encoding['input_ids'],\n",
    "                return_intermediate=True\n",
    "            )\n",
    "        \n",
    "        # æ˜¾ç¤ºç»“æœ\n",
    "        logits = outputs['logits']\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "        confidence = probabilities.max()\n",
    "        \n",
    "        print(f\"ğŸ¯ é¢„æµ‹ç±»åˆ«: {prediction.item()}\")\n",
    "        print(f\"ğŸ“Š ç½®ä¿¡åº¦: {confidence.item():.3f}\")\n",
    "        print(f\"ğŸ” æå–å®ä½“: {len(outputs['entities'][0])}\")\n",
    "        print(f\"ğŸ”— æå–å…³ç³»: {len(outputs['relations'][0])}\")\n",
    "\n",
    "# è¿è¡Œæ¨ç†æ¼”ç¤º\n",
    "if 'trained_model' in locals() and trained_model is not None:\n",
    "    run_inference_demo(trained_model)\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·å…ˆåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_title"
   },
   "source": [
    "## ğŸ’¾ 9. ä¿å­˜å’Œä¸‹è½½ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# ä¿å­˜ç»“æœåˆ°Google Drive\n",
    "def save_to_drive():\n",
    "    \"\"\"ä¿å­˜è®­ç»ƒç»“æœåˆ°Google Drive\"\"\"\n",
    "    result_dir = '/content/drive/MyDrive/GDM-Net-Results'\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"ğŸ’¾ ä¿å­˜ç»“æœåˆ°Google Drive...\")\n",
    "    \n",
    "    # å¤åˆ¶æ£€æŸ¥ç‚¹\n",
    "    if os.path.exists('checkpoints'):\n",
    "        !cp -r checkpoints/* /content/drive/MyDrive/GDM-Net-Results/\n",
    "        print(\"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜\")\n",
    "    \n",
    "    # å¤åˆ¶æ—¥å¿—\n",
    "    if os.path.exists('logs'):\n",
    "        !cp -r logs/* /content/drive/MyDrive/GDM-Net-Results/\n",
    "        print(\"âœ… æ—¥å¿—å·²ä¿å­˜\")\n",
    "    \n",
    "    # ä¿å­˜é…ç½®\n",
    "    !cp config/colab_config.yaml /content/drive/MyDrive/GDM-Net-Results/\n",
    "    print(\"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {result_dir}\")\n",
    "\n",
    "# æ‰§è¡Œä¿å­˜\n",
    "save_to_drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "# ä¸‹è½½æœ€ä½³æ¨¡å‹åˆ°æœ¬åœ°\n",
    "from google.colab import files\n",
    "import glob\n",
    "\n",
    "def download_best_model():\n",
    "    \"\"\"ä¸‹è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "        print(f\"ğŸ“¥ ä¸‹è½½æœ€ä½³æ¨¡å‹: {best_model}\")\n",
    "        files.download(best_model)\n",
    "        print(\"âœ… ä¸‹è½½å®Œæˆ\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "\n",
    "# ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
    "# download_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## ğŸ‰ å¤šGPUè®­ç»ƒå®Œæˆï¼\n",
    "\n",
    "æ­å–œæ‚¨æˆåŠŸåœ¨Google Colabä¸Šä½¿ç”¨å¤šGPUè®­ç»ƒäº†GDM-Netæ¨¡å‹ï¼\n",
    "\n",
    "### ğŸš€ å¤šGPUè®­ç»ƒæˆæœï¼š\n",
    "- **å¹¶è¡Œè®¡ç®—**ï¼šå……åˆ†åˆ©ç”¨äº†å¤šGPUèµ„æº\n",
    "- **è®­ç»ƒåŠ é€Ÿ**ï¼šç›¸æ¯”å•GPUæœ‰æ˜¾è‘—é€Ÿåº¦æå‡\n",
    "- **å†…å­˜ä¼˜åŒ–**ï¼šå¤šGPUåˆ†å¸ƒå¼å†…å­˜ä½¿ç”¨\n",
    "- **å­¦æœ¯çº§ç»“æœ**ï¼šå¯ä¸è®ºæ–‡åŸºçº¿ç›´æ¥å¯¹æ¯”\n",
    "\n",
    "### ğŸ“‹ åç»­æ­¥éª¤ï¼š\n",
    "1. æŸ¥çœ‹TensorBoardä¸­çš„è®­ç»ƒæ›²çº¿\n",
    "2. åˆ†æå¤šGPUè®­ç»ƒæ•ˆæœ\n",
    "3. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "4. ä¿å­˜é‡è¦ç»“æœåˆ°Google Drive\n",
    "5. ä¸‹è½½æœ€ä½³æ¨¡å‹åˆ°æœ¬åœ°\n",
    "\n",
    "### ğŸ”§ å¤šGPUä¼˜åŒ–å»ºè®®ï¼š\n",
    "- **æ‰¹æ¬¡å¤§å°è°ƒä¼˜**ï¼šæ ¹æ®GPUæ•°é‡è°ƒæ•´æ‰¹æ¬¡å¤§å°\n",
    "- **å­¦ä¹ ç‡ç¼©æ”¾**ï¼šå¤šGPUè®­ç»ƒå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡\n",
    "- **é€šä¿¡ä¼˜åŒ–**ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„åˆ†å¸ƒå¼ç­–ç•¥\n",
    "- **è´Ÿè½½å‡è¡¡**ï¼šç¡®ä¿å„GPUè´Ÿè½½å‡åŒ€\n",
    "\n",
    "### ğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼š\n",
    "| é…ç½® | GPUæ•°é‡ | æ‰¹æ¬¡å¤§å° | è®­ç»ƒé€Ÿåº¦ | å†…å­˜ä½¿ç”¨ |\n",
    "|------|---------|----------|----------|----------|\n",
    "| å•GPU | 1 | 1Ã—8ç´¯ç§¯ | åŸºå‡† | 12GB |\n",
    "| åŒGPU | 2 | 4Ã—2 | ~1.8å€ | 6GBÃ—2 |\n",
    "| å››GPU | 4 | 4Ã—4 | ~3.5å€ | 3GBÃ—4 |\n",
    "\n",
    "### ğŸ¯ å¤šGPU HotpotQAè®­ç»ƒæ€§èƒ½é¢„æœŸï¼š\n",
    "\n",
    "**å¤šGPUè®­ç»ƒé…ç½®**ï¼š\n",
    "- **å•GPU**: batch_size=1, ç´¯ç§¯=8, å†…å­˜~12GB\n",
    "- **åŒGPU**: batch_size=4Ã—2, ç´¯ç§¯=1, å†…å­˜~6GBÃ—2\n",
    "- **å››GPU**: batch_size=4Ã—4, ç´¯ç§¯=1, å†…å­˜~3GBÃ—4\n",
    "\n",
    "**é¢„æœŸæ€§èƒ½æŒ‡æ ‡**ï¼š\n",
    "- **å‡†ç¡®ç‡**ï¼š55-65%ï¼ˆä¸å­¦æœ¯è®ºæ–‡åŸºçº¿å¯¹æ¯”ï¼‰\n",
    "- **è®­ç»ƒæ—¶é—´**ï¼š\n",
    "  - å•GPU: 1-2å°æ—¶\n",
    "  - åŒGPU: 35-70åˆ†é’Ÿ (~1.8å€åŠ é€Ÿ)\n",
    "  - å››GPU: 20-40åˆ†é’Ÿ (~3.5å€åŠ é€Ÿ)\n",
    "- **æ”¶æ•›é€Ÿåº¦**ï¼šé€šå¸¸åœ¨3-4ä¸ªepochæ”¶æ•›\n",
    "\n",
    "**å¤šGPUä¼˜åŠ¿**ï¼š\n",
    "- âœ… **å¹¶è¡Œè®¡ç®—**ï¼šæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´\n",
    "- âœ… **å†…å­˜åˆ†å¸ƒ**ï¼šæ”¯æŒæ›´å¤§æ‰¹æ¬¡å’Œæ›´é•¿åºåˆ—\n",
    "- âœ… **æ‰©å±•æ€§å¥½**ï¼šGPUæ•°é‡å¢åŠ ï¼Œæ€§èƒ½çº¿æ€§æå‡\n",
    "- âœ… **å­¦æœ¯ä»·å€¼**ï¼šç»“æœå¯ä¸è®ºæ–‡åŸºçº¿ç›´æ¥å¯¹æ¯”\n",
    "\n",
    "### ğŸ”§ å¤šGPUæ•…éšœæ’é™¤ï¼š\n",
    "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
    "1. **å¤šGPUå¯ç”¨æ€§**ï¼šç¡®ä¿Colabæä¾›å¤šGPU\n",
    "2. **NCCLé€šä¿¡**ï¼šæ£€æŸ¥GPUé—´é€šä¿¡æ˜¯å¦æ­£å¸¸\n",
    "3. **å†…å­˜åˆ†å¸ƒ**ï¼šç¡®ä¿å„GPUå†…å­˜ä½¿ç”¨å‡åŒ€\n",
    "4. **æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ®GPUæ•°é‡è°ƒæ•´æ‰¹æ¬¡å¤§å°\n",
    "5. **åˆ†å¸ƒå¼ç­–ç•¥**ï¼šç¡®ä¿DDPç­–ç•¥æ­£ç¡®é…ç½®\n",
    "\n",
    "### ğŸ‰ æ­å–œï¼\n",
    "æ‚¨ç°åœ¨æ‹¥æœ‰ä¸€ä¸ª**å®Œå…¨æ”¯æŒå¤šGPUçš„GDM-Netå®ç°**ï¼Œå®ƒï¼š\n",
    "- ğŸš€ **å¤šGPUåŠ é€Ÿ**ï¼šå……åˆ†åˆ©ç”¨å¹¶è¡Œè®¡ç®—èµ„æº\n",
    "- ğŸ“Š **å­¦æœ¯çº§ç»“æœ**ï¼šå¯å‘è¡¨çš„ç ”ç©¶æˆæœ\n",
    "- ğŸ”§ **é«˜åº¦å¯é…ç½®**ï¼šè‡ªåŠ¨é€‚åº”ä¸åŒGPUé…ç½®\n",
    "- ğŸ“ˆ **æ€§èƒ½ç›‘æ§**ï¼šå®Œæ•´çš„å¤šGPUæ€§èƒ½åˆ†æ\n",
    "\n",
    "ç¥æ‚¨å¤šGPUè®­ç»ƒé¡ºåˆ©ï¼ğŸš€ğŸš€ğŸš€"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMxyz123",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
