{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyMxyz123"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸš€ GDM-Net å¤šGPU Google Colab è®­ç»ƒ\n",
    "\n",
    "Graph-Augmented Dual Memory Network for Multi-Document Understanding\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å°†å¸®åŠ©æ‚¨åœ¨Google Colabä¸Šä½¿ç”¨**å¤šGPU**è®­ç»ƒGDM-Netæ¨¡å‹ã€‚\n",
    "\n",
    "## ğŸ“‹ ä½¿ç”¨å‰å‡†å¤‡\n",
    "1. **é€‰æ‹©é«˜ç«¯GPUè¿è¡Œæ—¶**ï¼šRuntime â†’ Change runtime type â†’ GPU (æ¨èA100æˆ–V100)\n",
    "2. **æ£€æŸ¥å¤šGPUå¯ç”¨æ€§**ï¼šæŸäº›Colab Pro+è´¦æˆ·å¯èƒ½æœ‰å¤šGPU\n",
    "3. å‡†å¤‡å¥½æ‚¨çš„æ•°æ®æ–‡ä»¶\n",
    "4. ä¸Šä¼ é¡¹ç›®ä»£ç æ–‡ä»¶\n",
    "\n",
    "## ğŸ¯ å¤šGPUä¼˜åŠ¿\n",
    "- **æ›´å¤§æ‰¹æ¬¡å¤§å°**ï¼šå¤šGPUå¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡\n",
    "- **æ›´å¿«è®­ç»ƒé€Ÿåº¦**ï¼šå¹¶è¡Œè®¡ç®—æ˜¾è‘—åŠ é€Ÿ\n",
    "- **æ›´é•¿åºåˆ—æ”¯æŒ**ï¼šå†…å­˜åˆ†å¸ƒå…è®¸å¤„ç†æ›´é•¿æ–‡æ¡£"
   ],
   "metadata": {
    "id": "title_cell"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ”§ 1. ç¯å¢ƒæ£€æŸ¥å’Œè®¾ç½®"
   ],
   "metadata": {
    "id": "setup_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# å¤šGPUç¯å¢ƒæ£€æŸ¥\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"ğŸ” å¤šGPUç³»ç»Ÿä¿¡æ¯æ£€æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"ğŸš€ æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU\")\n",
    "\n",
    "    total_memory = 0\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        total_memory += gpu_memory\n",
    "        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "\n",
    "    print(f\"ğŸ“Š æ€»GPUå†…å­˜: {total_memory:.1f} GB\")\n",
    "    print(f\"ğŸ”¥ PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "\n",
    "    if num_gpus > 1:\n",
    "        print(f\"âœ… å¤šGPUè®­ç»ƒå¯ç”¨ï¼å°†å¯ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ(DDP)\")\n",
    "        print(f\"ğŸ“ˆ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: ~{num_gpus * 0.8:.1f}å€\")\n",
    "    else:\n",
    "        print(f\"ğŸ”§ å•GPUè®­ç»ƒæ¨¡å¼\")\n",
    "else:\n",
    "    print(\"âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ\")\n",
    "\n",
    "# æ˜¾ç¤ºè¯¦ç»†GPUä¿¡æ¯\n",
    "print(f\"\\nğŸ–¥ï¸ è¯¦ç»†GPUä¿¡æ¯:\")\n",
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "check_gpu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# å®Œæ•´ä¿®å¤PyTorchã€transformerså’ŒNumPyç¯å¢ƒ\n",
    "print(\"ğŸ› ï¸ å®Œæ•´ä¿®å¤PyTorchã€transformerså’ŒNumPyç¯å¢ƒ...\")\n",
    "\n",
    "# å®Œå…¨å¸è½½å¯èƒ½å†²çªçš„åŒ…\n",
    "print(\"ğŸ§¹ å®Œå…¨æ¸…ç†ç°æœ‰ç¯å¢ƒ...\")\n",
    "!pip uninstall torch torchvision torchaudio transformers torch-geometric pytorch-lightning numpy -y -q\n",
    "\n",
    "# æ¸…ç†pipç¼“å­˜\n",
    "!pip cache purge\n",
    "\n",
    "# é¦–å…ˆå®‰è£…å…¼å®¹çš„NumPyç‰ˆæœ¬\n",
    "print(\"ğŸ“¦ å®‰è£…å…¼å®¹çš„NumPyç‰ˆæœ¬...\")\n",
    "!pip install \"numpy<2.0\"\n",
    "\n",
    "# å®‰è£…ç¨³å®šç‰ˆæœ¬çš„PyTorch\n",
    "print(\"ğŸ“¦ å®‰è£…ç¨³å®šç‰ˆæœ¬çš„PyTorch...\")\n",
    "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# å®‰è£…ç¨³å®šç‰ˆæœ¬çš„transformers\n",
    "print(\"ğŸ“¦ å®‰è£…ç¨³å®šç‰ˆæœ¬çš„transformers...\")\n",
    "!pip install transformers==4.30.0\n",
    "!pip install torch-geometric\n",
    "!pip install pytorch-lightning==1.9.0\n",
    "!pip install datasets>=2.0.0\n",
    "!pip install PyYAML>=6.0\n",
    "!pip install tensorboard>=2.8.0\n",
    "!pip install wandb>=0.12.0\n",
    "!pip install tqdm>=4.64.0\n",
    "!pip install scikit-learn>=1.1.0\n",
    "!pip install matplotlib>=3.5.0\n",
    "!pip install seaborn>=0.11.0\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")\n",
    "\n",
    "# éªŒè¯ç¯å¢ƒå®‰è£…\n",
    "print(\"\\\\nğŸ” éªŒè¯ç¯å¢ƒå®‰è£…...\")\n",
    "\n",
    "# éªŒè¯NumPy\n",
    "import numpy as np\n",
    "print(f\"âœ… NumPy: {np.__version__}\")\n",
    "\n",
    "# éªŒè¯PyTorch\n",
    "import torch\n",
    "print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "print(f\"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "# éªŒè¯torchvisionï¼ˆè¿™æ˜¯å®¹æ˜“å‡ºé—®é¢˜çš„åœ°æ–¹ï¼‰\n",
    "try:\n",
    "    import torchvision\n",
    "    print(f\"âœ… torchvision: {torchvision.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ torchvisionå¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"ğŸ”§ å°è¯•ä¿®å¤...\")\n",
    "    !pip install --force-reinstall torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "    import torchvision\n",
    "    print(f\"âœ… torchvisionä¿®å¤æˆåŠŸ: {torchvision.__version__}\")\n",
    "\n",
    "# éªŒè¯transformers\n",
    "try:\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    print(\"âœ… transformerså¯¼å…¥æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ transformerså¯¼å…¥å¤±è´¥: {e}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ],
   "metadata": {
    "id": "install_deps"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“ 2. é¡¹ç›®æ–‡ä»¶å‡†å¤‡"
   ],
   "metadata": {
    "id": "files_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# æŒ‚è½½Google Driveï¼ˆå¯é€‰ï¼‰\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# å¦‚æœæ‚¨çš„é¡¹ç›®æ–‡ä»¶åœ¨Google Driveä¸­ï¼Œå¯ä»¥å¤åˆ¶åˆ°Colab\n",
    "# !cp -r /content/drive/MyDrive/GDM-Net/* /content/\n",
    "\n",
    "print(\"âœ… Google Drive æŒ‚è½½å®Œæˆ\")"
   ],
   "metadata": {
    "id": "mount_drive"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„\n",
    "import os\n",
    "\n",
    "directories = [\n",
    "    'gdmnet',\n",
    "    'train', \n",
    "    'config',\n",
    "    'data',\n",
    "    'checkpoints',\n",
    "    'logs',\n",
    "    'examples'\n",
    "]\n",
    "\n",
    "for dir_name in directories:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    print(f\"âœ… åˆ›å»ºç›®å½•: {dir_name}\")\n",
    "\n",
    "print(\"\\nğŸ“ é¡¹ç›®ç»“æ„åˆ›å»ºå®Œæˆ\")"
   ],
   "metadata": {
    "id": "create_dirs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## âš™ï¸ 3. é…ç½®æ–‡ä»¶åˆ›å»º"
   ],
   "metadata": {
    "id": "config_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# åˆ›å»ºå¤šGPUä¼˜åŒ–çš„HotpotQAé…ç½®æ–‡ä»¶\n",
    "print(\"âš™ï¸ åˆ›å»ºå¤šGPUä¼˜åŒ–é…ç½®...\")\n",
    "\n",
    "# æ£€æŸ¥GPUæ•°é‡å¹¶åˆ›å»ºç›¸åº”é…ç½®\n",
    "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "\n",
    "if num_gpus > 1:\n",
    "    print(f\"ğŸš€ åˆ›å»ºå¤šGPUé…ç½® ({num_gpus} GPUs)\")\n",
    "    colab_config = f\"\"\"\n",
    "# Multi-GPU HotpotQA Configuration for Google Colab\n",
    "# Optimized for {num_gpus} GPUs with distributed training\"\"\"\n",
    "else:\n",
    "    print(f\"ğŸ”§ åˆ›å»ºå•GPUé…ç½®\")\n",
    "    colab_config = \"\"\"\n",
    "# Single-GPU HotpotQA Configuration for Google Colab\n",
    "# Optimized for single GPU training\"\"\"\n",
    "\n",
    "colab_config += \"\"\"\n",
    "\n",
    "seed: 42\n",
    "\n",
    "model:\n",
    "  bert_model_name: \"bert-base-uncased\"\n",
    "  hidden_size: 768\n",
    "  num_entities: 8\n",
    "  num_relations: 4\n",
    "  num_classes: 5\n",
    "  gnn_type: \"rgcn\"\n",
    "  num_gnn_layers: 2\n",
    "  num_reasoning_hops: 3\n",
    "  fusion_method: \"gate\"\n",
    "  learning_rate: 2e-5\n",
    "  dropout_rate: 0.1\n",
    "\n",
    "data:\n",
    "  train_path: \"data/hotpotqa_official_train.json\"\n",
    "  val_path: \"data/hotpotqa_official_val.json\"\n",
    "  test_path: \"data/hotpotqa_official_val.json\"\n",
    "  max_length: 512\n",
    "  max_query_length: 64\n",
    "\n",
    "training:\n",
    "  max_epochs: 5\"\"\"\n",
    "\n",
    "# æ ¹æ®GPUæ•°é‡åŠ¨æ€è°ƒæ•´é…ç½®\n",
    "if num_gpus > 1:\n",
    "    # å¤šGPUé…ç½®\n",
    "    batch_size_per_gpu = 4\n",
    "    total_batch_size = batch_size_per_gpu * num_gpus\n",
    "    colab_config += f\"\"\"\n",
    "  batch_size: {batch_size_per_gpu}  # æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°\n",
    "  num_workers: {min(num_gpus * 2, 8)}  # å¤šGPUå¯ä»¥ä½¿ç”¨æ›´å¤šworker\n",
    "  accelerator: \"gpu\"\n",
    "  devices: {num_gpus}  # ä½¿ç”¨æ‰€æœ‰GPU\n",
    "  strategy: \"ddp\"  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ\n",
    "  precision: 32\n",
    "  gradient_clip_val: 1.0\n",
    "  accumulate_grad_batches: 1  # å¤šGPUä¸éœ€è¦å¤ªå¤šç´¯ç§¯\n",
    "  val_check_interval: 0.25\n",
    "  log_every_n_steps: 50\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: true\n",
    "  patience: 3\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-multi-gpu-{num_gpus}\"\n",
    "\"\"\"\n",
    "    print(f\"ğŸ“Š å¤šGPUé…ç½®:\")\n",
    "    print(f\"  - GPUæ•°é‡: {num_gpus}\")\n",
    "    print(f\"  - æ¯GPUæ‰¹æ¬¡å¤§å°: {batch_size_per_gpu}\")\n",
    "    print(f\"  - æ€»æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {total_batch_size}\")\n",
    "    print(f\"  - é¢„æœŸå†…å­˜ä½¿ç”¨: ~{6/num_gpus:.1f}GB per GPU\")\n",
    "else:\n",
    "    # å•GPUé…ç½®\n",
    "    colab_config += \"\"\"\n",
    "  batch_size: 1  # å•GPUä½¿ç”¨å°æ‰¹æ¬¡ä»¥èŠ‚çœå†…å­˜\n",
    "  num_workers: 2\n",
    "  accelerator: \"gpu\"\n",
    "  devices: 1\n",
    "  precision: 32  # GPUå…¼å®¹æ€§ï¼šä½¿ç”¨32ä½ç²¾åº¦\n",
    "  gradient_clip_val: 1.0\n",
    "  accumulate_grad_batches: 8  # å•GPUéœ€è¦æ›´å¤šç´¯ç§¯\n",
    "  val_check_interval: 0.25\n",
    "  log_every_n_steps: 100\n",
    "  checkpoint_dir: \"checkpoints\"\n",
    "  early_stopping: true\n",
    "  patience: 2\n",
    "\n",
    "logging:\n",
    "  type: \"tensorboard\"\n",
    "  save_dir: \"logs\"\n",
    "  name: \"gdmnet-single-gpu\"\n",
    "\"\"\"\n",
    "\n",
    "# ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "config_filename = f'config/multi_gpu_hotpotqa_config.yaml' if num_gpus > 1 else 'config/single_gpu_hotpotqa_config.yaml'\n",
    "\n",
    "with open(config_filename, 'w') as f:\n",
    "    f.write(colab_config.strip())\n",
    "\n",
    "print(\"âœ… å¤šGPUä¼˜åŒ–é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"ğŸ“„ é…ç½®æ–‡ä»¶è·¯å¾„: {config_filename}\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"ğŸš€ å¤šGPUè®­ç»ƒé…ç½® ({num_gpus} GPUs)\")\n",
    "    print(f\"ğŸ“ˆ é¢„æœŸè®­ç»ƒé€Ÿåº¦æå‡: ~{num_gpus * 0.8:.1f}å€\")\n",
    "else:\n",
    "    print(\"ğŸ”§ å•GPUè®­ç»ƒé…ç½®\")"
   ],
   "metadata": {
    "id": "create_config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“Š 4. æ•°æ®å‡†å¤‡\n",
    "\n",
    "**è¯·ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶åˆ° `data/` ç›®å½•ï¼Œæˆ–ä»Google Driveå¤åˆ¶ã€‚**"
   ],
   "metadata": {
    "id": "data_title"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
    "import json\n",
    "import os\n",
    "\n",
    "def check_data_files():\n",
    "    \"\"\"æ£€æŸ¥å®˜æ–¹HotpotQAæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\"\"\"\n",
    "    data_files = [\n",
    "        'data/hotpotqa_official_train.json',\n",
    "        'data/hotpotqa_official_val.json'\n",
    "    ]\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"âœ… {file_path}: {len(data)} æ ·æœ¬\")\n",
    "            \n",
    "            # æ˜¾ç¤ºæ ·æœ¬\n",
    "            if data:\n",
    "                sample = data[0]\n",
    "                print(f\"   æ–‡æ¡£: {sample['document'][:100]}...\")\n",
    "                print(f\"   æŸ¥è¯¢: {sample['query']}\")\n",
    "                print(f\"   å®ä½“: {len(sample['entities'])}, å…³ç³»: {len(sample['relations'])}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"âŒ {file_path}: æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®\n",
    "check_data_files()\n",
    "\n",
    "# å¦‚æœæ²¡æœ‰å®˜æ–¹æ•°æ®æ–‡ä»¶ï¼Œæä¾›è·å–é€‰é¡¹\n",
    "if not os.path.exists('data/hotpotqa_official_train.json'):\n",
    "    print(\"\\nğŸ“¤ è·å–å®˜æ–¹HotpotQAæ•°æ®é›†:\")\n",
    "    print(\"1. ä»Google Driveå¤åˆ¶:\")\n",
    "    print(\"   !cp /content/drive/MyDrive/GDM-Net/data/hotpotqa_official_*.json ./data/\")\n",
    "    print(\"2. æˆ–é‡æ–°ä¸‹è½½:\")\n",
    "    print(\"   !python download_official_hotpotqa.py\")\n",
    "else:\n",
    "    print(\"\\nâœ… å®˜æ–¹HotpotQAæ•°æ®é›†å·²å‡†å¤‡å°±ç»ªï¼\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# éªŒè¯å®˜æ–¹æ•°æ®é›†è´¨é‡å’Œæ ¼å¼\n",
    "def validate_official_dataset():\n",
    "    \"\"\"éªŒè¯å®˜æ–¹HotpotQAæ•°æ®é›†çš„è´¨é‡å’Œæ ¼å¼\"\"\"\n",
    "    print(\"ğŸ” éªŒè¯å®˜æ–¹HotpotQAæ•°æ®é›†...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    if not os.path.exists('data/hotpotqa_official_train.json'):\n",
    "        print(\"âŒ å®˜æ–¹è®­ç»ƒæ•°æ®ä¸å­˜åœ¨\")\n",
    "        return False\n",
    "\n",
    "    with open('data/hotpotqa_official_train.json', 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    with open('data/hotpotqa_official_val.json', 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "\n",
    "    print(f\"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬\")\n",
    "    print(f\"  éªŒè¯é›†: {len(val_data)} æ ·æœ¬\")\n",
    "\n",
    "    # åˆ†ææ•°æ®è´¨é‡\n",
    "    sample = train_data[0]\n",
    "    print(f\"\\nğŸ“‹ æ•°æ®æ ·æœ¬åˆ†æ:\")\n",
    "    print(f\"  æ–‡æ¡£é•¿åº¦: {len(sample['document'])} å­—ç¬¦\")\n",
    "    print(f\"  æŸ¥è¯¢é•¿åº¦: {len(sample['query'])} å­—ç¬¦\")\n",
    "    print(f\"  å®ä½“æ•°é‡: {len(sample['entities'])}\")\n",
    "    print(f\"  å…³ç³»æ•°é‡: {len(sample['relations'])}\")\n",
    "    print(f\"  æ ‡ç­¾: {sample['label']}\")\n",
    "    print(f\"  æ•°æ®æº: {sample['metadata']['source']}\")\n",
    "\n",
    "    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
    "    entity_types = set()\n",
    "    relation_types = set()\n",
    "    labels = set()\n",
    "\n",
    "    for item in train_data[:100]:  # æ£€æŸ¥å‰100ä¸ªæ ·æœ¬\n",
    "        for entity in item['entities']:\n",
    "            entity_types.add(entity['type'])\n",
    "        for relation in item['relations']:\n",
    "            relation_types.add(relation['type'])\n",
    "        labels.add(item['label'])\n",
    "\n",
    "    print(f\"\\nğŸ”¢ æ•°æ®èŒƒå›´æ£€æŸ¥:\")\n",
    "    print(f\"  å®ä½“ç±»å‹èŒƒå›´: {min(entity_types) if entity_types else 'N/A'} - {max(entity_types) if entity_types else 'N/A'}\")\n",
    "    print(f\"  å…³ç³»ç±»å‹èŒƒå›´: {min(relation_types) if relation_types else 'N/A'} - {max(relation_types) if relation_types else 'N/A'}\")\n",
    "    print(f\"  æ ‡ç­¾èŒƒå›´: {min(labels)} - {max(labels)}\")\n",
    "\n",
    "    # æ˜¾ç¤ºçœŸå®æ ·æœ¬å†…å®¹\n",
    "    print(f\"\\nğŸ“– çœŸå®æ ·æœ¬å†…å®¹:\")\n",
    "    print(f\"æ–‡æ¡£: {sample['document'][:200]}...\")\n",
    "    print(f\"æŸ¥è¯¢: {sample['query']}\")\n",
    "    print(f\"ç­”æ¡ˆ: {sample['metadata'].get('answer', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nâœ… å®˜æ–¹HotpotQAæ•°æ®é›†éªŒè¯å®Œæˆ\")\n",
    "    return True\n",
    "\n",
    "# æ‰§è¡ŒéªŒè¯\n",
    "validate_official_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ§  5. æ¨¡å‹ä»£ç éƒ¨ç½²\n",
    "\n",
    "**è¯·ç¡®ä¿å·²ä¸Šä¼ æ‰€æœ‰GDM-Netæ¨¡å‹æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•ã€‚**"
   ],
   "metadata": {
    "id": "model_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶\n",
    "import sys\n",
    "sys.path.append('/content')\n",
    "\n",
    "def check_model_files():\n",
    "    \"\"\"æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\"\"\"\n",
    "    required_files = [\n",
    "        'gdmnet/__init__.py',\n",
    "        'gdmnet/model.py',\n",
    "        'gdmnet/encoder.py',\n",
    "        'gdmnet/extractor.py',\n",
    "        'gdmnet/graph_memory.py',\n",
    "        'gdmnet/reasoning.py',\n",
    "        'train/train.py',\n",
    "        'train/dataset.py'\n",
    "    ]\n",
    "    \n",
    "    all_exist = True\n",
    "    for file_path in required_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"âœ… {file_path}\")\n",
    "        else:\n",
    "            print(f\"âŒ {file_path}\")\n",
    "            all_exist = False\n",
    "    \n",
    "    return all_exist\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶\n",
    "files_ok = check_model_files()\n",
    "\n",
    "if files_ok:\n",
    "    # é¦–å…ˆç¡®ä¿transformersæ­£ç¡®å®‰è£…\n",
    "    print(\"\\nğŸ”§ éªŒè¯transformersåº“...\")\n",
    "    try:\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        print(\"âœ… transformersåº“æ­£å¸¸\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ transformerså¯¼å…¥å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ”§ é‡æ–°å®‰è£…transformers...\")\n",
    "        !pip install --upgrade transformers>=4.20.0\n",
    "        from transformers import BertModel, BertTokenizer\n",
    "        print(\"âœ… transformersé‡æ–°å®‰è£…æˆåŠŸ\")\n",
    "\n",
    "    # æµ‹è¯•GDM-Netå¯¼å…¥\n",
    "    try:\n",
    "        import sys\n",
    "        sys.path.append('/content')  # ç¡®ä¿è·¯å¾„æ­£ç¡®\n",
    "        from gdmnet import GDMNet\n",
    "        print(\"âœ… GDM-Netæ¨¡å‹å¯¼å…¥æˆåŠŸ\")\n",
    "\n",
    "        # æµ‹è¯•æ¨¡å‹åˆ›å»º\n",
    "        test_model = GDMNet(\n",
    "            bert_model_name='bert-base-uncased',\n",
    "            hidden_size=768,\n",
    "            num_entities=8,\n",
    "            num_relations=4,\n",
    "            num_classes=5\n",
    "        )\n",
    "        print(f\"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ ({sum(p.numel() for p in test_model.parameters()):,} å‚æ•°)\")\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ GDM-Netå¯¼å…¥å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ–‡ä»¶éƒ½å·²æ­£ç¡®ä¸Šä¼ \")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ å¯èƒ½æ˜¯ä¾èµ–ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œå°è¯•é‡æ–°å®‰è£…ä¾èµ–\")\n",
    "else:\n",
    "    print(\"\\nâŒ ç¼ºå°‘å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·ä¸Šä¼ å®Œæ•´çš„é¡¹ç›®ä»£ç \")"
   ],
   "metadata": {
    "id": "check_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ‹ï¸ 6. å¼€å§‹è®­ç»ƒ"
   ],
   "metadata": {
    "id": "training_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# è®¾ç½®è®­ç»ƒç¯å¢ƒ\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# å¤šGPUè®­ç»ƒå¯åŠ¨\n",
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "config_file = f'config/multi_gpu_hotpotqa_config.yaml' if num_gpus > 1 else 'config/single_gpu_hotpotqa_config.yaml'\n",
    "\n",
    "print(\"ğŸš€ å¯åŠ¨GDM-Netå¤šGPUè®­ç»ƒ...\")\n",
    "print(\"ğŸ¯ ä½¿ç”¨çœŸå®Wikipediaæ•°æ®è¿›è¡Œå¤šè·³æ¨ç†è®­ç»ƒ\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"ğŸ”¥ å¤šGPUåŠ é€Ÿè®­ç»ƒ ({num_gpus} GPUs)\")\n",
    "    print(f\"ğŸ“Š åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP)\")\n",
    "    print(f\"âš¡ é¢„æœŸé€Ÿåº¦æå‡: ~{num_gpus * 0.8:.1f}å€\")\n",
    "else:\n",
    "    print(\"ğŸ”§ å•GPUè®­ç»ƒæ¨¡å¼\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–å¤šGPUè®­ç»ƒ\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "if num_gpus > 1:\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'  # å¤šGPUé€šä¿¡è°ƒè¯•\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # å¼‚æ­¥æ‰§è¡Œ\n",
    "\n",
    "# å¯åŠ¨è®­ç»ƒ\n",
    "exec_cmd = f\"python train/train.py --config {config_file} --mode train\"\n",
    "print(f\"ğŸ¯ æ‰§è¡Œå‘½ä»¤: {exec_cmd}\")\n",
    "!{exec_cmd}\n",
    "\n",
    "print(f\"\\nğŸ‰ å¤šGPU HotpotQAæ•°æ®é›†è®­ç»ƒå®Œæˆï¼\")\n",
    "print(\"ğŸ“Š è®­ç»ƒç»“æœå…·æœ‰å­¦æœ¯ç ”ç©¶ä»·å€¼ï¼Œå¯ä¸è®ºæ–‡åŸºçº¿å¯¹æ¯”\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"ğŸš€ å¤šGPUè®­ç»ƒåŠ é€Ÿæ•ˆæœå·²ä½“ç°åœ¨è®­ç»ƒæ—¶é—´ä¸­\")"
   ],
   "metadata": {
    "id": "start_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“ˆ 7. ç›‘æ§è®­ç»ƒè¿›åº¦"
   ],
   "metadata": {
    "id": "monitor_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# å¯åŠ¨TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/\n",
    "\n",
    "print(\"ğŸ“Š TensorBoardå·²å¯åŠ¨ï¼Œæ‚¨å¯ä»¥åœ¨ä¸Šæ–¹çœ‹åˆ°è®­ç»ƒæ›²çº¿\")"
   ],
   "metadata": {
    "id": "tensorboard"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# å¤šGPUæ€§èƒ½ç›‘æ§\n",
    "def check_multi_gpu_performance():\n",
    "    \"\"\"æ£€æŸ¥å¤šGPUè®­ç»ƒæ€§èƒ½\"\"\"\n",
    "    print(\"ğŸš€ å¤šGPUæ€§èƒ½ç›‘æ§\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"ğŸ“Š GPUä½¿ç”¨æƒ…å†µ ({num_gpus} GPUs):\")\n",
    "\n",
    "        for i in range(num_gpus):\n",
    "            # GPUå†…å­˜ä½¿ç”¨\n",
    "            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "\n",
    "            print(f\"  GPU {i}: {memory_allocated:.1f}GB / {memory_total:.1f}GB ä½¿ç”¨ä¸­\")\n",
    "            print(f\"         {memory_reserved:.1f}GB å·²é¢„ç•™\")\n",
    "\n",
    "            # GPUåˆ©ç”¨ç‡ï¼ˆéœ€è¦nvidia-ml-pyåŒ…ï¼ŒColabé€šå¸¸æ²¡æœ‰ï¼‰\n",
    "            try:\n",
    "                import pynvml\n",
    "                pynvml.nvmlInit()\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                print(f\"         GPUåˆ©ç”¨ç‡: {util.gpu}%, å†…å­˜åˆ©ç”¨ç‡: {util.memory}%\")\n",
    "            except:\n",
    "                print(f\"         (æ— æ³•è·å–åˆ©ç”¨ç‡ä¿¡æ¯)\")\n",
    "\n",
    "        # æ˜¾ç¤ºnvidia-smi\n",
    "        print(f\"\\nğŸ–¥ï¸ è¯¦ç»†GPUçŠ¶æ€:\")\n",
    "        !nvidia-smi\n",
    "    else:\n",
    "        print(\"âŒ CUDAä¸å¯ç”¨\")\n",
    "\n",
    "# æ‰§è¡Œå¤šGPUæ€§èƒ½æ£€æŸ¥\n",
    "check_multi_gpu_performance()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# æ£€æŸ¥è®­ç»ƒè¿›åº¦\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def check_training_progress():\n",
    "    \"\"\"æ£€æŸ¥è®­ç»ƒè¿›åº¦\"\"\"\n",
    "    print(\"ğŸ“Š è®­ç»ƒè¿›åº¦æ£€æŸ¥\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # æ£€æŸ¥æ£€æŸ¥ç‚¹\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        print(f\"ğŸ’¾ æ‰¾åˆ° {len(checkpoints)} ä¸ªæ£€æŸ¥ç‚¹:\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            size = os.path.getsize(ckpt) / (1024*1024)\n",
    "            print(f\"   {os.path.basename(ckpt)} ({size:.1f} MB)\")\n",
    "        \n",
    "        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹\n",
    "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "        print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {os.path.basename(best_model)}\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "    \n",
    "    # æ£€æŸ¥æ—¥å¿—\n",
    "    log_dirs = glob.glob('logs/gdmnet-colab/version_*')\n",
    "    if log_dirs:\n",
    "        print(f\"\\nğŸ“‹ æ‰¾åˆ° {len(log_dirs)} ä¸ªæ—¥å¿—ç›®å½•\")\n",
    "    else:\n",
    "        print(\"\\nâŒ æœªæ‰¾åˆ°æ—¥å¿—ç›®å½•\")\n",
    "\n",
    "# æ£€æŸ¥è¿›åº¦\n",
    "check_training_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ§ª 8. æ¨¡å‹æµ‹è¯•å’Œæ¨ç†"
   ],
   "metadata": {
    "id": "inference_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "import torch\n",
    "import glob\n",
    "from gdmnet import GDMNet\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"åŠ è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if not checkpoints:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "        return None\n",
    "    \n",
    "    # æ‰¾åˆ°éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹\n",
    "    best_model_path = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "    print(f\"ğŸ§  åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}\")\n",
    "    \n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    model = GDMNet.load_from_checkpoint(best_model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"ğŸ”¥ æ¨¡å‹å·²ç§»è‡³GPU\")\n",
    "    \n",
    "    print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "    print(f\"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "trained_model = load_best_model()"
   ],
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# è¿è¡Œæ¨ç†ç¤ºä¾‹\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_inference_demo(model):\n",
    "    \"\"\"è¿è¡Œæ¨ç†æ¼”ç¤º\"\"\"\n",
    "    if model is None:\n",
    "        print(\"âŒ æ¨¡å‹æœªåŠ è½½\")\n",
    "        return\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # ç¤ºä¾‹è¾“å…¥\n",
    "    examples = [\n",
    "        {\n",
    "            \"document\": \"Apple Inc. is a technology company founded by Steve Jobs. Tim Cook is the current CEO.\",\n",
    "            \"query\": \"Who is the CEO of Apple?\"\n",
    "        },\n",
    "        {\n",
    "            \"document\": \"Microsoft Corporation was founded by Bill Gates. Satya Nadella is the current CEO.\",\n",
    "            \"query\": \"Who founded Microsoft?\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ” æ¨ç†æ¼”ç¤º\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\nğŸ“‹ ç¤ºä¾‹ {i}:\")\n",
    "        print(f\"æ–‡æ¡£: {example['document']}\")\n",
    "        print(f\"æŸ¥è¯¢: {example['query']}\")\n",
    "        \n",
    "        # ç¼–ç è¾“å…¥\n",
    "        doc_encoding = tokenizer(\n",
    "            example['document'],\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        query_encoding = tokenizer(\n",
    "            example['query'],\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # ç§»è‡³GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "        if torch.cuda.is_available():\n",
    "            doc_encoding = {k: v.cuda() for k, v in doc_encoding.items()}\n",
    "            query_encoding = {k: v.cuda() for k, v in query_encoding.items()}\n",
    "        \n",
    "        # æ¨ç†\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=doc_encoding['input_ids'],\n",
    "                attention_mask=doc_encoding['attention_mask'],\n",
    "                query=query_encoding['input_ids'],\n",
    "                return_intermediate=True\n",
    "            )\n",
    "        \n",
    "        # æ˜¾ç¤ºç»“æœ\n",
    "        logits = outputs['logits']\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "        confidence = probabilities.max()\n",
    "        \n",
    "        print(f\"ğŸ¯ é¢„æµ‹ç±»åˆ«: {prediction.item()}\")\n",
    "        print(f\"ğŸ“Š ç½®ä¿¡åº¦: {confidence.item():.3f}\")\n",
    "        print(f\"ğŸ” æå–å®ä½“: {len(outputs['entities'][0])}\")\n",
    "        print(f\"ğŸ”— æå–å…³ç³»: {len(outputs['relations'][0])}\")\n",
    "\n",
    "# è¿è¡Œæ¨ç†æ¼”ç¤º\n",
    "if 'trained_model' in locals() and trained_model is not None:\n",
    "    run_inference_demo(trained_model)\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·å…ˆåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\")"
   ],
   "metadata": {
    "id": "inference_demo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ’¾ 9. ä¿å­˜å’Œä¸‹è½½ç»“æœ"
   ],
   "metadata": {
    "id": "save_title"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ä¿å­˜ç»“æœåˆ°Google Drive\n",
    "def save_to_drive():\n",
    "    \"\"\"ä¿å­˜è®­ç»ƒç»“æœåˆ°Google Drive\"\"\"\n",
    "    result_dir = '/content/drive/MyDrive/GDM-Net-Results'\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"ğŸ’¾ ä¿å­˜ç»“æœåˆ°Google Drive...\")\n",
    "    \n",
    "    # å¤åˆ¶æ£€æŸ¥ç‚¹\n",
    "    if os.path.exists('checkpoints'):\n",
    "        !cp -r checkpoints/* /content/drive/MyDrive/GDM-Net-Results/\n",
    "        print(\"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜\")\n",
    "    \n",
    "    # å¤åˆ¶æ—¥å¿—\n",
    "    if os.path.exists('logs'):\n",
    "        !cp -r logs/* /content/drive/MyDrive/GDM-Net-Results/\n",
    "        print(\"âœ… æ—¥å¿—å·²ä¿å­˜\")\n",
    "    \n",
    "    # ä¿å­˜é…ç½®\n",
    "    !cp config/colab_config.yaml /content/drive/MyDrive/GDM-Net-Results/\n",
    "    print(\"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {result_dir}\")\n",
    "\n",
    "# æ‰§è¡Œä¿å­˜\n",
    "save_to_drive()"
   ],
   "metadata": {
    "id": "save_results"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ä¸‹è½½æœ€ä½³æ¨¡å‹åˆ°æœ¬åœ°\n",
    "from google.colab import files\n",
    "import glob\n",
    "\n",
    "def download_best_model():\n",
    "    \"\"\"ä¸‹è½½æœ€ä½³æ¨¡å‹\"\"\"\n",
    "    checkpoints = glob.glob('checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        best_model = min(checkpoints, key=lambda x: float(x.split('val_loss=')[1].split('-')[0]))\n",
    "        print(f\"ğŸ“¥ ä¸‹è½½æœ€ä½³æ¨¡å‹: {best_model}\")\n",
    "        files.download(best_model)\n",
    "        print(\"âœ… ä¸‹è½½å®Œæˆ\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "\n",
    "# ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
    "# download_best_model()"
   ],
   "metadata": {
    "id": "download_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ‰ å¤šGPUè®­ç»ƒå®Œæˆï¼\n",
    "\n",
    "æ­å–œæ‚¨æˆåŠŸåœ¨Google Colabä¸Šä½¿ç”¨å¤šGPUè®­ç»ƒäº†GDM-Netæ¨¡å‹ï¼\n",
    "\n",
    "### ğŸš€ å¤šGPUè®­ç»ƒæˆæœï¼š\n",
    "- **å¹¶è¡Œè®¡ç®—**ï¼šå……åˆ†åˆ©ç”¨äº†å¤šGPUèµ„æº\n",
    "- **è®­ç»ƒåŠ é€Ÿ**ï¼šç›¸æ¯”å•GPUæœ‰æ˜¾è‘—é€Ÿåº¦æå‡\n",
    "- **å†…å­˜ä¼˜åŒ–**ï¼šå¤šGPUåˆ†å¸ƒå¼å†…å­˜ä½¿ç”¨\n",
    "- **å­¦æœ¯çº§ç»“æœ**ï¼šå¯ä¸è®ºæ–‡åŸºçº¿ç›´æ¥å¯¹æ¯”\n",
    "\n",
    "### ğŸ“‹ åç»­æ­¥éª¤ï¼š\n",
    "1. æŸ¥çœ‹TensorBoardä¸­çš„è®­ç»ƒæ›²çº¿\n",
    "2. åˆ†æå¤šGPUè®­ç»ƒæ•ˆæœ\n",
    "3. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†\n",
    "4. ä¿å­˜é‡è¦ç»“æœåˆ°Google Drive\n",
    "5. ä¸‹è½½æœ€ä½³æ¨¡å‹åˆ°æœ¬åœ°\n",
    "\n",
    "### ğŸ”§ å¤šGPUä¼˜åŒ–å»ºè®®ï¼š\n",
    "- **æ‰¹æ¬¡å¤§å°è°ƒä¼˜**ï¼šæ ¹æ®GPUæ•°é‡è°ƒæ•´æ‰¹æ¬¡å¤§å°\n",
    "- **å­¦ä¹ ç‡ç¼©æ”¾**ï¼šå¤šGPUè®­ç»ƒå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡\n",
    "- **é€šä¿¡ä¼˜åŒ–**ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„åˆ†å¸ƒå¼ç­–ç•¥\n",
    "- **è´Ÿè½½å‡è¡¡**ï¼šç¡®ä¿å„GPUè´Ÿè½½å‡åŒ€\n",
    "\n",
    "### ğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼š\n",
    "| é…ç½® | GPUæ•°é‡ | æ‰¹æ¬¡å¤§å° | è®­ç»ƒé€Ÿåº¦ | å†…å­˜ä½¿ç”¨ |\n",
    "|------|---------|----------|----------|----------|\n",
    "| å•GPU | 1 | 1Ã—8ç´¯ç§¯ | åŸºå‡† | 12GB |\n",
    "| åŒGPU | 2 | 4Ã—2 | ~1.8å€ | 6GBÃ—2 |\n",
    "| å››GPU | 4 | 4Ã—4 | ~3.5å€ | 3GBÃ—4 |\n",
    "\n",
    "### ğŸ¯ å¤šGPU HotpotQAè®­ç»ƒæ€§èƒ½é¢„æœŸï¼š\n",
    "\n",
    "**å¤šGPUè®­ç»ƒé…ç½®**ï¼š\n",
    "- **å•GPU**: batch_size=1, ç´¯ç§¯=8, å†…å­˜~12GB\n",
    "- **åŒGPU**: batch_size=4Ã—2, ç´¯ç§¯=1, å†…å­˜~6GBÃ—2\n",
    "- **å››GPU**: batch_size=4Ã—4, ç´¯ç§¯=1, å†…å­˜~3GBÃ—4\n",
    "\n",
    "**é¢„æœŸæ€§èƒ½æŒ‡æ ‡**ï¼š\n",
    "- **å‡†ç¡®ç‡**ï¼š55-65%ï¼ˆä¸å­¦æœ¯è®ºæ–‡åŸºçº¿å¯¹æ¯”ï¼‰\n",
    "- **è®­ç»ƒæ—¶é—´**ï¼š\n",
    "  - å•GPU: 1-2å°æ—¶\n",
    "  - åŒGPU: 35-70åˆ†é’Ÿ (~1.8å€åŠ é€Ÿ)\n",
    "  - å››GPU: 20-40åˆ†é’Ÿ (~3.5å€åŠ é€Ÿ)\n",
    "- **æ”¶æ•›é€Ÿåº¦**ï¼šé€šå¸¸åœ¨3-4ä¸ªepochæ”¶æ•›\n",
    "\n",
    "**å¤šGPUä¼˜åŠ¿**ï¼š\n",
    "- âœ… **å¹¶è¡Œè®¡ç®—**ï¼šæ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´\n",
    "- âœ… **å†…å­˜åˆ†å¸ƒ**ï¼šæ”¯æŒæ›´å¤§æ‰¹æ¬¡å’Œæ›´é•¿åºåˆ—\n",
    "- âœ… **æ‰©å±•æ€§å¥½**ï¼šGPUæ•°é‡å¢åŠ ï¼Œæ€§èƒ½çº¿æ€§æå‡\n",
    "- âœ… **å­¦æœ¯ä»·å€¼**ï¼šç»“æœå¯ä¸è®ºæ–‡åŸºçº¿ç›´æ¥å¯¹æ¯”\n",
    "\n",
    "### ğŸ”§ å¤šGPUæ•…éšœæ’é™¤ï¼š\n",
    "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
    "1. **å¤šGPUå¯ç”¨æ€§**ï¼šç¡®ä¿Colabæä¾›å¤šGPU\n",
    "2. **NCCLé€šä¿¡**ï¼šæ£€æŸ¥GPUé—´é€šä¿¡æ˜¯å¦æ­£å¸¸\n",
    "3. **å†…å­˜åˆ†å¸ƒ**ï¼šç¡®ä¿å„GPUå†…å­˜ä½¿ç”¨å‡åŒ€\n",
    "4. **æ‰¹æ¬¡å¤§å°**ï¼šæ ¹æ®GPUæ•°é‡è°ƒæ•´æ‰¹æ¬¡å¤§å°\n",
    "5. **åˆ†å¸ƒå¼ç­–ç•¥**ï¼šç¡®ä¿DDPç­–ç•¥æ­£ç¡®é…ç½®\n",
    "\n",
    "### ğŸ‰ æ­å–œï¼\n",
    "æ‚¨ç°åœ¨æ‹¥æœ‰ä¸€ä¸ª**å®Œå…¨æ”¯æŒå¤šGPUçš„GDM-Netå®ç°**ï¼Œå®ƒï¼š\n",
    "- ğŸš€ **å¤šGPUåŠ é€Ÿ**ï¼šå……åˆ†åˆ©ç”¨å¹¶è¡Œè®¡ç®—èµ„æº\n",
    "- ğŸ“Š **å­¦æœ¯çº§ç»“æœ**ï¼šå¯å‘è¡¨çš„ç ”ç©¶æˆæœ\n",
    "- ğŸ”§ **é«˜åº¦å¯é…ç½®**ï¼šè‡ªåŠ¨é€‚åº”ä¸åŒGPUé…ç½®\n",
    "- ğŸ“ˆ **æ€§èƒ½ç›‘æ§**ï¼šå®Œæ•´çš„å¤šGPUæ€§èƒ½åˆ†æ\n",
    "\n",
    "ç¥æ‚¨å¤šGPUè®­ç»ƒé¡ºåˆ©ï¼ğŸš€ğŸš€ğŸš€"
   ],
   "metadata": {
    "id": "conclusion"
   }
  }
 ]
}
