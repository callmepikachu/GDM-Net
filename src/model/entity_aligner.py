"""
å®ä½“å¯¹é½å™¨ - è´Ÿè´£å°†æ–°æå–çš„å®ä½“ä¸å…¨å±€å›¾ä¸­çš„ç°æœ‰å®ä½“è¿›è¡Œå¯¹é½
"""

import numpy as np
from typing import Dict, Any, Optional, TYPE_CHECKING
from sklearn.metrics.pairwise import cosine_similarity

if TYPE_CHECKING:
    from .persistent_graph_memory import PersistentGraphMemory # ç”¨äºç±»å‹æç¤ºï¼Œé¿å…å¾ªç¯å¯¼å…¥


class EntityAligner:
    """
    ğŸš€ å¤šæ¨¡æ€å®ä½“å¯¹é½å™¨ - ç»“åˆåµŒå…¥ã€æ–‡æœ¬ã€ç±»å‹ç­‰å¤šç§ä¿¡æ¯è¿›è¡Œå®ä½“å¯¹é½
    """
    def __init__(self,
                 similarity_threshold: float = 0.85,
                 embedding_weight: float = 0.7,
                 text_weight: float = 0.2,
                 type_weight: float = 0.1):
        self.similarity_threshold = similarity_threshold

        # ğŸš€ å¤šæ¨¡æ€æƒé‡é…ç½®
        self.embedding_weight = embedding_weight
        self.text_weight = text_weight
        self.type_weight = type_weight

        # ç¡®ä¿æƒé‡å’Œä¸º1
        total_weight = embedding_weight + text_weight + type_weight
        self.embedding_weight /= total_weight
        self.text_weight /= total_weight
        self.type_weight /= total_weight

    def align(self, entity: Dict[str, Any], entity_embedding: np.ndarray, graph_memory: 'PersistentGraphMemory') -> Optional[str]:
        """
        å°è¯•å°†ä¸€ä¸ªæ–°å®ä½“ä¸å…¨å±€å›¾ä¸­çš„ç°æœ‰å®ä½“å¯¹é½ã€‚
        Args:
            entity: æ–°å®ä½“çš„å­—å…¸ä¿¡æ¯ã€‚
            entity_embedding: æ–°å®ä½“çš„åµŒå…¥å‘é‡ã€‚
            graph_memory: å…¨å±€å›¾è®°å¿†å®ä¾‹ã€‚
        Returns:
            str or None: å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œåˆ™è¿”å›å…¨å±€èŠ‚ç‚¹IDï¼›å¦åˆ™è¿”å›Noneã€‚
        """
        # ç­–ç•¥1ï¼šä½¿ç”¨ç¼“å­˜åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ä¸”ç›¸å…³ï¼‰
        if graph_memory._embedding_cache is not None and graph_memory._embedding_cache.size > 0:
            similarities = cosine_similarity(entity_embedding.reshape(1, -1), graph_memory._embedding_cache).flatten()
            max_sim_idx = np.argmax(similarities)
            max_sim = similarities[max_sim_idx]
            if max_sim > self.similarity_threshold:
                return graph_memory._cache_node_ids[max_sim_idx]

        # ç­–ç•¥2ï¼šğŸš€ å¤šæ¨¡æ€å…¨å±€æœç´¢
        if not graph_memory.nodes:
            return None

        # ä¼˜åŒ–ï¼šå…ˆæŒ‰ç±»å‹è¿‡æ»¤å€™é€‰èŠ‚ç‚¹
        entity_type = entity.get('type', 'UNKNOWN')
        candidate_nodes = {}

        # é¦–å…ˆå°è¯•åŒç±»å‹åŒ¹é…
        if entity_type in graph_memory.entity_type_index:
            for node_id in graph_memory.entity_type_index[entity_type]:
                if node_id in graph_memory.nodes:
                    candidate_nodes[node_id] = graph_memory.nodes[node_id]

        # å¦‚æœåŒç±»å‹å€™é€‰å¤ªå°‘ï¼Œæ‰©å±•åˆ°æ‰€æœ‰èŠ‚ç‚¹
        if len(candidate_nodes) < 10:
            candidate_nodes = graph_memory.nodes

        # ğŸš€ è®¡ç®—å¤šæ¨¡æ€ç›¸ä¼¼åº¦
        best_similarity = 0.0
        best_node_id = None

        for node_id, node_data in candidate_nodes.items():
            # æ„é€ å€™é€‰å®ä½“å­—å…¸
            candidate_entity = {
                'type': node_data.get('type', 'UNKNOWN'),
                'text': node_data.get('text', ''),
            }

            # è®¡ç®—å¤šæ¨¡æ€ç›¸ä¼¼åº¦
            similarity = self._compute_multimodal_similarity(
                entity, entity_embedding,
                candidate_entity, node_data['embedding']
            )

            if similarity > best_similarity:
                best_similarity = similarity
                best_node_id = node_id

        if best_similarity > self.similarity_threshold:
            return best_node_id

        return None # æœªæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„å®ä½“

    def _compute_multimodal_similarity(self, entity1: Dict[str, Any], embedding1: np.ndarray,
                                     entity2: Dict[str, Any], embedding2: np.ndarray) -> float:
        """
        ğŸš€ è®¡ç®—å¤šæ¨¡æ€ç›¸ä¼¼åº¦ï¼šç»“åˆåµŒå…¥ã€æ–‡æœ¬ã€ç±»å‹ç­‰ä¿¡æ¯
        Args:
            entity1, entity2: å®ä½“å­—å…¸
            embedding1, embedding2: å®ä½“åµŒå…¥å‘é‡
        Returns:
            float: ç»¼åˆç›¸ä¼¼åº¦åˆ†æ•°
        """
        # 1. åµŒå…¥ç›¸ä¼¼åº¦
        embedding_sim = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0, 0]

        # 2. ç±»å‹åŒ¹é…
        type1 = entity1.get('type', 'UNKNOWN')
        type2 = entity2.get('type', 'UNKNOWN')
        type_sim = 1.0 if type1 == type2 else 0.0

        # 3. æ–‡æœ¬ç›¸ä¼¼åº¦
        text1 = entity1.get('text', '').lower().strip()
        text2 = entity2.get('text', '').lower().strip()
        text_sim = self._compute_text_similarity(text1, text2)

        # 4. ç»¼åˆç›¸ä¼¼åº¦
        total_similarity = (self.embedding_weight * embedding_sim +
                          self.text_weight * text_sim +
                          self.type_weight * type_sim)

        return total_similarity

    def _compute_text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
        Args:
            text1, text2: æ–‡æœ¬å­—ç¬¦ä¸²
        Returns:
            float: æ–‡æœ¬ç›¸ä¼¼åº¦ [0, 1]
        """
        if not text1 or not text2:
            return 0.0

        # ç²¾ç¡®åŒ¹é…
        if text1 == text2:
            return 1.0

        # åŒ…å«å…³ç³»
        if text1 in text2 or text2 in text1:
            return 0.8

        # Jaccardç›¸ä¼¼åº¦ï¼ˆåŸºäºè¯æ±‡ï¼‰
        words1 = set(text1.split())
        words2 = set(text2.split())

        if not words1 or not words2:
            return 0.0

        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))

        jaccard_sim = intersection / union if union > 0 else 0.0

        # ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦ï¼ˆå¯¹äºçŸ­æ–‡æœ¬ï¼‰
        if len(text1) <= 20 and len(text2) <= 20:
            edit_distance = self._levenshtein_distance(text1, text2)
            max_len = max(len(text1), len(text2))
            edit_sim = 1.0 - (edit_distance / max_len) if max_len > 0 else 0.0

            # å–è¾ƒé«˜çš„ç›¸ä¼¼åº¦
            return max(jaccard_sim, edit_sim)

        return jaccard_sim

    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]
