"""
æŒä¹…åŒ–å›¾è®°å¿†ç³»ç»Ÿ - ç®¡ç†è·¨è¾“å…¥æ ·æœ¬çš„å…¨å±€å›¾çŠ¶æ€
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from typing import Dict, List, Tuple, Optional, Any
import uuid
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from .relation_type_manager import get_global_relation_manager

# ğŸš€ å°è¯•å¯¼å…¥Faissè¿›è¡Œé«˜æ•ˆç›¸ä¼¼åº¦æœç´¢
try:
    import faiss
    FAISS_AVAILABLE = True
    print("âœ… Faiss available for accelerated similarity search")
except ImportError:
    FAISS_AVAILABLE = False
    print("âš ï¸ Faiss not available, using sklearn for similarity search")


class PersistentGraphMemory:
    """
    ç®¡ç†è·¨è¾“å…¥æ ·æœ¬çš„æŒä¹…åŒ–å…¨å±€å›¾è®°å¿†ã€‚
    ä½¿ç”¨å†…å­˜å­˜å‚¨èŠ‚ç‚¹å’Œè¾¹ï¼Œå¹¶æä¾›æ›´æ–°å’ŒæŸ¥è¯¢æ¥å£ã€‚
    """
    def __init__(self, node_dim: int = 768, device: torch.device = torch.device('cpu')):
        self.node_dim = node_dim
        self.device = device

        # å­˜å‚¨å…¨å±€å›¾çŠ¶æ€
        # ä½¿ç”¨å­—å…¸å­˜å‚¨èŠ‚ç‚¹ï¼Œé”®ä¸ºå”¯ä¸€IDï¼Œå€¼ä¸ºèŠ‚ç‚¹ä¿¡æ¯
        self.nodes: Dict[str, Dict[str, Any]] = {}
        # å­˜å‚¨è¾¹ï¼Œ(src_id, rel_type, dst_id) ä½œä¸ºé”®ï¼Œå€¼ä¸ºè¾¹ä¿¡æ¯ï¼ˆå¦‚æƒé‡ã€æ—¶é—´æˆ³ï¼‰
        self.edges: Dict[Tuple[str, str, str], Dict[str, Any]] = {}

        # å®ä½“ç±»å‹åˆ°IDçš„æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
        self.entity_type_index: Dict[str, List[str]] = {}

        # ç”¨äºå®ä½“å¯¹é½çš„ç¼“å­˜ï¼ˆå¯é€‰ï¼Œå­˜å‚¨æœ€è¿‘è®¿é—®çš„èŠ‚ç‚¹åµŒå…¥ä»¥åŠ é€Ÿï¼‰
        self._embedding_cache: Optional[np.ndarray] = None
        self._cache_node_ids: List[str] = []

        # ğŸš€ Faissç´¢å¼•ç”¨äºé«˜æ•ˆç›¸ä¼¼åº¦æœç´¢
        self.faiss_index: Optional[Any] = None
        self.faiss_node_ids: List[str] = []
        self.faiss_needs_update: bool = False

        # ğŸš€ å…³ç³»ç±»å‹ç®¡ç†å™¨
        self.relation_manager = get_global_relation_manager()

    def add_or_update_nodes(self, local_entities: List[Dict], local_node_features: torch.Tensor, aligner: 'EntityAligner') -> Dict[int, str]:
        """
        å°†å±€éƒ¨å›¾ä¸­çš„èŠ‚ç‚¹æ·»åŠ åˆ°å…¨å±€å›¾æˆ–æ›´æ–°ç°æœ‰èŠ‚ç‚¹ã€‚
        Args:
            local_entities: ä»å½“å‰å—æå–çš„å®ä½“åˆ—è¡¨ã€‚
            local_node_features: å¯¹åº”çš„å±€éƒ¨èŠ‚ç‚¹ç‰¹å¾ [num_local_entities, node_dim]ã€‚
            aligner: å®ä½“å¯¹é½å™¨å®ä¾‹ã€‚
        Returns:
            local_to_global_map: ä¸€ä¸ªå­—å…¸ï¼Œå°†å±€éƒ¨èŠ‚ç‚¹ç´¢å¼•æ˜ å°„åˆ°å…¨å±€èŠ‚ç‚¹IDã€‚
        """
        local_to_global_map = {}
        updated_embeddings = []
        updated_node_ids = []

        for i, entity in enumerate(local_entities):
            local_feat = local_node_features[i].cpu().detach().numpy() # è½¬ä¸ºnumpyè¿›è¡Œè®¡ç®—

            # 1. å®ä½“å¯¹é½ï¼šæŸ¥æ‰¾å…¨å±€å›¾ä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥å®ä½“
            global_node_id = aligner.align(entity, local_feat, self)

            if global_node_id is None:
                # 2a. æœªæ‰¾åˆ°åŒ¹é…å®ä½“ï¼Œåˆ›å»ºæ–°èŠ‚ç‚¹
                global_node_id = str(uuid.uuid4()) # ç”Ÿæˆå”¯ä¸€ID
                # å¯ä»¥è€ƒè™‘åŠ å…¥æ›´å¤šå®ä½“ä¿¡æ¯ï¼Œå¦‚æ–‡æœ¬ã€ç±»å‹ç­‰
                self.nodes[global_node_id] = {
                    'embedding': local_feat,
                    'type': entity.get('type', 'UNKNOWN'),
                    'text': entity.get('text', ''), # éœ€è¦ä»åŸå§‹æ•°æ®ä¸­è·å–æˆ–å­˜å‚¨
                    'count': 1 # è®°å½•å‡ºç°æ¬¡æ•°
                }
                # æ›´æ–°ç±»å‹ç´¢å¼•
                ent_type = entity.get('type', 'UNKNOWN')
                if ent_type not in self.entity_type_index:
                    self.entity_type_index[ent_type] = []
                self.entity_type_index[ent_type].append(global_node_id)
            else:
                # 2b. æ‰¾åˆ°åŒ¹é…å®ä½“ï¼Œæ›´æ–°å…¶åµŒå…¥ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ç§»åŠ¨å¹³å‡ï¼‰
                existing_node = self.nodes[global_node_id]
                alpha = 0.1 # æ›´æ–°ç‡
                updated_embedding = (1 - alpha) * existing_node['embedding'] + alpha * local_feat
                self.nodes[global_node_id]['embedding'] = updated_embedding
                self.nodes[global_node_id]['count'] += 1

            local_to_global_map[i] = global_node_id
            updated_embeddings.append(self.nodes[global_node_id]['embedding'])
            updated_node_ids.append(global_node_id)

        # æ›´æ–°ç¼“å­˜
        if updated_embeddings:
            self._embedding_cache = np.array(updated_embeddings)
            self._cache_node_ids = updated_node_ids
            # æ ‡è®°Faissç´¢å¼•éœ€è¦æ›´æ–°
            self.faiss_needs_update = True

        return local_to_global_map

    def add_or_update_edges(self, local_relations: List[Dict], local_to_global_map: Dict[int, str]):
        """
        å°†å±€éƒ¨å›¾ä¸­çš„è¾¹æ·»åŠ åˆ°å…¨å±€å›¾æˆ–æ›´æ–°ç°æœ‰è¾¹ã€‚
        Args:
            local_relations: ä»å½“å‰å—æå–çš„å…³ç³»åˆ—è¡¨ã€‚
            local_to_global_map: å±€éƒ¨èŠ‚ç‚¹ç´¢å¼•åˆ°å…¨å±€èŠ‚ç‚¹IDçš„æ˜ å°„ã€‚
        """
        for relation in local_relations:
            local_head_idx = relation['head']
            local_tail_idx = relation['tail']

            # æ£€æŸ¥å¤´å°¾å®ä½“æ˜¯å¦éƒ½åœ¨å…¨å±€å›¾ä¸­ï¼ˆç†è®ºä¸Šåº”è¯¥éƒ½åœ¨ï¼‰
            if local_head_idx in local_to_global_map and local_tail_idx in local_to_global_map:
                global_head_id = local_to_global_map[local_head_idx]
                global_tail_id = local_to_global_map[local_tail_idx]

                # ğŸš€ ä½¿ç”¨å…³ç³»ç±»å‹ç®¡ç†å™¨å¤„ç†å…³ç³»ç±»å‹
                rel_type_str = str(relation['type'])
                rel_type_id = self.relation_manager.get_id(rel_type_str)

                edge_key = (global_head_id, rel_type_id, global_tail_id)

                if edge_key in self.edges:
                    # æ›´æ–°ç°æœ‰è¾¹ï¼ˆä¾‹å¦‚ï¼Œå¢åŠ è®¡æ•°ï¼‰
                    self.edges[edge_key]['count'] += 1
                else:
                    # æ·»åŠ æ–°è¾¹
                    self.edges[edge_key] = {
                        'count': 1,
                        # å¯ä»¥æ·»åŠ å…¶ä»–è¾¹å±æ€§ï¼Œå¦‚ç½®ä¿¡åº¦ã€æ—¶é—´æˆ³ç­‰
                    }

    def _update_faiss_index(self):
        """ğŸš€ æ›´æ–°æˆ–æ„å»ºFaissç´¢å¼•ä»¥åŠ é€Ÿç›¸ä¼¼åº¦æœç´¢"""
        if not FAISS_AVAILABLE or not self.nodes:
            return

        try:
            # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹åµŒå…¥
            embeddings = np.array([node['embedding'] for node in self.nodes.values()]).astype('float32')
            self.faiss_node_ids = list(self.nodes.keys())

            # æ„å»ºFaissç´¢å¼•
            dimension = embeddings.shape[1]

            # é€‰æ‹©ç´¢å¼•ç±»å‹ï¼šå¯¹äºä¸­å°è§„æ¨¡ä½¿ç”¨FlatIPï¼Œå¤§è§„æ¨¡ä½¿ç”¨IVF
            if len(self.nodes) < 10000:
                self.faiss_index = faiss.IndexFlatIP(dimension)  # ç²¾ç¡®æœç´¢
            else:
                # å¤§è§„æ¨¡æ•°æ®ä½¿ç”¨è¿‘ä¼¼æœç´¢
                nlist = min(100, len(self.nodes) // 100)  # èšç±»æ•°é‡
                quantizer = faiss.IndexFlatIP(dimension)
                self.faiss_index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
                # è®­ç»ƒç´¢å¼•ï¼ˆIVFéœ€è¦è®­ç»ƒï¼‰
                self.faiss_index.train(embeddings)

            # L2å½’ä¸€åŒ–ä»¥æ”¯æŒä½™å¼¦ç›¸ä¼¼åº¦
            faiss.normalize_L2(embeddings)

            # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
            self.faiss_index.add(embeddings)

            self.faiss_needs_update = False
            print(f"âœ… Updated Faiss index with {len(self.nodes)} nodes")

        except Exception as e:
            print(f"âš ï¸ Failed to update Faiss index: {e}")
            self.faiss_index = None

    def get_subgraph_for_query(self, query_embedding: np.ndarray, top_k: int = 50) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[str]]:
        """
        ğŸš€ æ ¹æ®æŸ¥è¯¢åµŒå…¥æ£€ç´¢æœ€ç›¸å…³çš„èŠ‚ç‚¹å­é›†åŠå…¶è¿æ¥çš„è¾¹ï¼Œæ„å»ºä¸€ä¸ªå­å›¾ç”¨äºæ¨ç†ã€‚
        ä½¿ç”¨FaissåŠ é€Ÿç›¸ä¼¼åº¦æœç´¢ã€‚
        Args:
            query_embedding: æŸ¥è¯¢çš„åµŒå…¥å‘é‡ [node_dim]ã€‚
            top_k: è¿”å›æœ€ç›¸å…³çš„ top_k ä¸ªèŠ‚ç‚¹ã€‚
        Returns:
            node_features: å­å›¾èŠ‚ç‚¹ç‰¹å¾ [num_nodes, node_dim] (åœ¨deviceä¸Š)ã€‚
            edge_index: å­å›¾è¾¹ç´¢å¼• [2, num_edges] (åœ¨deviceä¸Š)ã€‚
            edge_type: å­å›¾è¾¹ç±»å‹ [num_edges] (åœ¨deviceä¸Š)ã€‚
            node_ids: å­å›¾ä¸­èŠ‚ç‚¹çš„å…¨å±€IDåˆ—è¡¨ã€‚
        """
        if not self.nodes:
            # å¦‚æœå…¨å±€å›¾ä¸ºç©ºï¼Œè¿”å›ç©ºå›¾
            return (torch.empty(0, self.node_dim, device=self.device),
                    torch.empty(2, 0, dtype=torch.long, device=self.device),
                    torch.empty(0, dtype=torch.long, device=self.device),
                    [])

        # ğŸš€ ä½¿ç”¨FaissåŠ é€Ÿæœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if FAISS_AVAILABLE and self.faiss_index is not None:
            if self.faiss_needs_update:
                self._update_faiss_index()

            try:
                # å‡†å¤‡æŸ¥è¯¢å‘é‡
                query_vec = query_embedding.astype('float32').reshape(1, -1)
                faiss.normalize_L2(query_vec)  # L2å½’ä¸€åŒ–ä»¥æ”¯æŒä½™å¼¦ç›¸ä¼¼åº¦

                # Faissæœç´¢
                k = min(top_k, len(self.faiss_node_ids))
                similarities, indices = self.faiss_index.search(query_vec, k)

                # è·å–ç»“æœ
                top_k_indices = indices[0]
                subgraph_node_ids = [self.faiss_node_ids[i] for i in top_k_indices]
                subgraph_node_features = torch.tensor(
                    np.array([self.nodes[node_id]['embedding'] for node_id in subgraph_node_ids]),
                    dtype=torch.float, device=self.device
                )

            except Exception as e:
                print(f"âš ï¸ Faiss search failed, falling back to sklearn: {e}")
                # å›é€€åˆ°sklearnæ–¹æ³•
                return self._fallback_similarity_search(query_embedding, top_k)
        else:
            # å›é€€åˆ°sklearnæ–¹æ³•
            return self._fallback_similarity_search(query_embedding, top_k)

        # æ„å»ºå­å›¾è¾¹ï¼ˆåªåŒ…å«å­å›¾èŠ‚ç‚¹ä¹‹é—´çš„è¾¹ï¼‰
        subgraph_edges = []
        subgraph_edge_types = []
        node_id_to_index = {nid: idx for idx, nid in enumerate(subgraph_node_ids)}

        for (src, rel_id, dst), edge_data in self.edges.items():
            if src in node_id_to_index and dst in node_id_to_index:
                subgraph_edges.append([node_id_to_index[src], node_id_to_index[dst]])
                # ğŸš€ ç›´æ¥ä½¿ç”¨å…³ç³»ç±»å‹IDï¼ˆå·²ç»æ˜¯æ•´æ•°ï¼‰
                subgraph_edge_types.append(rel_id)

        if subgraph_edges:
            subgraph_edge_index = torch.tensor(subgraph_edges, dtype=torch.long, device=self.device).t().contiguous()
            subgraph_edge_type = torch.tensor(subgraph_edge_types, dtype=torch.long, device=self.device)
        else:
            subgraph_edge_index = torch.empty(2, 0, dtype=torch.long, device=self.device)
            subgraph_edge_type = torch.empty(0, dtype=torch.long, device=self.device)

        return subgraph_node_features, subgraph_edge_index, subgraph_edge_type, subgraph_node_ids

    def _fallback_similarity_search(self, query_embedding: np.ndarray, top_k: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[str]]:
        """å›é€€åˆ°sklearnçš„ç›¸ä¼¼åº¦æœç´¢æ–¹æ³•"""
        all_node_embeddings = np.array([node['embedding'] for node in self.nodes.values()])
        all_node_ids = list(self.nodes.keys())

        # è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_embedding.reshape(1, -1), all_node_embeddings).flatten()
        top_k_indices = np.argpartition(similarities, -top_k)[-top_k:]
        top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]  # æŒ‰ç›¸ä¼¼åº¦æ’åº

        # æ„å»ºå­å›¾èŠ‚ç‚¹
        subgraph_node_ids = [all_node_ids[i] for i in top_k_indices]
        subgraph_node_features = torch.tensor(all_node_embeddings[top_k_indices], dtype=torch.float, device=self.device)

        # æ„å»ºå­å›¾è¾¹ï¼ˆåªåŒ…å«å­å›¾èŠ‚ç‚¹ä¹‹é—´çš„è¾¹ï¼‰
        subgraph_edges = []
        subgraph_edge_types = []
        node_id_to_index = {nid: idx for idx, nid in enumerate(subgraph_node_ids)}

        for (src, rel, dst), edge_data in self.edges.items():
            if src in node_id_to_index and dst in node_id_to_index:
                subgraph_edges.append([node_id_to_index[src], node_id_to_index[dst]])
                # è¿™é‡Œéœ€è¦ä¸€ä¸ªä»å­—ç¬¦ä¸²rel_typeåˆ°æ•´æ•°IDçš„æ˜ å°„ï¼Œå‡è®¾åœ¨æ¨¡å‹ä¸­å·²å®šä¹‰
                # ä¸ºç®€åŒ–ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨hashæˆ–é¢„å®šä¹‰æ˜ å°„
                rel_id = hash(rel) % 1000000 # ç¤ºä¾‹ï¼Œå®é™…åº”ä½¿ç”¨é¢„å®šä¹‰æ˜ å°„
                subgraph_edge_types.append(rel_id)

        if subgraph_edges:
            subgraph_edge_index = torch.tensor(subgraph_edges, dtype=torch.long, device=self.device).t().contiguous()
            subgraph_edge_type = torch.tensor(subgraph_edge_types, dtype=torch.long, device=self.device)
        else:
            subgraph_edge_index = torch.empty(2, 0, dtype=torch.long, device=self.device)
            subgraph_edge_type = torch.empty(0, dtype=torch.long, device=self.device)

        return subgraph_node_features, subgraph_edge_index, subgraph_edge_type, subgraph_node_ids

    def get_all_nodes(self) -> Tuple[torch.Tensor, List[str]]:
        """è·å–æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾å’ŒIDï¼Œç”¨äºè°ƒè¯•æˆ–å®Œæ•´å›¾åˆ†æã€‚"""
        if not self.nodes:
             return (torch.empty(0, self.node_dim, device=self.device), [])
        all_node_embeddings = torch.tensor(np.array([node['embedding'] for node in self.nodes.values()]), dtype=torch.float, device=self.device)
        all_node_ids = list(self.nodes.keys())
        return all_node_embeddings, all_node_ids

    def save_to_disk(self, filepath: str):
        """å°†å›¾çŠ¶æ€ä¿å­˜åˆ°ç£ç›˜ (ç¤ºä¾‹ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„åºåˆ—åŒ–)"""
        import pickle
        state = {
            'nodes': self.nodes,
            'edges': self.edges,
            'entity_type_index': self.entity_type_index
        }
        with open(filepath, 'wb') as f:
            pickle.dump(state, f)

    def load_from_disk(self, filepath: str):
        """ä»ç£ç›˜åŠ è½½å›¾çŠ¶æ€ (ç¤ºä¾‹)"""
        import pickle
        with open(filepath, 'rb') as f:
            state = pickle.load(f)
        self.nodes = state['nodes']
        self.edges = state['edges']
        self.entity_type_index = state['entity_type_index']
        # æ¸…é™¤ç¼“å­˜
        self._embedding_cache = None
        self._cache_node_ids = []
