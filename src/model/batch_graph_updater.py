"""
æ‰¹å¤„ç†å›¾è®°å¿†æ›´æ–°å™¨ - ä¼˜åŒ–æ‰¹é‡æ›´æ–°å›¾è®°å¿†çš„æ€§èƒ½
"""

import torch
import numpy as np
from typing import List, Dict, Any, Tuple
from .persistent_graph_memory import PersistentGraphMemory
from .entity_aligner import EntityAligner


class BatchGraphUpdater:
    """
    æ‰¹å¤„ç†å›¾è®°å¿†æ›´æ–°å™¨ï¼Œä¼˜åŒ–å¤šæ ·æœ¬åŒæ—¶æ›´æ–°å›¾è®°å¿†çš„æ€§èƒ½
    """
    
    def __init__(self, graph_memory: PersistentGraphMemory, entity_aligner: EntityAligner):
        self.graph_memory = graph_memory
        self.entity_aligner = entity_aligner
        
        # æ‰¹å¤„ç†ç¼“å­˜
        self.batch_entities = []
        self.batch_relations = []
        self.batch_node_features = []
        
    def add_batch_sample(self, entities: List[Dict], relations: List[Dict], node_features: torch.Tensor):
        """
        æ·»åŠ ä¸€ä¸ªæ ·æœ¬åˆ°æ‰¹å¤„ç†ç¼“å­˜
        Args:
            entities: å®ä½“åˆ—è¡¨
            relations: å…³ç³»åˆ—è¡¨  
            node_features: èŠ‚ç‚¹ç‰¹å¾ [num_entities, hidden_size]
        """
        self.batch_entities.append(entities)
        self.batch_relations.append(relations)
        self.batch_node_features.append(node_features.cpu().detach().numpy())
    
    def flush_batch(self) -> Dict[str, int]:
        """
        æ‰¹é‡å¤„ç†æ‰€æœ‰ç¼“å­˜çš„æ ·æœ¬ï¼Œæ›´æ–°å›¾è®°å¿†
        Returns:
            Dict: æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.batch_entities:
            return {'nodes_added': 0, 'nodes_updated': 0, 'edges_added': 0, 'edges_updated': 0}
        
        stats = {'nodes_added': 0, 'nodes_updated': 0, 'edges_added': 0, 'edges_updated': 0}
        
        # ğŸš€ æ‰¹é‡å¤„ç†å®ä½“å¯¹é½å’Œæ›´æ–°
        all_entities = []
        all_node_features = []
        sample_entity_counts = []
        
        # æ”¶é›†æ‰€æœ‰å®ä½“å’Œç‰¹å¾
        for entities, node_features in zip(self.batch_entities, self.batch_node_features):
            all_entities.extend(entities)
            all_node_features.extend(node_features)
            sample_entity_counts.append(len(entities))
        
        if all_entities:
            # ğŸ”§ ç¡®ä¿å®ä½“å’Œç‰¹å¾æ•°é‡åŒ¹é…
            if len(all_entities) != len(all_node_features):
                print(f"âš ï¸ Entity-feature mismatch: {len(all_entities)} entities vs {len(all_node_features)} features")
                # å–è¾ƒå°çš„æ•°é‡ä»¥é¿å…ç´¢å¼•é”™è¯¯
                min_count = min(len(all_entities), len(all_node_features))
                all_entities = all_entities[:min_count]
                all_node_features = all_node_features[:min_count]

            # æ‰¹é‡å®ä½“å¯¹é½
            all_node_features_np = np.array(all_node_features)
            global_node_ids = self._batch_entity_alignment(all_entities, all_node_features_np, stats)
            
            # é‡æ–°åˆ†ç»„ä¸ºæ¯ä¸ªæ ·æœ¬çš„æ˜ å°„
            sample_mappings = []
            start_idx = 0
            for count in sample_entity_counts:
                end_idx = start_idx + count
                # ğŸ”§ ä¿®å¤ç´¢å¼•è¶Šç•Œï¼šç¡®ä¿ä¸è¶…å‡ºglobal_node_idsçš„èŒƒå›´
                if end_idx <= len(global_node_ids):
                    sample_mapping = {i: global_node_ids[start_idx + i] for i in range(count)}
                else:
                    # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œåªæ˜ å°„å¯ç”¨çš„éƒ¨åˆ†
                    available_count = len(global_node_ids) - start_idx
                    sample_mapping = {i: global_node_ids[start_idx + i] for i in range(min(count, available_count))}
                    print(f"âš ï¸ Index mismatch: expected {count} entities, got {available_count}")

                sample_mappings.append(sample_mapping)
                start_idx = end_idx
            
            # ğŸš€ æ‰¹é‡å¤„ç†å…³ç³»
            for relations, mapping in zip(self.batch_relations, sample_mappings):
                self._batch_relation_update(relations, mapping, stats)
        
        # æ¸…ç©ºç¼“å­˜
        self.clear_batch()
        
        # æ ‡è®°Faissç´¢å¼•éœ€è¦æ›´æ–°
        self.graph_memory.faiss_needs_update = True
        
        return stats
    
    def _batch_entity_alignment(self, entities: List[Dict], node_features: np.ndarray, stats: Dict) -> List[str]:
        """
        æ‰¹é‡å®ä½“å¯¹é½
        Args:
            entities: æ‰€æœ‰å®ä½“åˆ—è¡¨
            node_features: æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾ [total_entities, hidden_size]
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        Returns:
            List[str]: å¯¹åº”çš„å…¨å±€èŠ‚ç‚¹IDåˆ—è¡¨
        """
        global_node_ids = []
        
        # ğŸš€ å¦‚æœå›¾è®°å¿†ä¸ºç©ºï¼Œç›´æ¥æ‰¹é‡æ·»åŠ 
        if not self.graph_memory.nodes:
            for i, (entity, features) in enumerate(zip(entities, node_features)):
                global_node_id = self._create_new_node(entity, features)
                global_node_ids.append(global_node_id)
                stats['nodes_added'] += 1
            return global_node_ids
        
        # ğŸš€ æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—
        existing_embeddings = np.array([node['embedding'] for node in self.graph_memory.nodes.values()])
        existing_node_ids = list(self.graph_memory.nodes.keys())
        
        if len(existing_embeddings) > 0:
            # è®¡ç®—æ‰€æœ‰æ–°å®ä½“ä¸æ‰€æœ‰ç°æœ‰å®ä½“çš„ç›¸ä¼¼åº¦çŸ©é˜µ
            from sklearn.metrics.pairwise import cosine_similarity
            similarity_matrix = cosine_similarity(node_features, existing_embeddings)
            
            # å¯¹æ¯ä¸ªæ–°å®ä½“æ‰¾æœ€ç›¸ä¼¼çš„ç°æœ‰å®ä½“
            for i, (entity, features) in enumerate(zip(entities, node_features)):
                similarities = similarity_matrix[i]
                max_sim_idx = np.argmax(similarities)
                max_sim = similarities[max_sim_idx]
                
                if max_sim > self.entity_aligner.similarity_threshold:
                    # æ‰¾åˆ°åŒ¹é…å®ä½“ï¼Œæ›´æ–°
                    global_node_id = existing_node_ids[max_sim_idx]
                    self._update_existing_node(global_node_id, features)
                    global_node_ids.append(global_node_id)
                    stats['nodes_updated'] += 1
                else:
                    # åˆ›å»ºæ–°å®ä½“
                    global_node_id = self._create_new_node(entity, features)
                    global_node_ids.append(global_node_id)
                    stats['nodes_added'] += 1
        else:
            # æ²¡æœ‰ç°æœ‰å®ä½“ï¼Œå…¨éƒ¨åˆ›å»ºæ–°çš„
            for entity, features in zip(entities, node_features):
                global_node_id = self._create_new_node(entity, features)
                global_node_ids.append(global_node_id)
                stats['nodes_added'] += 1
        
        return global_node_ids
    
    def _create_new_node(self, entity: Dict, features: np.ndarray) -> str:
        """åˆ›å»ºæ–°èŠ‚ç‚¹"""
        import uuid
        global_node_id = str(uuid.uuid4())
        
        self.graph_memory.nodes[global_node_id] = {
            'embedding': features,
            'type': entity.get('type', 'UNKNOWN'),
            'text': entity.get('text', ''),
            'count': 1
        }
        
        # æ›´æ–°ç±»å‹ç´¢å¼•
        ent_type = entity.get('type', 'UNKNOWN')
        if ent_type not in self.graph_memory.entity_type_index:
            self.graph_memory.entity_type_index[ent_type] = []
        self.graph_memory.entity_type_index[ent_type].append(global_node_id)
        
        return global_node_id
    
    def _update_existing_node(self, global_node_id: str, features: np.ndarray):
        """æ›´æ–°ç°æœ‰èŠ‚ç‚¹"""
        existing_node = self.graph_memory.nodes[global_node_id]
        alpha = 0.1  # æ›´æ–°ç‡
        updated_embedding = (1 - alpha) * existing_node['embedding'] + alpha * features
        self.graph_memory.nodes[global_node_id]['embedding'] = updated_embedding
        self.graph_memory.nodes[global_node_id]['count'] += 1
    
    def _batch_relation_update(self, relations: List[Dict], local_to_global_map: Dict[int, str], stats: Dict):
        """æ‰¹é‡æ›´æ–°å…³ç³»"""
        for relation in relations:
            local_head_idx = relation['head']
            local_tail_idx = relation['tail']
            
            if local_head_idx in local_to_global_map and local_tail_idx in local_to_global_map:
                global_head_id = local_to_global_map[local_head_idx]
                global_tail_id = local_to_global_map[local_tail_idx]
                
                # ä½¿ç”¨å…³ç³»ç±»å‹ç®¡ç†å™¨
                rel_type_str = str(relation['type'])
                rel_type_id = self.graph_memory.relation_manager.get_id(rel_type_str)
                
                edge_key = (global_head_id, rel_type_id, global_tail_id)
                
                if edge_key in self.graph_memory.edges:
                    # æ›´æ–°ç°æœ‰è¾¹
                    self.graph_memory.edges[edge_key]['count'] += 1
                    stats['edges_updated'] += 1
                else:
                    # æ·»åŠ æ–°è¾¹
                    self.graph_memory.edges[edge_key] = {'count': 1}
                    stats['edges_added'] += 1
    
    def clear_batch(self):
        """æ¸…ç©ºæ‰¹å¤„ç†ç¼“å­˜"""
        self.batch_entities.clear()
        self.batch_relations.clear()
        self.batch_node_features.clear()
    
    def get_batch_size(self) -> int:
        """è·å–å½“å‰æ‰¹å¤„ç†å¤§å°"""
        return len(self.batch_entities)
