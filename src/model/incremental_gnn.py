"""
å¢é‡å­¦ä¹ å›¾ç¥ç»ç½‘ç»œ - æ”¯æŒåŠ¨æ€å›¾æ›´æ–°çš„GNN
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, MessagePassing
from typing import Dict, List, Tuple, Optional
import copy


class IncrementalGCNConv(MessagePassing):
    """
    ğŸš€ å¢é‡å­¦ä¹ çš„GCNå±‚ - æ”¯æŒåŠ¨æ€æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
    """
    
    def __init__(self, in_channels: int, out_channels: int, bias: bool = True):
        super().__init__(aggr='add')
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # å¯å­¦ä¹ å‚æ•°
        self.weight = nn.Parameter(torch.Tensor(in_channels, out_channels))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)
        
        # ğŸš€ å¢é‡å­¦ä¹ ç›¸å…³
        self.node_embeddings_cache = {}  # ç¼“å­˜èŠ‚ç‚¹åµŒå…¥
        self.edge_cache = set()  # ç¼“å­˜è¾¹ä¿¡æ¯
        
        self.reset_parameters()
    
    def reset_parameters(self):
        """åˆå§‹åŒ–å‚æ•°"""
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.zeros_(self.bias)
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                node_ids: Optional[List[str]] = None,
                incremental: bool = False) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, in_channels]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            node_ids: èŠ‚ç‚¹IDåˆ—è¡¨ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ¨¡å¼
        Returns:
            æ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾ [num_nodes, out_channels]
        """
        if incremental and node_ids is not None:
            return self._incremental_forward(x, edge_index, node_ids)
        else:
            return self._full_forward(x, edge_index)
    
    def _full_forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        """å®Œæ•´å‰å‘ä¼ æ’­"""
        # çº¿æ€§å˜æ¢
        x = torch.matmul(x, self.weight)
        
        # æ¶ˆæ¯ä¼ é€’
        out = self.propagate(edge_index, x=x)
        
        if self.bias is not None:
            out += self.bias
        
        return out
    
    def _incremental_forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                           node_ids: List[str]) -> torch.Tensor:
        """
        ğŸš€ å¢é‡å‰å‘ä¼ æ’­ - åªæ›´æ–°å—å½±å“çš„èŠ‚ç‚¹
        """
        # è¯†åˆ«æ–°èŠ‚ç‚¹å’Œå—å½±å“çš„èŠ‚ç‚¹
        new_nodes = set()
        affected_nodes = set()
        
        for i, node_id in enumerate(node_ids):
            if node_id not in self.node_embeddings_cache:
                new_nodes.add(i)
                affected_nodes.add(i)
        
        # è¯†åˆ«æ–°è¾¹å’Œå—å½±å“çš„èŠ‚ç‚¹
        current_edges = set()
        for j in range(edge_index.size(1)):
            src, dst = edge_index[0, j].item(), edge_index[1, j].item()
            edge = (src, dst)
            current_edges.add(edge)
            
            if edge not in self.edge_cache:
                # æ–°è¾¹ï¼Œå½±å“ç›¸å…³èŠ‚ç‚¹
                affected_nodes.add(src)
                affected_nodes.add(dst)
        
        # æ›´æ–°è¾¹ç¼“å­˜
        self.edge_cache = current_edges
        
        if not affected_nodes:
            # æ²¡æœ‰å—å½±å“çš„èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›ç¼“å­˜ç»“æœ
            cached_embeddings = []
            for node_id in node_ids:
                if node_id in self.node_embeddings_cache:
                    cached_embeddings.append(self.node_embeddings_cache[node_id])
                else:
                    # æ–°èŠ‚ç‚¹ï¼Œéœ€è¦è®¡ç®—
                    cached_embeddings.append(torch.zeros(self.out_channels, device=x.device))
            return torch.stack(cached_embeddings)
        
        # åªå¯¹å—å½±å“çš„èŠ‚ç‚¹è¿›è¡Œå®Œæ•´è®¡ç®—
        affected_indices = list(affected_nodes)
        if affected_indices:
            # æå–å—å½±å“èŠ‚ç‚¹çš„å­å›¾
            subgraph_x = x[affected_indices]
            
            # æ„å»ºå­å›¾è¾¹ç´¢å¼•
            index_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(affected_indices)}
            subgraph_edges = []
            
            for j in range(edge_index.size(1)):
                src, dst = edge_index[0, j].item(), edge_index[1, j].item()
                if src in index_mapping and dst in index_mapping:
                    subgraph_edges.append([index_mapping[src], index_mapping[dst]])
            
            if subgraph_edges:
                subgraph_edge_index = torch.tensor(subgraph_edges, dtype=torch.long, device=x.device).t()
                # å¯¹å­å›¾è¿›è¡ŒGCNè®¡ç®—
                subgraph_out = self._full_forward(subgraph_x, subgraph_edge_index)
                
                # æ›´æ–°ç¼“å­˜
                for i, old_idx in enumerate(affected_indices):
                    if old_idx < len(node_ids):
                        self.node_embeddings_cache[node_ids[old_idx]] = subgraph_out[i]
        
        # æ„å»ºå®Œæ•´è¾“å‡º
        output_embeddings = []
        for i, node_id in enumerate(node_ids):
            if node_id in self.node_embeddings_cache:
                output_embeddings.append(self.node_embeddings_cache[node_id])
            else:
                # æ–°èŠ‚ç‚¹ï¼Œä½¿ç”¨çº¿æ€§å˜æ¢
                linear_out = torch.matmul(x[i], self.weight)
                if self.bias is not None:
                    linear_out += self.bias
                output_embeddings.append(linear_out)
                self.node_embeddings_cache[node_id] = linear_out
        
        return torch.stack(output_embeddings)
    
    def message(self, x_j: torch.Tensor) -> torch.Tensor:
        """æ¶ˆæ¯å‡½æ•°"""
        return x_j
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.node_embeddings_cache.clear()
        self.edge_cache.clear()


class IncrementalGNN(nn.Module):
    """
    ğŸš€ å¢é‡å­¦ä¹ å›¾ç¥ç»ç½‘ç»œ - å¤šå±‚å¢é‡GCN
    """
    
    def __init__(self, 
                 input_dim: int,
                 hidden_dim: int,
                 output_dim: int,
                 num_layers: int = 2,
                 dropout: float = 0.1):
        super().__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # æ„å»ºå¢é‡GCNå±‚
        self.layers = nn.ModuleList()
        
        # è¾“å…¥å±‚
        self.layers.append(IncrementalGCNConv(input_dim, hidden_dim))
        
        # éšè—å±‚
        for _ in range(num_layers - 2):
            self.layers.append(IncrementalGCNConv(hidden_dim, hidden_dim))
        
        # è¾“å‡ºå±‚
        if num_layers > 1:
            self.layers.append(IncrementalGCNConv(hidden_dim, output_dim))
        else:
            self.layers[0] = IncrementalGCNConv(input_dim, output_dim)
        
        self.dropout_layer = nn.Dropout(dropout)
    
    def forward(self, 
                x: torch.Tensor, 
                edge_index: torch.Tensor,
                node_ids: Optional[List[str]] = None,
                incremental: bool = False) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            node_ids: èŠ‚ç‚¹IDåˆ—è¡¨
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ¨¡å¼
        Returns:
            èŠ‚ç‚¹åµŒå…¥ [num_nodes, output_dim]
        """
        for i, layer in enumerate(self.layers):
            x = layer(x, edge_index, node_ids, incremental)
            
            if i < len(self.layers) - 1:  # ä¸åœ¨æœ€åä¸€å±‚åº”ç”¨æ¿€æ´»å’Œdropout
                x = F.relu(x)
                x = self.dropout_layer(x)
        
        return x
    
    def update_incrementally(self, 
                           new_x: torch.Tensor,
                           new_edge_index: torch.Tensor,
                           new_node_ids: List[str]) -> torch.Tensor:
        """
        ğŸš€ å¢é‡æ›´æ–° - åªæ›´æ–°æ–°å¢æˆ–ä¿®æ”¹çš„éƒ¨åˆ†
        Args:
            new_x: æ–°çš„/æ›´æ–°çš„èŠ‚ç‚¹ç‰¹å¾
            new_edge_index: æ–°çš„è¾¹ç´¢å¼•
            new_node_ids: æ–°çš„èŠ‚ç‚¹ID
        Returns:
            æ›´æ–°åçš„èŠ‚ç‚¹åµŒå…¥
        """
        return self.forward(new_x, new_edge_index, new_node_ids, incremental=True)
    
    def clear_all_caches(self):
        """æ¸…ç©ºæ‰€æœ‰å±‚çš„ç¼“å­˜"""
        for layer in self.layers:
            if hasattr(layer, 'clear_cache'):
                layer.clear_cache()
    
    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for i, layer in enumerate(self.layers):
            if hasattr(layer, 'node_embeddings_cache'):
                stats[f'layer_{i}_cached_nodes'] = len(layer.node_embeddings_cache)
                stats[f'layer_{i}_cached_edges'] = len(layer.edge_cache)
        return stats


class AdaptiveIncrementalGNN(IncrementalGNN):
    """
    ğŸš€ è‡ªé€‚åº”å¢é‡GNN - æ ¹æ®å›¾å˜åŒ–ç¨‹åº¦è‡ªåŠ¨é€‰æ‹©æ›´æ–°ç­–ç•¥
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # è‡ªé€‚åº”å‚æ•°
        self.change_threshold = 0.1  # å˜åŒ–é˜ˆå€¼
        self.full_update_interval = 100  # å®Œæ•´æ›´æ–°é—´éš”
        self.update_count = 0
    
    def adaptive_forward(self,
                        x: torch.Tensor,
                        edge_index: torch.Tensor,
                        node_ids: Optional[List[str]] = None,
                        prev_x: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ğŸš€ è‡ªé€‚åº”å‰å‘ä¼ æ’­ - æ ¹æ®å˜åŒ–ç¨‹åº¦é€‰æ‹©æ›´æ–°ç­–ç•¥
        """
        self.update_count += 1
        
        # å¼ºåˆ¶å®Œæ•´æ›´æ–°çš„æ¡ä»¶
        force_full_update = (
            self.update_count % self.full_update_interval == 0 or
            prev_x is None or
            node_ids is None
        )
        
        if force_full_update:
            # å®Œæ•´æ›´æ–°
            self.clear_all_caches()
            return self.forward(x, edge_index, node_ids, incremental=False)
        
        # è®¡ç®—å˜åŒ–ç¨‹åº¦
        if prev_x.shape == x.shape:
            change_ratio = torch.norm(x - prev_x) / torch.norm(prev_x)
            
            if change_ratio > self.change_threshold:
                # å˜åŒ–è¾ƒå¤§ï¼Œä½¿ç”¨å®Œæ•´æ›´æ–°
                self.clear_all_caches()
                return self.forward(x, edge_index, node_ids, incremental=False)
        
        # å˜åŒ–è¾ƒå°ï¼Œä½¿ç”¨å¢é‡æ›´æ–°
        return self.forward(x, edge_index, node_ids, incremental=True)
