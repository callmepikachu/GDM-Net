"""
å›¾è®°å¿†ç®¡ç†å·¥å…· - æä¾›å›¾è®°å¿†çš„åˆ†æã€æ¸…ç†å’Œç®¡ç†åŠŸèƒ½
"""

import os
import pickle
import argparse
from typing import Dict, Any
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


class GraphMemoryManager:
    """å›¾è®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, memory_path: str):
        self.memory_path = memory_path
        self.memory_data = None
        self.load_memory()
    
    def load_memory(self):
        """åŠ è½½å›¾è®°å¿†æ•°æ®"""
        if os.path.exists(self.memory_path):
            with open(self.memory_path, 'rb') as f:
                self.memory_data = pickle.load(f)
            print(f"âœ… Loaded graph memory from {self.memory_path}")
        else:
            print(f"âŒ Memory file not found: {self.memory_path}")
            self.memory_data = {'nodes': {}, 'edges': {}, 'entity_type_index': {}}
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å›¾è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.memory_data:
            return {}
        
        nodes = self.memory_data.get('nodes', {})
        edges = self.memory_data.get('edges', {})
        entity_types = self.memory_data.get('entity_type_index', {})
        
        # è®¡ç®—èŠ‚ç‚¹ç»Ÿè®¡
        node_counts_by_type = {}
        for node_id, node_data in nodes.items():
            node_type = node_data.get('type', 'UNKNOWN')
            node_counts_by_type[node_type] = node_counts_by_type.get(node_type, 0) + 1
        
        # è®¡ç®—è¾¹ç»Ÿè®¡
        edge_counts_by_type = {}
        for (src, rel_type, dst), edge_data in edges.items():
            edge_counts_by_type[rel_type] = edge_counts_by_type.get(rel_type, 0) + 1
        
        return {
            'total_nodes': len(nodes),
            'total_edges': len(edges),
            'entity_types': len(entity_types),
            'node_counts_by_type': node_counts_by_type,
            'edge_counts_by_type': edge_counts_by_type,
            'avg_node_degree': len(edges) * 2 / len(nodes) if len(nodes) > 0 else 0
        }
    
    def find_similar_entities(self, query_text: str, top_k: int = 10):
        """æŸ¥æ‰¾ä¸æŸ¥è¯¢æ–‡æœ¬ç›¸ä¼¼çš„å®ä½“"""
        if not self.memory_data or not self.memory_data.get('nodes'):
            print("No nodes in memory")
            return []
        
        nodes = self.memory_data['nodes']
        similar_entities = []
        
        for node_id, node_data in nodes.items():
            entity_text = node_data.get('text', '')
            if query_text.lower() in entity_text.lower():
                similar_entities.append({
                    'id': node_id,
                    'text': entity_text,
                    'type': node_data.get('type', 'UNKNOWN'),
                    'count': node_data.get('count', 1)
                })
        
        # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
        similar_entities.sort(key=lambda x: x['count'], reverse=True)
        return similar_entities[:top_k]
    
    def clean_low_frequency_nodes(self, min_count: int = 2):
        """æ¸…ç†ä½é¢‘èŠ‚ç‚¹"""
        if not self.memory_data:
            return
        
        nodes = self.memory_data['nodes']
        edges = self.memory_data['edges']
        
        # æ‰¾åˆ°ä½é¢‘èŠ‚ç‚¹
        low_freq_nodes = set()
        for node_id, node_data in nodes.items():
            if node_data.get('count', 1) < min_count:
                low_freq_nodes.add(node_id)
        
        # åˆ é™¤ä½é¢‘èŠ‚ç‚¹
        for node_id in low_freq_nodes:
            del nodes[node_id]
        
        # åˆ é™¤æ¶‰åŠä½é¢‘èŠ‚ç‚¹çš„è¾¹
        edges_to_remove = []
        for (src, rel_type, dst) in edges.keys():
            if src in low_freq_nodes or dst in low_freq_nodes:
                edges_to_remove.append((src, rel_type, dst))
        
        for edge_key in edges_to_remove:
            del edges[edge_key]
        
        print(f"ğŸ—‘ï¸ Removed {len(low_freq_nodes)} low-frequency nodes and {len(edges_to_remove)} edges")
    
    def save_memory(self, output_path: str = None):
        """ä¿å­˜æ¸…ç†åçš„å›¾è®°å¿†"""
        save_path = output_path or self.memory_path
        with open(save_path, 'wb') as f:
            pickle.dump(self.memory_data, f)
        print(f"ğŸ’¾ Saved cleaned memory to {save_path}")
    
    def export_to_text(self, output_path: str):
        """å¯¼å‡ºå›¾è®°å¿†ä¸ºå¯è¯»æ–‡æœ¬æ ¼å¼"""
        if not self.memory_data:
            return
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=== Graph Memory Export ===\n\n")
            
            # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
            stats = self.get_statistics()
            f.write("Statistics:\n")
            for key, value in stats.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")
            
            # å†™å…¥èŠ‚ç‚¹ä¿¡æ¯
            f.write("Nodes:\n")
            nodes = self.memory_data.get('nodes', {})
            for node_id, node_data in list(nodes.items())[:100]:  # åªæ˜¾ç¤ºå‰100ä¸ª
                f.write(f"  {node_id}: {node_data.get('text', 'N/A')} ({node_data.get('type', 'UNKNOWN')})\n")
            
            if len(nodes) > 100:
                f.write(f"  ... and {len(nodes) - 100} more nodes\n")
            
            f.write("\n")
            
            # å†™å…¥è¾¹ä¿¡æ¯
            f.write("Edges (sample):\n")
            edges = self.memory_data.get('edges', {})
            for i, ((src, rel_type, dst), edge_data) in enumerate(list(edges.items())[:50]):
                src_text = nodes.get(src, {}).get('text', src[:8])
                dst_text = nodes.get(dst, {}).get('text', dst[:8])
                f.write(f"  {src_text} --[{rel_type}]--> {dst_text} (count: {edge_data.get('count', 1)})\n")
                if i >= 49:
                    break
            
            if len(edges) > 50:
                f.write(f"  ... and {len(edges) - 50} more edges\n")
        
        print(f"ğŸ“„ Exported memory to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Graph Memory Manager")
    parser.add_argument("--memory_path", required=True, help="Path to graph memory file")
    parser.add_argument("--action", choices=['stats', 'clean', 'export', 'search'], 
                       default='stats', help="Action to perform")
    parser.add_argument("--min_count", type=int, default=2, 
                       help="Minimum count for cleaning (default: 2)")
    parser.add_argument("--output", help="Output path for export/save")
    parser.add_argument("--query", help="Query text for search")
    parser.add_argument("--top_k", type=int, default=10, help="Top K results for search")
    
    args = parser.parse_args()
    
    manager = GraphMemoryManager(args.memory_path)
    
    if args.action == 'stats':
        stats = manager.get_statistics()
        print("\nğŸ“Š Graph Memory Statistics:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
    
    elif args.action == 'clean':
        manager.clean_low_frequency_nodes(args.min_count)
        if args.output:
            manager.save_memory(args.output)
        else:
            manager.save_memory()
    
    elif args.action == 'export':
        output_path = args.output or args.memory_path.replace('.pkl', '.txt')
        manager.export_to_text(output_path)
    
    elif args.action == 'search':
        if not args.query:
            print("âŒ Please provide --query for search")
            return
        
        results = manager.find_similar_entities(args.query, args.top_k)
        print(f"\nğŸ” Search results for '{args.query}':")
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result['text']} ({result['type']}) - count: {result['count']}")


if __name__ == "__main__":
    main()
