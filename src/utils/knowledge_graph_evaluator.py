"""
çŸ¥è¯†å›¾è°±è´¨é‡è¯„ä¼°å™¨ - è¯„ä¼°æŒä¹…åŒ–å›¾è®°å¿†çš„è´¨é‡å’Œæœ‰æ•ˆæ€§
"""

import pickle
import numpy as np
from typing import Dict, List, Tuple, Any, Set
from collections import defaultdict, Counter
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


class KnowledgeGraphEvaluator:
    """
    çŸ¥è¯†å›¾è°±è´¨é‡è¯„ä¼°å™¨
    """
    
    def __init__(self, memory_path: str):
        self.memory_path = memory_path
        self.memory_data = None
        self.load_memory()
    
    def load_memory(self):
        """åŠ è½½å›¾è®°å¿†æ•°æ®"""
        try:
            with open(self.memory_path, 'rb') as f:
                self.memory_data = pickle.load(f)
            print(f"âœ… Loaded graph memory from {self.memory_path}")
        except Exception as e:
            print(f"âŒ Failed to load memory: {e}")
            self.memory_data = {'nodes': {}, 'edges': {}, 'entity_type_index': {}}
    
    def evaluate_completeness(self) -> Dict[str, float]:
        """
        ğŸš€ è¯„ä¼°çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§
        Returns:
            å®Œæ•´æ€§è¯„ä¼°æŒ‡æ ‡
        """
        nodes = self.memory_data.get('nodes', {})
        edges = self.memory_data.get('edges', {})
        
        if not nodes:
            return {'completeness_score': 0.0, 'coverage_ratio': 0.0, 'density': 0.0}
        
        # 1. èŠ‚ç‚¹è¦†ç›–ç‡ï¼ˆæœ‰æ–‡æœ¬ä¿¡æ¯çš„èŠ‚ç‚¹æ¯”ä¾‹ï¼‰
        nodes_with_text = sum(1 for node in nodes.values() if node.get('text', '').strip())
        coverage_ratio = nodes_with_text / len(nodes)
        
        # 2. å›¾å¯†åº¦
        num_nodes = len(nodes)
        num_edges = len(edges)
        max_possible_edges = num_nodes * (num_nodes - 1) // 2
        density = num_edges / max_possible_edges if max_possible_edges > 0 else 0
        
        # 3. ç±»å‹å¤šæ ·æ€§
        entity_types = set(node.get('type', 'UNKNOWN') for node in nodes.values())
        type_diversity = len(entity_types) / max(len(nodes), 1)
        
        # 4. ç»¼åˆå®Œæ•´æ€§åˆ†æ•°
        completeness_score = (coverage_ratio * 0.4 + density * 0.3 + type_diversity * 0.3)
        
        return {
            'completeness_score': completeness_score,
            'coverage_ratio': coverage_ratio,
            'density': density,
            'type_diversity': type_diversity,
            'num_entity_types': len(entity_types)
        }
    
    def evaluate_consistency(self) -> Dict[str, float]:
        """
        ğŸš€ è¯„ä¼°çŸ¥è¯†å›¾è°±çš„ä¸€è‡´æ€§
        Returns:
            ä¸€è‡´æ€§è¯„ä¼°æŒ‡æ ‡
        """
        nodes = self.memory_data.get('nodes', {})
        edges = self.memory_data.get('edges', {})
        
        if not nodes:
            return {'consistency_score': 0.0}
        
        # 1. å®ä½“åµŒå…¥ä¸€è‡´æ€§ï¼ˆåŒç±»å‹å®ä½“çš„åµŒå…¥ç›¸ä¼¼æ€§ï¼‰
        type_consistency_scores = []
        entity_types = self.memory_data.get('entity_type_index', {})
        
        for entity_type, node_ids in entity_types.items():
            if len(node_ids) >= 2:
                embeddings = []
                for node_id in node_ids:
                    if node_id in nodes:
                        embeddings.append(nodes[node_id]['embedding'])
                
                if len(embeddings) >= 2:
                    embeddings = np.array(embeddings)
                    # è®¡ç®—ç±»å†…ç›¸ä¼¼åº¦
                    similarities = cosine_similarity(embeddings)
                    # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªç›¸ä¼¼åº¦ï¼‰
                    mask = ~np.eye(similarities.shape[0], dtype=bool)
                    avg_similarity = similarities[mask].mean()
                    type_consistency_scores.append(avg_similarity)
        
        type_consistency = np.mean(type_consistency_scores) if type_consistency_scores else 0.0
        
        # 2. å…³ç³»ä¸€è‡´æ€§ï¼ˆç›¸åŒå…³ç³»ç±»å‹çš„é¢‘ç‡åˆ†å¸ƒï¼‰
        relation_counts = Counter()
        for (src, rel_type, dst), edge_data in edges.items():
            relation_counts[rel_type] += edge_data.get('count', 1)
        
        if relation_counts:
            # è®¡ç®—å…³ç³»åˆ†å¸ƒçš„ç†µï¼ˆè¶Šä½è¶Šä¸€è‡´ï¼‰
            total_relations = sum(relation_counts.values())
            probabilities = [count / total_relations for count in relation_counts.values()]
            entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)
            max_entropy = np.log2(len(relation_counts))
            relation_consistency = 1.0 - (entropy / max_entropy) if max_entropy > 0 else 1.0
        else:
            relation_consistency = 0.0
        
        # 3. ç»¼åˆä¸€è‡´æ€§åˆ†æ•°
        consistency_score = (type_consistency * 0.6 + relation_consistency * 0.4)
        
        return {
            'consistency_score': consistency_score,
            'type_consistency': type_consistency,
            'relation_consistency': relation_consistency,
            'num_relation_types': len(relation_counts)
        }
    
    def evaluate_connectivity(self) -> Dict[str, float]:
        """
        ğŸš€ è¯„ä¼°çŸ¥è¯†å›¾è°±çš„è¿é€šæ€§
        Returns:
            è¿é€šæ€§è¯„ä¼°æŒ‡æ ‡
        """
        nodes = self.memory_data.get('nodes', {})
        edges = self.memory_data.get('edges', {})
        
        if not nodes or not edges:
            return {'connectivity_score': 0.0, 'largest_component_ratio': 0.0}
        
        # æ„å»ºNetworkXå›¾
        G = nx.Graph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for node_id in nodes.keys():
            G.add_node(node_id)
        
        # æ·»åŠ è¾¹
        for (src, rel_type, dst) in edges.keys():
            if src in nodes and dst in nodes:
                G.add_edge(src, dst)
        
        # 1. è¿é€šåˆ†é‡åˆ†æ
        connected_components = list(nx.connected_components(G))
        largest_component_size = max(len(comp) for comp in connected_components) if connected_components else 0
        largest_component_ratio = largest_component_size / len(nodes) if len(nodes) > 0 else 0
        
        # 2. å¹³å‡è·¯å¾„é•¿åº¦
        if largest_component_size > 1:
            largest_component = max(connected_components, key=len)
            subgraph = G.subgraph(largest_component)
            try:
                avg_path_length = nx.average_shortest_path_length(subgraph)
                # å½’ä¸€åŒ–ï¼ˆè¾ƒçŸ­çš„è·¯å¾„æ›´å¥½ï¼‰
                normalized_path_length = 1.0 / (1.0 + avg_path_length)
            except:
                normalized_path_length = 0.0
        else:
            normalized_path_length = 0.0
        
        # 3. èšç±»ç³»æ•°
        clustering_coefficient = nx.average_clustering(G) if len(G.nodes()) > 0 else 0.0
        
        # 4. ç»¼åˆè¿é€šæ€§åˆ†æ•°
        connectivity_score = (
            largest_component_ratio * 0.4 +
            normalized_path_length * 0.3 +
            clustering_coefficient * 0.3
        )
        
        return {
            'connectivity_score': connectivity_score,
            'largest_component_ratio': largest_component_ratio,
            'avg_path_length': avg_path_length if 'avg_path_length' in locals() else float('inf'),
            'clustering_coefficient': clustering_coefficient,
            'num_components': len(connected_components)
        }
    
    def evaluate_redundancy(self) -> Dict[str, float]:
        """
        ğŸš€ è¯„ä¼°çŸ¥è¯†å›¾è°±çš„å†—ä½™æ€§
        Returns:
            å†—ä½™æ€§è¯„ä¼°æŒ‡æ ‡
        """
        nodes = self.memory_data.get('nodes', {})
        
        if len(nodes) < 2:
            return {'redundancy_score': 0.0, 'duplicate_ratio': 0.0}
        
        # 1. åŸºäºåµŒå…¥çš„é‡å¤æ£€æµ‹
        embeddings = []
        node_ids = []
        
        for node_id, node_data in nodes.items():
            embeddings.append(node_data['embedding'])
            node_ids.append(node_id)
        
        embeddings = np.array(embeddings)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(embeddings)
        
        # æ‰¾åˆ°é«˜ç›¸ä¼¼åº¦å¯¹ï¼ˆæ’é™¤è‡ªç›¸ä¼¼ï¼‰
        high_similarity_threshold = 0.95
        duplicate_pairs = 0
        total_pairs = 0
        
        for i in range(len(node_ids)):
            for j in range(i + 1, len(node_ids)):
                total_pairs += 1
                if similarity_matrix[i, j] > high_similarity_threshold:
                    duplicate_pairs += 1
        
        duplicate_ratio = duplicate_pairs / total_pairs if total_pairs > 0 else 0.0
        
        # 2. åŸºäºæ–‡æœ¬çš„é‡å¤æ£€æµ‹
        text_duplicates = 0
        text_pairs = 0
        
        texts = [node.get('text', '').lower().strip() for node in nodes.values()]
        
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                if texts[i] and texts[j]:
                    text_pairs += 1
                    if texts[i] == texts[j]:
                        text_duplicates += 1
        
        text_duplicate_ratio = text_duplicates / text_pairs if text_pairs > 0 else 0.0
        
        # 3. ç»¼åˆå†—ä½™æ€§åˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥ç”¨1å‡å»ï¼‰
        redundancy_score = 1.0 - (duplicate_ratio * 0.6 + text_duplicate_ratio * 0.4)
        
        return {
            'redundancy_score': redundancy_score,
            'duplicate_ratio': duplicate_ratio,
            'text_duplicate_ratio': text_duplicate_ratio,
            'potential_duplicates': duplicate_pairs
        }
    
    def evaluate_overall_quality(self) -> Dict[str, Any]:
        """
        ğŸš€ ç»¼åˆè¯„ä¼°çŸ¥è¯†å›¾è°±è´¨é‡
        Returns:
            ç»¼åˆè´¨é‡è¯„ä¼°æŠ¥å‘Š
        """
        print("ğŸ” Evaluating knowledge graph quality...")
        
        # å„ç»´åº¦è¯„ä¼°
        completeness = self.evaluate_completeness()
        consistency = self.evaluate_consistency()
        connectivity = self.evaluate_connectivity()
        redundancy = self.evaluate_redundancy()
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        overall_score = (
            completeness['completeness_score'] * 0.25 +
            consistency['consistency_score'] * 0.25 +
            connectivity['connectivity_score'] * 0.25 +
            redundancy['redundancy_score'] * 0.25
        )
        
        # è´¨é‡ç­‰çº§
        if overall_score >= 0.8:
            quality_grade = "Excellent"
        elif overall_score >= 0.6:
            quality_grade = "Good"
        elif overall_score >= 0.4:
            quality_grade = "Fair"
        else:
            quality_grade = "Poor"
        
        return {
            'overall_score': overall_score,
            'quality_grade': quality_grade,
            'completeness': completeness,
            'consistency': consistency,
            'connectivity': connectivity,
            'redundancy': redundancy,
            'recommendations': self._generate_recommendations(
                completeness, consistency, connectivity, redundancy
            )
        }
    
    def _generate_recommendations(self, completeness: Dict, consistency: Dict, 
                                connectivity: Dict, redundancy: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if completeness['completeness_score'] < 0.6:
            recommendations.append("Improve data completeness by adding more entity text information")
        
        if consistency['consistency_score'] < 0.6:
            recommendations.append("Enhance entity alignment to improve type consistency")
        
        if connectivity['connectivity_score'] < 0.6:
            recommendations.append("Add more relationships to improve graph connectivity")
        
        if redundancy['redundancy_score'] < 0.7:
            recommendations.append("Remove duplicate entities to reduce redundancy")
        
        if completeness['density'] < 0.1:
            recommendations.append("Increase relationship extraction to improve graph density")
        
        return recommendations


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Knowledge Graph Quality Evaluator")
    parser.add_argument("--memory_path", required=True, help="Path to graph memory file")
    parser.add_argument("--output", help="Output file for evaluation report")
    
    args = parser.parse_args()
    
    evaluator = KnowledgeGraphEvaluator(args.memory_path)
    quality_report = evaluator.evaluate_overall_quality()
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“Š KNOWLEDGE GRAPH QUALITY REPORT")
    print("="*50)
    print(f"Overall Score: {quality_report['overall_score']:.3f}")
    print(f"Quality Grade: {quality_report['quality_grade']}")
    print()
    
    print("ğŸ“ˆ Detailed Metrics:")
    for category, metrics in quality_report.items():
        if isinstance(metrics, dict) and category != 'recommendations':
            print(f"\n{category.title()}:")
            for metric, value in metrics.items():
                if isinstance(value, float):
                    print(f"  {metric}: {value:.3f}")
                else:
                    print(f"  {metric}: {value}")
    
    print("\nğŸ’¡ Recommendations:")
    for i, rec in enumerate(quality_report['recommendations'], 1):
        print(f"  {i}. {rec}")
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        import json
        with open(args.output, 'w') as f:
            json.dump(quality_report, f, indent=2, default=str)
        print(f"\nğŸ“„ Report saved to {args.output}")


if __name__ == "__main__":
    main()
